{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfoZ52WolNRaCyQF8IYik/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditijha2000/FRE/blob/main/FRE_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax jaxlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83-_WABttigR",
        "outputId": "50ca6f37-5189-49ae-df79-3aab60d88b97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax) (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml-collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NePGTwL-uSXT",
        "outputId": "fbea33dd-5c47-4aa4-9ff9-30ce9a821994"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml-collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections) (21.6.0)\n",
            "Building wheels for collected packages: ml-collections\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=0fc9b3966b2041571b41e95b65f7bd9d80dde9a25f4a63832237be8dc793781d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built ml-collections\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dm_control"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWY8FJpS6LaJ",
        "outputId": "cf6f897b-2614-48a0-84cb-57d6d87a8325"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dm_control\n",
            "  Downloading dm_control-1.0.18-py3-none-any.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dm_control) (1.4.0)\n",
            "Collecting dm-env (from dm_control)\n",
            "  Downloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: dm-tree!=0.1.2 in /usr/local/lib/python3.10/dist-packages (from dm_control) (0.1.8)\n",
            "Collecting glfw (from dm_control)\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting labmaze (from dm_control)\n",
            "  Downloading labmaze-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from dm_control) (4.9.4)\n",
            "Collecting mujoco>=3.1.4 (from dm_control)\n",
            "  Downloading mujoco-3.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dm_control) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.10/dist-packages (from dm_control) (3.20.3)\n",
            "Requirement already satisfied: pyopengl>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from dm_control) (3.1.7)\n",
            "Requirement already satisfied: pyparsing>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dm_control) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dm_control) (2.31.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0 in /usr/local/lib/python3.10/dist-packages (from dm_control) (67.7.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dm_control) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dm_control) (4.66.2)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=3.1.4->dm_control) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dm_control) (2024.2.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.1.4->dm_control) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.1.4->dm_control) (6.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.1.4->dm_control) (4.11.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=3.1.4->dm_control) (3.18.1)\n",
            "Installing collected packages: glfw, labmaze, dm-env, mujoco, dm_control\n",
            "Successfully installed dm-env-1.6 dm_control-1.0.18 glfw-2.7.0 labmaze-1.0.6 mujoco-3.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dm-env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KEWqMPM9Ozs",
        "outputId": "806d2784-aa9a-46ec-ea54-5f668d6176e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dm-env in /usr/local/lib/python3.10/dist-packages (1.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from dm-env) (1.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-env) (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dm-env) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NXVDiZvYxnsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b6a00b-2f00-4d5f-d216-aea432918a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to exorl data is <_io.TextIOWrapper name='/content/download.py' mode='r' encoding='UTF-8'>\n"
          ]
        }
      ],
      "source": [
        "#fre\\common\\typing\n",
        "\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "\n",
        "PRNGKey = Any\n",
        "Params = flax.core.FrozenDict[str, Any]\n",
        "PRNGKey = Any\n",
        "Shape = Sequence[int]\n",
        "Dtype = Any  # this could be a real type?\n",
        "InfoDict = Dict[str, float]\n",
        "Array = Union[np.ndarray, jnp.ndarray]\n",
        "Data = Union[Array, Dict[str, \"Data\"]]\n",
        "Batch = Dict[str, Data]\n",
        "ModuleMethod = Union[\n",
        "    str, Callable, None\n",
        "]  # A method to be passed into TrainState.__call__\n",
        "\n",
        "\n",
        "#fre\\common\\dataset\n",
        "###############################\n",
        "#\n",
        "#  Dataset Pytrees for offline data, replay buffers, etc.\n",
        "#\n",
        "###############################\n",
        "\n",
        "import numpy as np\n",
        "#from fre.common.typing import Data, Array\n",
        "from flax.core.frozen_dict import FrozenDict\n",
        "from jax import tree_util\n",
        "\n",
        "\n",
        "def get_size(data: Data) -> int:\n",
        "    sizes = tree_util.tree_map(lambda arr: len(arr), data)\n",
        "    return max(tree_util.tree_leaves(sizes))\n",
        "\n",
        "\n",
        "class Dataset(FrozenDict):\n",
        "    \"\"\"\n",
        "    A class for storing (and retrieving batches of) data in nested dictionary format.\n",
        "\n",
        "    Example:\n",
        "        dataset = Dataset({\n",
        "            'observations': {\n",
        "                'image': np.random.randn(100, 28, 28, 1),\n",
        "                'state': np.random.randn(100, 4),\n",
        "            },\n",
        "            'actions': np.random.randn(100, 2),\n",
        "        })\n",
        "\n",
        "        batch = dataset.sample(32)\n",
        "        # Batch should have nested shape: {\n",
        "        # 'observations': {'image': (32, 28, 28, 1), 'state': (32, 4)},\n",
        "        # 'actions': (32, 2)\n",
        "        # }\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def create(\n",
        "        cls,\n",
        "        observations: Data,\n",
        "        actions: Array,\n",
        "        rewards: Array,\n",
        "        masks: Array,\n",
        "        next_observations: Data,\n",
        "        freeze=True,\n",
        "        **extra_fields\n",
        "    ):\n",
        "        data = {\n",
        "            \"observations\": observations,\n",
        "            \"actions\": actions,\n",
        "            \"rewards\": rewards,\n",
        "            \"masks\": masks,\n",
        "            \"next_observations\": next_observations,\n",
        "            **extra_fields,\n",
        "        }\n",
        "        # Force freeze\n",
        "        if freeze:\n",
        "            tree_util.tree_map(lambda arr: arr.setflags(write=False), data)\n",
        "        return cls(data)\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.size = get_size(self._dict)\n",
        "\n",
        "    def sample(self, batch_size: int, indx=None):\n",
        "        \"\"\"\n",
        "        Sample a batch of data from the dataset. Use `indx` to specify a specific\n",
        "        set of indices to retrieve. Otherwise, a random sample will be drawn.\n",
        "\n",
        "        Returns a dictionary with the same structure as the original dataset.\n",
        "        \"\"\"\n",
        "        if indx is None:\n",
        "            indx = np.random.randint(self.size, size=batch_size)\n",
        "        return self.get_subset(indx)\n",
        "\n",
        "    def get_subset(self, indx):\n",
        "        return tree_util.tree_map(lambda arr: arr[indx], self._dict)\n",
        "\n",
        "\n",
        "class ReplayBuffer(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset where data is added to the buffer.\n",
        "\n",
        "    Example:\n",
        "        example_transition = {\n",
        "            'observations': {\n",
        "                'image': np.random.randn(28, 28, 1),\n",
        "                'state': np.random.randn(4),\n",
        "            },\n",
        "            'actions': np.random.randn(2),\n",
        "        }\n",
        "        buffer = ReplayBuffer.create(example_transition, size=1000)\n",
        "        buffer.add_transition(example_transition)\n",
        "        batch = buffer.sample(32)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls, transition: Data, size: int):\n",
        "        def create_buffer(example):\n",
        "            example = np.array(example)\n",
        "            return np.zeros((size, *example.shape), dtype=example.dtype)\n",
        "\n",
        "        buffer_dict = tree_util.tree_map(create_buffer, transition)\n",
        "        return cls(buffer_dict)\n",
        "\n",
        "    @classmethod\n",
        "    def create_from_initial_dataset(cls, init_dataset: dict, size: int):\n",
        "        def create_buffer(init_buffer):\n",
        "            buffer = np.zeros((size, *init_buffer.shape[1:]), dtype=init_buffer.dtype)\n",
        "            buffer[: len(init_buffer)] = init_buffer\n",
        "            return buffer\n",
        "\n",
        "        buffer_dict = tree_util.tree_map(create_buffer, init_dataset)\n",
        "        dataset = cls(buffer_dict)\n",
        "        dataset.size = dataset.pointer = get_size(init_dataset)\n",
        "        return dataset\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.max_size = get_size(self._dict)\n",
        "        self.size = 0\n",
        "        self.pointer = 0\n",
        "\n",
        "    def add_transition(self, transition):\n",
        "        def set_idx(buffer, new_element):\n",
        "            buffer[self.pointer] = new_element\n",
        "\n",
        "        tree_util.tree_map(set_idx, self._dict, transition)\n",
        "        self.pointer = (self.pointer + 1) % self.max_size\n",
        "        self.size = max(self.pointer, self.size)\n",
        "\n",
        "    def clear(self):\n",
        "        self.size = self.pointer = 0\n",
        "\n",
        "\n",
        "\n",
        "  # fre/common/evaluation\n",
        "\n",
        "###############################\n",
        "#\n",
        "#  Tools for evaluating policies in environments.\n",
        "#\n",
        "###############################\n",
        "\n",
        "\n",
        "from typing import Dict\n",
        "import jax\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import time\n",
        "#import wandb\n",
        "\n",
        "\n",
        "def flatten(d, parent_key=\"\", sep=\".\"):\n",
        "    \"\"\"\n",
        "    Helper function that flattens a dictionary of dictionaries into a single dictionary.\n",
        "    E.g: flatten({'a': {'b': 1}}) -> {'a.b': 1}\n",
        "    \"\"\"\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = parent_key + sep + k if parent_key else k\n",
        "        if hasattr(v, \"items\"):\n",
        "            items.extend(flatten(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "\n",
        "def add_to(dict_of_lists, single_dict):\n",
        "    for k, v in single_dict.items():\n",
        "        dict_of_lists[k].append(v)\n",
        "\n",
        "\n",
        "def evaluate(policy_fn, env: gym.Env, num_episodes: int, record_video : bool = False,\n",
        "             return_trajectories=False, clip_return_at_goal=False, binary_return=False, use_discrete_xy=False, clip_margin=0):\n",
        "    print(\"Clip return at goal is\", clip_return_at_goal)\n",
        "    stats = defaultdict(list)\n",
        "    frames = []\n",
        "    trajectories = []\n",
        "    for i in range(num_episodes):\n",
        "        now = time.time()\n",
        "        trajectory = defaultdict(list)\n",
        "        ob_list = []\n",
        "        ac_list = []\n",
        "        observation, done = env.reset(), False\n",
        "        ob_list.append(observation)\n",
        "        while not done:\n",
        "            if use_discrete_xy:\n",
        "                #import fre.common.envs.d4rl.d4rl_ant as d4rl_ant\n",
        "                ob_input = d4rl_ant.discretize_obs(observation)\n",
        "            else:\n",
        "                ob_input = observation\n",
        "            action = policy_fn(ob_input)\n",
        "            action = np.array(action)\n",
        "            next_observation, r, done, info = env.step(action)\n",
        "            add_to(stats, flatten(info))\n",
        "\n",
        "            if type(observation) is dict:\n",
        "                obs_pure = observation['observation']\n",
        "                next_obs_pure = next_observation['observation']\n",
        "            else:\n",
        "                obs_pure = observation\n",
        "                next_obs_pure = next_observation\n",
        "            transition = dict(\n",
        "                observation=obs_pure,\n",
        "                next_observation=next_obs_pure,\n",
        "                action=action,\n",
        "                reward=r,\n",
        "                done=done,\n",
        "                info=info,\n",
        "            )\n",
        "            observation = next_observation\n",
        "            ob_list.append(observation)\n",
        "            ac_list.append(action)\n",
        "            add_to(trajectory, transition)\n",
        "\n",
        "            if i <= 3 and record_video:\n",
        "                frames.append(env.render(mode=\"rgb_array\"))\n",
        "        add_to(stats, flatten(info, parent_key=\"final\"))\n",
        "        trajectories.append(trajectory)\n",
        "        print(\"Finished Episode\", i, \"in\", time.time() - now, \"seconds\")\n",
        "\n",
        "    if clip_return_at_goal and 'episode.return' in stats:\n",
        "        print(\"Episode finished. Return is {}. Length is {}.\".format(stats['episode.return'], stats['episode.length']))\n",
        "        stats['episode.return'] = np.clip(np.array(stats['episode.length']) + np.array(stats['episode.return']) - clip_margin, 0, 1) # Goal is a binary indicator.\n",
        "        print(\"Clipped return is {}.\".format(stats['episode.return']))\n",
        "    elif binary_return and 'episode.return' in stats:\n",
        "        # Assume that the reward is either 0 or 1 at each timestep.\n",
        "        print(\"Episode finished. Return is {}. Length is {}.\".format(stats['episode.return'], stats['episode.length']))\n",
        "        stats['episode.return'] = np.clip(np.array(stats['episode.return']), 0, 1)\n",
        "        print(\"Clipped return is {}.\".format(stats['episode.return']))\n",
        "\n",
        "    if 'episode.return' in stats:\n",
        "        print(\"Episode finished. Return is {}. Length is {}.\".format(stats['episode.return'], stats['episode.length']))\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        stats[k] = np.mean(v)\n",
        "\n",
        "    if record_video:\n",
        "        stacked = np.stack(frames)\n",
        "        stacked = stacked.transpose(0, 3, 1, 2)\n",
        "        while stacked.shape[2] > 160:\n",
        "            stacked = stacked[:, :, ::2, ::2]\n",
        "        stats['video'] = wandb.Video(stacked, fps=60)\n",
        "\n",
        "    if return_trajectories:\n",
        "        return stats, trajectories\n",
        "    else:\n",
        "        return stats\n",
        "\n",
        "  #fre/common/train_state\n",
        "  ###############################\n",
        "#\n",
        "#  Structures for managing training of flax networks.\n",
        "#\n",
        "###############################\n",
        "\n",
        "#from fre.common.typing import *\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import tree_util\n",
        "import optax\n",
        "import functools\n",
        "\n",
        "import gym\n",
        "\n",
        "nonpytree_field = functools.partial(flax.struct.field, pytree_node=False)\n",
        "\n",
        "\n",
        "def shard_batch(batch):\n",
        "    d = jax.local_device_count()\n",
        "\n",
        "    def reshape(x):\n",
        "        assert (\n",
        "            x.shape[0] % d == 0\n",
        "        ), f\"Batch size needs to be divisible by # devices, got {x.shape[0]} and {d}\"\n",
        "        return x.reshape((d, x.shape[0] // d, *x.shape[1:]))\n",
        "\n",
        "    return tree_util.tree_map(reshape, batch)\n",
        "\n",
        "\n",
        "def target_update(\n",
        "    model: \"TrainState\", target_model: \"TrainState\", tau: float\n",
        ") -> \"TrainState\":\n",
        "    new_target_params = jax.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "class TrainState(flax.struct.PyTreeNode):\n",
        "    \"\"\"\n",
        "    Core abstraction of a model in this repository.\n",
        "\n",
        "    Creation:\n",
        "    ```\n",
        "        model_def = nn.Dense(12) # or any other flax.linen Module\n",
        "        params = model_def.init(jax.random.PRNGKey(0), jnp.ones((1, 4)))['params']\n",
        "        model = TrainState.create(model_def, params, tx=None) # Optionally, pass in an optax optimizer\n",
        "    ```\n",
        "\n",
        "    Usage:\n",
        "    ```\n",
        "        y = model(jnp.ones((1, 4))) # By default, uses the `__call__` method of the model_def and params stored in TrainState\n",
        "        y = model(jnp.ones((1, 4)), params=params) # You can pass in params (useful for gradient computation)\n",
        "        y = model(jnp.ones((1, 4)), method=method) # You can apply a different method as well\n",
        "    ```\n",
        "\n",
        "    More complete example:\n",
        "    ```\n",
        "        def loss(params):\n",
        "            y_pred = model(x, params=params)\n",
        "            return jnp.mean((y - y_pred) ** 2)\n",
        "\n",
        "        grads = jax.grad(loss)(model.params)\n",
        "        new_model = model.apply_gradients(grads=grads) # Alternatively, new_model = model.apply_loss_fn(loss_fn=loss)\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    step: int\n",
        "    apply_fn: Callable[..., Any] = nonpytree_field()\n",
        "    model_def: Any = nonpytree_field()\n",
        "    params: Params\n",
        "    tx: Optional[optax.GradientTransformation] = nonpytree_field()\n",
        "    opt_state: Optional[optax.OptState] = None\n",
        "\n",
        "    @classmethod\n",
        "    def create(\n",
        "        cls,\n",
        "        model_def: nn.Module,\n",
        "        params: Params,\n",
        "        tx: Optional[optax.GradientTransformation] = None,\n",
        "        **kwargs,\n",
        "    ) -> \"TrainState\":\n",
        "        if tx is not None:\n",
        "            opt_state = tx.init(params)\n",
        "        else:\n",
        "            opt_state = None\n",
        "\n",
        "        return cls(\n",
        "            step=1,\n",
        "            apply_fn=model_def.apply,\n",
        "            model_def=model_def,\n",
        "            params=params,\n",
        "            tx=tx,\n",
        "            opt_state=opt_state,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        *args,\n",
        "        params=None,\n",
        "        extra_variables: dict = None,\n",
        "        method: ModuleMethod = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Internally calls model_def.apply_fn with the following logic:\n",
        "\n",
        "        Arguments:\n",
        "            params: If not None, use these params instead of the ones stored in the model.\n",
        "            extra_variables: Additional variables to pass into apply_fn\n",
        "            method: If None, use the `__call__` method of the model_def. If a string, uses\n",
        "                the method of the model_def with that name (e.g. 'encode' -> model_def.encode).\n",
        "                If a function, uses that function.\n",
        "\n",
        "        \"\"\"\n",
        "        if params is None:\n",
        "            params = self.params\n",
        "\n",
        "        variables = {\"params\": params}\n",
        "\n",
        "        if extra_variables is not None:\n",
        "            variables = {**variables, **extra_variables}\n",
        "\n",
        "        if isinstance(method, str):\n",
        "            method = getattr(self.model_def, method)\n",
        "\n",
        "        return self.apply_fn(variables, *args, method=method, **kwargs)\n",
        "\n",
        "    def do(self, method):\n",
        "        return functools.partial(self, method=method)\n",
        "\n",
        "    def apply_gradients(self, *, grads, **kwargs):\n",
        "        \"\"\"Updates `step`, `params`, `opt_state` and `**kwargs` in return value.\n",
        "\n",
        "        Note that internally this function calls `.tx.update()` followed by a call\n",
        "        to `optax.apply_updates()` to update `params` and `opt_state`.\n",
        "\n",
        "        Args:\n",
        "            grads: Gradients that have the same pytree structure as `.params`.\n",
        "            **kwargs: Additional dataclass attributes that should be `.replace()`-ed.\n",
        "\n",
        "        Returns:\n",
        "            An updated instance of `self` with `step` incremented by one, `params`\n",
        "            and `opt_state` updated by applying `grads`, and additional attributes\n",
        "            replaced as specified by `kwargs`.\n",
        "        \"\"\"\n",
        "        updates, new_opt_state = self.tx.update(grads, self.opt_state, self.params)\n",
        "        new_params = optax.apply_updates(self.params, updates)\n",
        "\n",
        "        return self.replace(\n",
        "            step=self.step + 1,\n",
        "            params=new_params,\n",
        "            opt_state=new_opt_state,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def apply_loss_fn(self, *, loss_fn, pmap_axis=None, has_aux=False):\n",
        "        \"\"\"\n",
        "        Takes a gradient step towards minimizing `loss_fn`. Internally, this calls\n",
        "        `jax.grad` followed by `TrainState.apply_gradients`. If pmap_axis is provided,\n",
        "        additionally it averages gradients (and info) across devices before performing update.\n",
        "        \"\"\"\n",
        "        if has_aux:\n",
        "            grads, info = jax.grad(loss_fn, has_aux=has_aux)(self.params)\n",
        "            if pmap_axis is not None:\n",
        "                grads = jax.lax.pmean(grads, axis_name=pmap_axis)\n",
        "                info = jax.lax.pmean(info, axis_name=pmap_axis)\n",
        "\n",
        "            return self.apply_gradients(grads=grads), info\n",
        "\n",
        "        else:\n",
        "            grads = jax.grad(loss_fn, has_aux=has_aux)(self.params)\n",
        "            if pmap_axis is not None:\n",
        "                grads = jax.lax.pmean(grads, axis_name=pmap_axis)\n",
        "            return self.apply_gradients(grads=grads)\n",
        "\n",
        "class NormalizeActionWrapper(gym.Wrapper):\n",
        "    \"\"\"A wrapper that maps actions from [-1,1] to [low, hgih].\"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.active = type(env.action_space) == gym.spaces.Box\n",
        "        if self.active:\n",
        "            self.action_low = env.action_space.low\n",
        "            self.action_high = env.action_space.high\n",
        "            self.action_scale = (self.action_high - self.action_low) * 0.5\n",
        "            self.action_mid = (self.action_high + self.action_low) * 0.5\n",
        "            print(\"Normalizing Action Space from [{}, {}] to [-1, 1]\".format(self.action_low[0], self.action_high[0]))\n",
        "    def step(self, action):\n",
        "        if self.active:\n",
        "            action = np.clip(action, -1, 1)\n",
        "            action = action * self.action_scale\n",
        "            action = action + self.action_mid\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "  # fre/common/utils\n",
        "  ###############################\n",
        "#\n",
        "#  Some shared utility functions\n",
        "#\n",
        "###############################\n",
        "\n",
        "import jax\n",
        "\n",
        "def supply_rng(f, rng=jax.random.PRNGKey(0)):\n",
        "    \"\"\"\n",
        "    Wraps a function to supply jax rng. It will remember the rng state for that function.\n",
        "    \"\"\"\n",
        "    def wrapped(*args, **kwargs):\n",
        "        nonlocal rng\n",
        "        rng, key = jax.random.split(rng)\n",
        "        return f(*args, seed=key, **kwargs)\n",
        "\n",
        "    return wrapped\n",
        "\n",
        "#fre/common/wandb\n",
        "\"\"\"WandB logging helpers.\n",
        "\n",
        "Run setup_wandb(hyperparam_dict, ...) to initialize wandb logging.\n",
        "See default_wandb_config() for a list of available configurations.\n",
        "\n",
        "We recommend the following workflow (see examples/mujoco/d4rl_iql.py for a more full example):\n",
        "\n",
        "    from ml_collections import config_flags\n",
        "    from jaxrl_m.wandb import setup_wandb, default_wandb_config\n",
        "    import wandb\n",
        "\n",
        "    # This line allows us to change wandb config flags from the command line\n",
        "    config_flags.DEFINE_config_dict('wandb', default_wandb_config(), lock_config=False)\n",
        "\n",
        "    ...\n",
        "    def main(argv):\n",
        "        hyperparams = ...\n",
        "        setup_wandb(hyperparams, **FLAGS.wandb)\n",
        "\n",
        "        # Log metrics as you wish now\n",
        "        wandb.log({'metric': 0.0}, step=0)\n",
        "\n",
        "\n",
        "With the following setup, you may set wandb configurations from the command line, e.g.\n",
        "    python main.py --wandb.project=my_project --wandb.group=my_group --wandb.offline\n",
        "\"\"\"\n",
        "#import wandb\n",
        "\n",
        "import tempfile\n",
        "import absl.flags as flags\n",
        "import ml_collections\n",
        "from  ml_collections.config_dict import FieldReference\n",
        "import datetime\n",
        "#import wandb\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def get_flag_dict():\n",
        "    flag_dict = {k: getattr(flags.FLAGS, k) for k in flags.FLAGS}\n",
        "    for k in flag_dict:\n",
        "        if isinstance(flag_dict[k], ml_collections.ConfigDict):\n",
        "            flag_dict[k] = flag_dict[k].to_dict()\n",
        "    return flag_dict\n",
        "\n",
        "\n",
        "def default_wandb_config():\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.offline = False  # Syncs online or not?\n",
        "    config.project = \"jaxrl_m\"  # WandB Project Name\n",
        "    config.entity = FieldReference(None, field_type=str)  # Which entity to log as (default: your own user)\n",
        "\n",
        "    group_name = FieldReference(None, field_type=str)  # Group name\n",
        "    config.exp_prefix = group_name  # Group name (deprecated, but kept for backwards compatibility)\n",
        "    config.group = group_name  # Group name\n",
        "\n",
        "    experiment_name = FieldReference(None, field_type=str) # Experiment name\n",
        "    config.name = experiment_name  # Run name (will be formatted with flags / variant)\n",
        "    config.exp_descriptor = experiment_name  # Run name (deprecated, but kept for backwards compatibility)\n",
        "\n",
        "    config.unique_identifier = \"\"  # Unique identifier for run (will be automatically generated unless provided)\n",
        "    config.random_delay = 0  # Random delay for wandb.init (in seconds)\n",
        "    return config\n",
        "\n",
        "\n",
        "def setup_wandb(\n",
        "    hyperparam_dict,\n",
        "    entity=None,\n",
        "    project=\"jaxrl_m\",\n",
        "    group=None,\n",
        "    name=None,\n",
        "    unique_identifier=\"\",\n",
        "    offline=False,\n",
        "    random_delay=0,\n",
        "    **additional_init_kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    Utility for setting up wandb logging (based on Young's simplesac):\n",
        "\n",
        "    Arguments:\n",
        "        - hyperparam_dict: dict of hyperparameters for experiment\n",
        "        - offline: bool, whether to sync online or not\n",
        "        - project: str, wandb project name\n",
        "        - entity: str, wandb entity name (default is your user)\n",
        "        - group: str, Group name for wandb\n",
        "        - name: str, Experiment name for wandb (formatted with FLAGS & hyperparameter_dict)\n",
        "        - unique_identifier: str, Unique identifier for wandb (default is timestamp)\n",
        "        - random_delay: float, Random delay for wandb.init (in seconds) to avoid collisions\n",
        "        - additional_init_kwargs: dict, additional kwargs to pass to wandb.init\n",
        "    Returns:\n",
        "        - wandb.run\n",
        "\n",
        "    \"\"\"\n",
        "    if \"exp_descriptor\" in additional_init_kwargs:\n",
        "        # Remove deprecated exp_descriptor\n",
        "        additional_init_kwargs.pop(\"exp_descriptor\")\n",
        "        additional_init_kwargs.pop(\"exp_prefix\")\n",
        "\n",
        "    if not unique_identifier:\n",
        "        if random_delay:\n",
        "            time.sleep(np.random.uniform(0, random_delay))\n",
        "        unique_identifier = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        unique_identifier += f\"_{np.random.randint(0, 1000000):06d}\"\n",
        "        flag_dict = get_flag_dict()\n",
        "        if 'seed' in flag_dict:\n",
        "            unique_identifier += f\"_{flag_dict['seed']:02d}\"\n",
        "\n",
        "    if name is not None:\n",
        "        name = name.format(**{**get_flag_dict(), **hyperparam_dict})\n",
        "\n",
        "    if group is not None and name is not None:\n",
        "        experiment_id = f\"{name}_{unique_identifier}\"\n",
        "    elif name is not None:\n",
        "        experiment_id = f\"{name}_{unique_identifier}\"\n",
        "    else:\n",
        "        experiment_id = None\n",
        "\n",
        "    # check if dir exists.\n",
        "    wandb_output_dir = tempfile.mkdtemp()\n",
        "    tags = [group] if group is not None else None\n",
        "\n",
        "    init_kwargs = dict(\n",
        "        config=hyperparam_dict,\n",
        "        project=project,\n",
        "        entity=entity,\n",
        "        tags=tags,\n",
        "        group=group,\n",
        "        dir=wandb_output_dir,\n",
        "        id=experiment_id,\n",
        "        name=name,\n",
        "        settings=wandb.Settings(\n",
        "            start_method=\"thread\",\n",
        "            _disable_stats=False,\n",
        "        ),\n",
        "        mode=\"offline\" if offline else \"online\",\n",
        "        save_code=True,\n",
        "    )\n",
        "\n",
        "    init_kwargs.update(additional_init_kwargs)\n",
        "    run = wandb.init(**init_kwargs)\n",
        "\n",
        "    wandb.config.update(get_flag_dict())\n",
        "\n",
        "    wandb_config = dict(\n",
        "        exp_prefix=group,\n",
        "        exp_descriptor=name,\n",
        "        experiment_id=experiment_id,\n",
        "    )\n",
        "    wandb.config.update(wandb_config)\n",
        "    return run\n",
        "\n",
        "#fre/common/envs/data_transforms\n",
        "\n",
        "###############################\n",
        "#\n",
        "#  Helpful utilities for processing actions, observations.\n",
        "#\n",
        "###############################\n",
        "\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "class ActionTransform():\n",
        "    pass\n",
        "\n",
        "class ActionDiscretizeBins(ActionTransform):\n",
        "    def __init__(self, bins_per_dim, action_dim):\n",
        "        self.bins_per_dim = bins_per_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.bins = np.linspace(-1, 1, bins_per_dim + 1)\n",
        "\n",
        "    # Assumes action is in [-1, 1].\n",
        "    def action_to_ids(self, action):\n",
        "        ids = np.digitize(action, self.bins) - 1\n",
        "        ids = np.clip(ids, 0, self.bins_per_dim - 1)\n",
        "        return ids\n",
        "\n",
        "    def ids_to_action(self, ids):\n",
        "        action = (self.bins[ids] + self.bins[ids + 1]) / 2\n",
        "        return action\n",
        "\n",
        "class ActionDiscretizeCluster(ActionTransform):\n",
        "    def __init__(self, num_clusters, data_actions):\n",
        "        self.num_clusters = num_clusters\n",
        "        assert len(data_actions.shape) == 2 # (data_size, action_dim)\n",
        "        print(\"Clustering actions of shape\", data_actions.shape)\n",
        "\n",
        "        # Cluster the data.\n",
        "        from sklearn.cluster import KMeans\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data_actions)\n",
        "        self.centers = kmeans.cluster_centers_\n",
        "        # self.labels = kmeans.labels_\n",
        "        self.centers = jnp.array(self.centers)\n",
        "        print(\"Average cluster error is\", kmeans.inertia_ / len(data_actions))\n",
        "        print(\"Average cluster error per dimension is\", (kmeans.inertia_ / len(data_actions)) / data_actions.shape[1])\n",
        "        # print(self.centers.shape)\n",
        "\n",
        "    def action_to_ids(self, action):\n",
        "        if len(action.shape) == 1:\n",
        "            action = action[None]\n",
        "        assert len(action.shape) == 2 # (batch, action_dim,)\n",
        "        # Find the closest cluster center.\n",
        "        dists = jnp.linalg.norm(self.centers[None] - action[:, None], axis=-1)\n",
        "        ids = jnp.argmin(dists, axis=-1)\n",
        "        return ids\n",
        "\n",
        "    def ids_to_action(self, ids):\n",
        "        action = self.centers[ids]\n",
        "        return action\n",
        "\n",
        "# Test\n",
        "# action_discretize_bins = ActionDiscretizeBins(32, 2)\n",
        "# action = np.array([-1, -0.999, -0.5, 0, 0.5, 0.999, 1])\n",
        "# ids = action_discretize_bins.action_to_ids(action)\n",
        "# print(ids)\n",
        "# action_recreate = action_discretize_bins.ids_to_action(ids)\n",
        "# print(action_recreate)\n",
        "# assert np.abs(action - action_recreate).max() < 0.1\n",
        "\n",
        "# action_discretize_cluster = ActionDiscretizeCluster(32, np.random.uniform(low=-1, high=1, size=(10000, 1)))\n",
        "# action = np.array([-1, -0.999, -0.5, 0, 0.5, 0.999, 1])[:, None] # [7, 1]\n",
        "# ids = action_discretize_cluster.action_to_ids(action)\n",
        "# print(ids)\n",
        "# action_recreate = action_discretize_cluster.ids_to_action(ids)\n",
        "# print(action_recreate)\n",
        "# assert np.abs(action - action_recreate).max() < 0.1\n",
        "\n",
        "\n",
        "#fre\\common\\envs\\env_helper\n",
        "\n",
        "\n",
        "###############################\n",
        "#\n",
        "#   Helper that initializes environments with the proper imports.\n",
        "#   Returns an environment that is:\n",
        "#   - Action normalized.\n",
        "#   - Video rendering works.\n",
        "#   - Episode monitor attached.\n",
        "#\n",
        "###############################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path as osp\n",
        "import gym\n",
        "import numpy as np\n",
        "import functools as ft\n",
        "#from fre.common.train_state import NormalizeActionWrapper\n",
        "#from fre.common.envs.wrappers import EpisodeMonitor\n",
        "\n",
        "\n",
        "# Supported envs:\n",
        "env_list = [\n",
        "    # From Gym\n",
        "    'HalfCheetah-v2',\n",
        "    'Hopper-v2',\n",
        "    'Walker2d-v2',\n",
        "    'Pendulum-v1',\n",
        "    'CartPole-v1',\n",
        "    'Acrobot_v1',\n",
        "    'MountainCar-v0',\n",
        "    'MountainCarContinuous-v0',\n",
        "    # From DMC\n",
        "    'pendulum_swingup',\n",
        "    'acrobot_swingup',\n",
        "    'acrobot_swingup_sparse',\n",
        "    'cartpole_swingup', # has exorl dataset.\n",
        "    'cartpole_swingup_sparse',\n",
        "    'pointmass_easy',\n",
        "    'reacher_easy',\n",
        "    'reacher_hard',\n",
        "    'cheetah_run',  # has exorl dataset.\n",
        "    'hopper_hop',\n",
        "    'walker_stand', # has exorl dataset.\n",
        "    'walker_walk', # has exorl dataset.\n",
        "    'walker_run', # has exorl dataset.\n",
        "    'quadruped_walk', # has exorl dataset.\n",
        "    'quadruped_run', # has exorl dataset.\n",
        "    'humanoid_stand',\n",
        "    'humanoid_run',\n",
        "    'jaco_reach_top_left', # has exorl dataset.\n",
        "    'jaco_reach_bottom_right', # has exorl dataset.\n",
        "    # TODO: Atari games\n",
        "    # Offline D4RL envs\n",
        "    'antmaze-large-diverse-v2', # Start in the corner, goal is in the top corner.\n",
        "    'gc-antmaze-large-diverse-v2', # Start in the corner, goal is in the top corner.\n",
        "    'center-antmaze-large-diverse-v2', # Start in the center, goal is UNDEFINED (this is for RARE rewards).\n",
        "    'maze2d-large-v1',\n",
        "    'gc-maze2d-large-v1',\n",
        "    'center-maze2d-large-v1',\n",
        "    # D4RL mujoco\n",
        "    'halfcheetah-expert-v2',\n",
        "    'walker2d-expert-v2',\n",
        "    'hopper-expert-v2',\n",
        "    'kitchen-complete-v0' # broken\n",
        "    'kitchen-mixed-v0' # broken\n",
        "]\n",
        "\n",
        "# Making an environment.\n",
        "def make_env(env_name, **kwargs):\n",
        "    if 'exorl' in env_name:\n",
        "        import os\n",
        "        os.environ['DISPLAY'] = ':0'\n",
        "        #import fre.common.envs.exorl.dmc as dmc\n",
        "        _, env_name, task_name = env_name.split('_', 2)\n",
        "        def make_env(env_name, task_name):\n",
        "            env = dmc.make(f'{env_name}_{task_name}', obs_type='states', frame_stack=1, action_repeat=1, seed=0)\n",
        "            env = dmc.DMCWrapper(env, 0)\n",
        "            return env\n",
        "        env = make_env(env_name, task_name)\n",
        "        env.reset()\n",
        "    elif '_' in env_name: # DMC Control\n",
        "        #import fre.common.envs.dmc as dmc2gym\n",
        "        import os\n",
        "        os.environ['DISPLAY'] = ':0'\n",
        "        suite, task = env_name.split('_', 1)\n",
        "        print(suite, task)\n",
        "        if suite == 'pointmass':\n",
        "            suite = 'point_mass'\n",
        "        frame_skip = kwargs['frame_skip'] if 'frame_skip' in kwargs else 2\n",
        "        visualize_reward = kwargs['visualize_reward'] if 'visualize_reward' in kwargs else False\n",
        "        env = dmc2gym.make(\n",
        "            domain_name=suite,\n",
        "            task_name=task, seed=1,\n",
        "            frame_skip=frame_skip,\n",
        "            visualize_reward=visualize_reward)\n",
        "        env = NormalizeActionWrapper(env)\n",
        "    elif 'antmaze' in env_name:\n",
        "        #from fre.common.envs.d4rl.d4rl_ant import CenteredMaze, GoalReachingMaze, MazeWrapper\n",
        "        if 'gc-antmaze' in env_name:\n",
        "            env = GoalReachingMaze('antmaze-large-diverse-v2')\n",
        "        elif 'center-antmaze' in env_name:\n",
        "            env = CenteredMaze('antmaze-large-diverse-v2')\n",
        "        else:\n",
        "            env = MazeWrapper('antmaze-large-diverse-v2')\n",
        "    elif 'maze2d' in env_name:\n",
        "        #from fre.common.envs.d4rl.d4rl_ant import CenteredMaze, GoalReachingMaze, MazeWrapper\n",
        "        if 'gc-maze2d' in env_name:\n",
        "            env = GoalReachingMaze('maze2d-large-v1')\n",
        "        elif 'center-maze2d' in env_name:\n",
        "            env = CenteredMaze('maze2d-large-v1')\n",
        "        else:\n",
        "            env = CenteredMaze('maze2d-large-v1', start_loc='original')\n",
        "    elif 'halfcheetah-' in env_name or 'walker2d-' in env_name or 'hopper-' in env_name: # D4RL Mujoco\n",
        "        #import d4rl\n",
        "        #import d4rl.gym_mujoco\n",
        "        env = gym.make(env_name)\n",
        "    elif 'kitchen' in env_name: # This doesn't work yet.\n",
        "        import os\n",
        "        os.environ['DISPLAY'] = ':0'\n",
        "        #from fre.common.envs.d4rl.d4rl_utils import KitchenRenderWrapper\n",
        "        env = KitchenRenderWrapper(gym.make(env_name))\n",
        "    elif 'bandit' in env_name:\n",
        "        #from fre.common.envs.bandit.bandit import BanditEnv\n",
        "        env = BanditEnv()\n",
        "    else:\n",
        "        env = gym.make(env_name)\n",
        "    env = EpisodeMonitor(env)\n",
        "    return env\n",
        "\n",
        "# For getting offline data.\n",
        "def get_dataset(env, env_name, **kwargs):\n",
        "    if 'exorl' in env_name:\n",
        "        #from fre.common.envs.exorl.exorl_utils import get_dataset\n",
        "        env_name_short = env_name.split('_', 1)[1]\n",
        "        return get_dataset(env, env_name_short, **kwargs)\n",
        "    elif 'ant' in env_name or 'maze2d' in env_name or 'kitchen' in env_name or 'halfcheetah' in env_name or 'walker2d' in env_name or 'hopper' in env_name:\n",
        "        #from fre.common.envs.d4rl.d4rl_utils import get_dataset, normalize_dataset\n",
        "        dataset = get_dataset(env, env_name, **kwargs)\n",
        "        dataset = normalize_dataset(env_name, dataset)\n",
        "        return dataset\n",
        "    elif 'cartpole' in env_name or 'cheetah' in env_name or 'jaco' in env_name or 'quadruped' in env_name or 'walker' in env_name:\n",
        "        #from fre.common.envs.exorl.exorl_utils import get_dataset\n",
        "        return get_dataset(env, env_name, **kwargs)\n",
        "\n",
        "def make_vec_env(env_name, num_envs, **kwargs):\n",
        "    from gym.vector import SyncVectorEnv\n",
        "    envs = [lambda : make_env(env_name, **kwargs) for _ in range(num_envs)]\n",
        "    env = SyncVectorEnv(envs)\n",
        "    return env\n",
        "\n",
        "#fre\\common\\env\\gc_utils\n",
        "\n",
        "#from fre.common.dataset import Dataset\n",
        "from flax.core.frozen_dict import FrozenDict\n",
        "from flax.core import freeze\n",
        "import dataclasses\n",
        "import numpy as np\n",
        "import jax\n",
        "import ml_collections\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class GCDataset:\n",
        "    dataset: Dataset\n",
        "    p_randomgoal: float\n",
        "    p_trajgoal: float\n",
        "    p_currgoal: float\n",
        "    geom_sample: int\n",
        "    discount: float\n",
        "    terminal_key: str = 'dones_float'\n",
        "    reward_scale: float = 1.0\n",
        "    reward_shift: float = -1.0\n",
        "    mask_terminal: int = 1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        return ml_collections.ConfigDict({\n",
        "            'p_randomgoal': 0.3,\n",
        "            'p_trajgoal': 0.5,\n",
        "            'p_currgoal': 0.2,\n",
        "            'geom_sample': 1,\n",
        "            'discount': 0.99,\n",
        "            'reward_scale': 1.0,\n",
        "            'reward_shift': -1.0,\n",
        "            'mask_terminal': 1,\n",
        "        })\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.terminal_locs, = np.nonzero(self.dataset[self.terminal_key] > 0)\n",
        "        assert np.isclose(self.p_randomgoal + self.p_trajgoal + self.p_currgoal, 1.0)\n",
        "\n",
        "    def sample_goals(self, indx, p_randomgoal=None, p_trajgoal=None, p_currgoal=None):\n",
        "        if p_randomgoal is None:\n",
        "            p_randomgoal = self.p_randomgoal\n",
        "        if p_trajgoal is None:\n",
        "            p_trajgoal = self.p_trajgoal\n",
        "        if p_currgoal is None:\n",
        "            p_currgoal = self.p_currgoal\n",
        "\n",
        "        batch_size = len(indx)\n",
        "        # Random goals\n",
        "        goal_indx = np.random.randint(self.dataset.size, size=batch_size)\n",
        "\n",
        "        # Goals from the same trajectory\n",
        "        final_state_indx = self.terminal_locs[np.searchsorted(self.terminal_locs, indx)]\n",
        "\n",
        "        distance = np.random.rand(batch_size)\n",
        "        if self.geom_sample:\n",
        "            us = np.random.rand(batch_size)\n",
        "            middle_goal_indx = np.minimum(indx + np.ceil(np.log(1 - us) / np.log(self.discount)).astype(int), final_state_indx)\n",
        "        else:\n",
        "            middle_goal_indx = np.round((np.minimum(indx + 1, final_state_indx) * distance + final_state_indx * (1 - distance))).astype(int)\n",
        "\n",
        "        goal_indx = np.where(np.random.rand(batch_size) < p_trajgoal / (1.0 - p_currgoal), middle_goal_indx, goal_indx)\n",
        "\n",
        "        # Goals at the current state\n",
        "        goal_indx = np.where(np.random.rand(batch_size) < p_currgoal, indx, goal_indx)\n",
        "\n",
        "        return goal_indx\n",
        "\n",
        "    def sample(self, batch_size: int, indx=None):\n",
        "        if indx is None:\n",
        "            indx = np.random.randint(self.dataset.size-1, size=batch_size)\n",
        "\n",
        "        batch = self.dataset.sample(batch_size, indx)\n",
        "        goal_indx = self.sample_goals(indx)\n",
        "\n",
        "        success = (indx == goal_indx)\n",
        "        batch['rewards'] = success.astype(float) * self.reward_scale + self.reward_shift\n",
        "        batch['goals'] = jax.tree_map(lambda arr: arr[goal_indx], self.dataset['observations'])\n",
        "\n",
        "        if self.mask_terminal:\n",
        "            batch['masks'] = 1.0 - success.astype(float)\n",
        "        else:\n",
        "            batch['masks'] = np.ones(batch_size)\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def sample_traj_random(self, batch_size, num_traj_states, num_random_states, num_random_states_decode):\n",
        "        indx = np.random.randint(self.dataset.size-1, size=batch_size)\n",
        "        batch = self.dataset.sample(batch_size, indx)\n",
        "        indx_expand = np.repeat(indx, num_traj_states-1) # (batch_size * num_traj_states)\n",
        "        traj_indx = self.sample_goals(indx_expand, p_randomgoal=0.0, p_trajgoal=1.0, p_currgoal=0.0)\n",
        "        traj_indx = traj_indx.reshape(batch_size, num_traj_states-1) # (batch_size, num_traj_states)\n",
        "        batch['traj_states'] = jax.tree_map(lambda arr: arr[traj_indx], self.dataset['observations'])\n",
        "        batch['traj_states'] = np.concatenate([batch['observations'][:,None,:], batch['traj_states']], axis=1)\n",
        "\n",
        "        rand_indx = np.random.randint(self.dataset.size-1, size=batch_size * num_random_states)\n",
        "        rand_indx = rand_indx.reshape(batch_size, num_random_states)\n",
        "        batch['random_states'] = jax.tree_map(lambda arr: arr[rand_indx], self.dataset['observations'])\n",
        "\n",
        "        rand_indx_decode = np.random.randint(self.dataset.size-1, size=batch_size * num_random_states_decode)\n",
        "        rand_indx_decode = rand_indx_decode.reshape(batch_size, num_random_states_decode)\n",
        "        batch['random_states_decode'] = jax.tree_map(lambda arr: arr[rand_indx_decode], self.dataset['observations'])\n",
        "        return batch\n",
        "\n",
        "def flatten_obgoal(obgoal):\n",
        "    return np.concatenate([obgoal['observation'], obgoal['goal']], axis=-1)\n",
        "\n",
        "#fre\\common\\envs\\gc_utils\n",
        "#from fre.common.dataset import Dataset\n",
        "from flax.core.frozen_dict import FrozenDict\n",
        "from flax.core import freeze\n",
        "import dataclasses\n",
        "import numpy as np\n",
        "import jax\n",
        "import ml_collections\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class GCDataset:\n",
        "    dataset: Dataset\n",
        "    p_randomgoal: float\n",
        "    p_trajgoal: float\n",
        "    p_currgoal: float\n",
        "    geom_sample: int\n",
        "    discount: float\n",
        "    terminal_key: str = 'dones_float'\n",
        "    reward_scale: float = 1.0\n",
        "    reward_shift: float = -1.0\n",
        "    mask_terminal: int = 1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        return ml_collections.ConfigDict({\n",
        "            'p_randomgoal': 0.3,\n",
        "            'p_trajgoal': 0.5,\n",
        "            'p_currgoal': 0.2,\n",
        "            'geom_sample': 1,\n",
        "            'discount': 0.99,\n",
        "            'reward_scale': 1.0,\n",
        "            'reward_shift': -1.0,\n",
        "            'mask_terminal': 1,\n",
        "        })\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.terminal_locs, = np.nonzero(self.dataset[self.terminal_key] > 0)\n",
        "        assert np.isclose(self.p_randomgoal + self.p_trajgoal + self.p_currgoal, 1.0)\n",
        "\n",
        "    def sample_goals(self, indx, p_randomgoal=None, p_trajgoal=None, p_currgoal=None):\n",
        "        if p_randomgoal is None:\n",
        "            p_randomgoal = self.p_randomgoal\n",
        "        if p_trajgoal is None:\n",
        "            p_trajgoal = self.p_trajgoal\n",
        "        if p_currgoal is None:\n",
        "            p_currgoal = self.p_currgoal\n",
        "\n",
        "        batch_size = len(indx)\n",
        "        # Random goals\n",
        "        goal_indx = np.random.randint(self.dataset.size, size=batch_size)\n",
        "\n",
        "        # Goals from the same trajectory\n",
        "        final_state_indx = self.terminal_locs[np.searchsorted(self.terminal_locs, indx)]\n",
        "\n",
        "        distance = np.random.rand(batch_size)\n",
        "        if self.geom_sample:\n",
        "            us = np.random.rand(batch_size)\n",
        "            middle_goal_indx = np.minimum(indx + np.ceil(np.log(1 - us) / np.log(self.discount)).astype(int), final_state_indx)\n",
        "        else:\n",
        "            middle_goal_indx = np.round((np.minimum(indx + 1, final_state_indx) * distance + final_state_indx * (1 - distance))).astype(int)\n",
        "\n",
        "        goal_indx = np.where(np.random.rand(batch_size) < p_trajgoal / (1.0 - p_currgoal), middle_goal_indx, goal_indx)\n",
        "\n",
        "        # Goals at the current state\n",
        "        goal_indx = np.where(np.random.rand(batch_size) < p_currgoal, indx, goal_indx)\n",
        "\n",
        "        return goal_indx\n",
        "\n",
        "    def sample(self, batch_size: int, indx=None):\n",
        "        if indx is None:\n",
        "            indx = np.random.randint(self.dataset.size-1, size=batch_size)\n",
        "\n",
        "        batch = self.dataset.sample(batch_size, indx)\n",
        "        goal_indx = self.sample_goals(indx)\n",
        "\n",
        "        success = (indx == goal_indx)\n",
        "        batch['rewards'] = success.astype(float) * self.reward_scale + self.reward_shift\n",
        "        batch['goals'] = jax.tree_map(lambda arr: arr[goal_indx], self.dataset['observations'])\n",
        "\n",
        "        if self.mask_terminal:\n",
        "            batch['masks'] = 1.0 - success.astype(float)\n",
        "        else:\n",
        "            batch['masks'] = np.ones(batch_size)\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def sample_traj_random(self, batch_size, num_traj_states, num_random_states, num_random_states_decode):\n",
        "        indx = np.random.randint(self.dataset.size-1, size=batch_size)\n",
        "        batch = self.dataset.sample(batch_size, indx)\n",
        "        indx_expand = np.repeat(indx, num_traj_states-1) # (batch_size * num_traj_states)\n",
        "        traj_indx = self.sample_goals(indx_expand, p_randomgoal=0.0, p_trajgoal=1.0, p_currgoal=0.0)\n",
        "        traj_indx = traj_indx.reshape(batch_size, num_traj_states-1) # (batch_size, num_traj_states)\n",
        "        batch['traj_states'] = jax.tree_map(lambda arr: arr[traj_indx], self.dataset['observations'])\n",
        "        batch['traj_states'] = np.concatenate([batch['observations'][:,None,:], batch['traj_states']], axis=1)\n",
        "\n",
        "        rand_indx = np.random.randint(self.dataset.size-1, size=batch_size * num_random_states)\n",
        "        rand_indx = rand_indx.reshape(batch_size, num_random_states)\n",
        "        batch['random_states'] = jax.tree_map(lambda arr: arr[rand_indx], self.dataset['observations'])\n",
        "\n",
        "        rand_indx_decode = np.random.randint(self.dataset.size-1, size=batch_size * num_random_states_decode)\n",
        "        rand_indx_decode = rand_indx_decode.reshape(batch_size, num_random_states_decode)\n",
        "        batch['random_states_decode'] = jax.tree_map(lambda arr: arr[rand_indx_decode], self.dataset['observations'])\n",
        "        return batch\n",
        "\n",
        "def flatten_obgoal(obgoal):\n",
        "    return np.concatenate([obgoal['observation'], obgoal['goal']], axis=-1)\n",
        "\n",
        "\n",
        "#fre/common/envs/wrapper\n",
        "###############################\n",
        "#\n",
        "#  Wrappers on top of gym environments\n",
        "#\n",
        "###############################\n",
        "\n",
        "from typing import Dict\n",
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class EpisodeMonitor(gym.ActionWrapper):\n",
        "    \"\"\"A class that computes episode returns and lengths.\"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        self._reset_stats()\n",
        "        self.total_timesteps = 0\n",
        "\n",
        "    def _reset_stats(self):\n",
        "        self.reward_sum = 0.0\n",
        "        self.episode_length = 0\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def step(self, action: np.ndarray):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "\n",
        "        self.reward_sum += reward\n",
        "        self.episode_length += 1\n",
        "        self.total_timesteps += 1\n",
        "        info[\"total\"] = {\"timesteps\": self.total_timesteps}\n",
        "\n",
        "        if done:\n",
        "            info[\"episode\"] = {}\n",
        "            info[\"episode\"][\"return\"] = self.reward_sum\n",
        "            info[\"episode\"][\"length\"] = self.episode_length\n",
        "            info[\"episode\"][\"duration\"] = time.time() - self.start_time\n",
        "\n",
        "            if hasattr(self, \"get_normalized_score\"):\n",
        "                info[\"episode\"][\"normalized_return\"] = (\n",
        "                    self.get_normalized_score(info[\"episode\"][\"return\"]) * 100.0\n",
        "                )\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs) -> np.ndarray:\n",
        "        self._reset_stats()\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class RewardOverride(gym.ActionWrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        self.reward_fn = None\n",
        "\n",
        "    def step(self, action: np.ndarray):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "\n",
        "        if self.env.observation_space.shape[0] == 24:\n",
        "            horizontal_velocity = self.env.physics.horizontal_velocity()\n",
        "            torso_upright = self.env.physics.torso_upright()\n",
        "            torso_height = self.env.physics.torso_height()\n",
        "            aux = np.array([horizontal_velocity, torso_upright, torso_height])\n",
        "            observation_aux = np.concatenate([observation, aux])\n",
        "            reward = self.reward_fn(observation_aux)\n",
        "        elif self.env.observation_space.shape[0] == 17:\n",
        "            horizontal_velocity = self.env.physics.speed()\n",
        "            aux = np.array([horizontal_velocity])\n",
        "            observation_aux = np.concatenate([observation, aux])\n",
        "            reward = self.reward_fn(observation_aux)\n",
        "        else:\n",
        "            reward = self.reward_fn(observation)\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs) -> np.ndarray:\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class TruncateObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env: gym.Env, truncate_size: int):\n",
        "        super().__init__(env)\n",
        "        self.truncate_size = truncate_size\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        return observation[:self.truncate_size]\n",
        "\n",
        "class GoalWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__(env)\n",
        "        self.custom_goal = None\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        if self.custom_goal is not None:\n",
        "            return np.concatenate([observation, self.custom_goal])\n",
        "        else:\n",
        "            return observation\n",
        "\n",
        "#fre\\common\\envs\\bandit\\bandit\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Here's a super simple bandit environment that follows the OpenAI Gym API.\n",
        "# There is one continuous action. The observation is always zero.\n",
        "# A reward of 1 is given if the action is either 0.5 or -0.5.\n",
        "\n",
        "class BanditEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,))\n",
        "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(1,))\n",
        "        self._state = None\n",
        "        self.width = 0.15\n",
        "\n",
        "    def reset(self):\n",
        "        self._state = np.zeros(1)\n",
        "        return self._state\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        if (np.abs(action[0] - 0.5) < self.width) or (np.abs(action[0] + 0.5) < self.width):\n",
        "            reward = 1\n",
        "        self.last_action = action\n",
        "        return self._state, reward, True, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # Render the last action on a line. Also indicate where the reward is. Return this as a numpy array.\n",
        "        img = np.ones((20, 100, 3), dtype=np.uint8) * 255\n",
        "        # Render reward zones in green. 0-100 means actions between -1 and 1.\n",
        "        center_low = 25\n",
        "        center_high = 75\n",
        "        width_int = int(self.width * 50)\n",
        "        img[:, center_low-width_int:center_low+width_int, :] = [0, 255, 0]\n",
        "        img[:, center_high-width_int:center_high+width_int, :] = [0, 255, 0]\n",
        "        # Render the last action in red.\n",
        "        action = self.last_action[0]\n",
        "        action = int((action + 1) * 50)\n",
        "        img[:, action:action+1, :] = [255, 0, 0]\n",
        "        return img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "#fre/common/envs/d4rl/antmaze_actions.npy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load the .npy file\n",
        "file_path = '/content/antmaze_actions.npy'  # Replace with the actual path to your .npy file\n",
        "loaded_array = np.load(file_path)\n",
        "\n",
        "# Now you can use loaded_array in your code\n",
        "#print(loaded_array)\n",
        "\n",
        "#fre/common/envs/d4rl/d4rl_ant\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib import patches\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from functools import partial\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import gym\n",
        "#import d4rl\n",
        "import numpy as np\n",
        "import functools as ft\n",
        "import math\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "#from fre.common.envs.gc_utils import GCDataset\n",
        "\n",
        "class MazeWrapper(gym.Wrapper):\n",
        "    def __init__(self, env_name):\n",
        "        self.env = gym.make(env_name)\n",
        "        self.env.render(mode='rgb_array', width=200, height=200)\n",
        "        self.env_name = env_name\n",
        "        self.inner_env = get_inner_env(self.env)\n",
        "        if 'antmaze' in env_name:\n",
        "            if 'medium' in env_name:\n",
        "                self.env.viewer.cam.lookat[0] = 10\n",
        "                self.env.viewer.cam.lookat[1] = 10\n",
        "                self.env.viewer.cam.distance = 40\n",
        "                self.env.viewer.cam.elevation = -90\n",
        "            elif 'umaze' in env_name:\n",
        "                self.env.viewer.cam.lookat[0] = 4\n",
        "                self.env.viewer.cam.lookat[1] = 4\n",
        "                self.env.viewer.cam.distance = 30\n",
        "                self.env.viewer.cam.elevation = -90\n",
        "            elif 'large' in env_name:\n",
        "                self.env.viewer.cam.lookat[0] = 18\n",
        "                self.env.viewer.cam.lookat[1] = 13\n",
        "                self.env.viewer.cam.distance = 55\n",
        "                self.env.viewer.cam.elevation = -90\n",
        "            self.inner_env.goal_sampler = ft.partial(valid_goal_sampler, self.inner_env)\n",
        "        elif 'maze2d' in env_name:\n",
        "            if 'open' in env_name:\n",
        "                pass\n",
        "            elif 'large' in env_name:\n",
        "                self.env.viewer.cam.lookat[0] = 5\n",
        "                self.env.viewer.cam.lookat[1] = 6.5\n",
        "                self.env.viewer.cam.distance = 15\n",
        "                self.env.viewer.cam.elevation = -90\n",
        "                self.env.viewer.cam.azimuth = 180\n",
        "            self.draw_ant_maze = get_inner_env(gym.make('antmaze-large-diverse-v2'))\n",
        "        self.action_space = self.env.action_space\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        img = self.env.render(*args, **kwargs)\n",
        "        if 'maze2d' in self.env_name:\n",
        "            img = img[::-1]\n",
        "        return img\n",
        "\n",
        "    # ======== BELOW is helper stuff for drawing and visualizing ======== #\n",
        "\n",
        "    def get_starting_boundary(self):\n",
        "        if 'antmaze' in self.env_name:\n",
        "            self = self.inner_env\n",
        "        else:\n",
        "            self = self.draw_ant_maze\n",
        "        torso_x, torso_y = self._init_torso_x, self._init_torso_y\n",
        "        S =  self._maze_size_scaling\n",
        "        return (0 - S / 2 + S - torso_x, 0 - S/2 + S - torso_y), (len(self._maze_map[0]) * S - torso_x - S/2 - S, len(self._maze_map) * S - torso_y - S/2 - S)\n",
        "\n",
        "    def XY(self, n=20, m=30):\n",
        "        bl, tr = self.get_starting_boundary()\n",
        "        X = np.linspace(bl[0] + 0.04 * (tr[0] - bl[0]) , tr[0] - 0.04 * (tr[0] - bl[0]), m)\n",
        "        Y = np.linspace(bl[1] + 0.04 * (tr[1] - bl[1]) , tr[1] - 0.04 * (tr[1] - bl[1]), n)\n",
        "\n",
        "        X,Y = np.meshgrid(X,Y)\n",
        "        states = np.array([X.flatten(), Y.flatten()]).T\n",
        "        return states\n",
        "\n",
        "    def four_goals(self):\n",
        "        self = self.inner_env\n",
        "\n",
        "        valid_cells = []\n",
        "        goal_cells = []\n",
        "\n",
        "        for i in range(len(self._maze_map)):\n",
        "            for j in range(len(self._maze_map[0])):\n",
        "                if self._maze_map[i][j] in [0, 'r', 'g']:\n",
        "                    valid_cells.append(self._rowcol_to_xy((i, j), add_random_noise=False))\n",
        "\n",
        "        goals = []\n",
        "        goals.append(max(valid_cells, key=lambda x: -x[0]-x[1]))\n",
        "        goals.append(max(valid_cells, key=lambda x: x[0]-x[1]))\n",
        "        goals.append(max(valid_cells, key=lambda x: x[0]+x[1]))\n",
        "        goals.append(max(valid_cells, key=lambda x: -x[0] + x[1]))\n",
        "        return goals\n",
        "\n",
        "    def draw(self, ax=None, scale=1.0):\n",
        "        if not ax: ax = plt.gca()\n",
        "        if 'antmaze' in self.env_name:\n",
        "            self = self.inner_env\n",
        "        else:\n",
        "            self = self.draw_ant_maze\n",
        "        torso_x, torso_y = self._init_torso_x, self._init_torso_y\n",
        "        S =  self._maze_size_scaling\n",
        "        if scale < 1.0:\n",
        "            S *= 0.965\n",
        "            torso_x -= 0.7\n",
        "            torso_y -= 0.95\n",
        "        for i in range(len(self._maze_map)):\n",
        "            for j in range(len(self._maze_map[0])):\n",
        "                struct = self._maze_map[i][j]\n",
        "                if struct == 1:\n",
        "                    rect = patches.Rectangle((j *S - torso_x - S/ 2,\n",
        "                                            i * S- torso_y - S/ 2),\n",
        "                                            S,\n",
        "                                            S, linewidth=1, edgecolor='none', facecolor='grey', alpha=1.0)\n",
        "\n",
        "                    ax.add_patch(rect)\n",
        "        ax.set_xlim(0 - S /2 + 0.6 * S - torso_x, len(self._maze_map[0]) * S - torso_x - S/2 - S * 0.6)\n",
        "        ax.set_ylim(0 - S/2 + 0.6 * S - torso_y, len(self._maze_map) * S - torso_y - S/2 - S * 0.6)\n",
        "        ax.axis('off')\n",
        "\n",
        "class CenteredMaze(MazeWrapper):\n",
        "    start_loc: str = \"center\"\n",
        "\n",
        "    def __init__(self, env_name, start_loc=\"center\"):\n",
        "        super().__init__(env_name)\n",
        "        self.start_loc = start_loc\n",
        "        self.t = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        next_obs, r, done, info = self.env.step(action)\n",
        "        if 'antmaze' in self.env_name:\n",
        "            info['x'], info['y'] = self.get_xy()\n",
        "        self.t += 1\n",
        "        done = self.t >= 2000\n",
        "        return next_obs, r, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.t = 0\n",
        "        obs = self.env.reset(**kwargs)\n",
        "        if 'maze2d' in self.env_name:\n",
        "            if self.start_loc == 'center' or self.start_loc == 'center2':\n",
        "                obs = self.env.reset_to_location([4, 5.8])\n",
        "            elif self.start_loc == 'original':\n",
        "                obs = self.env.reset_to_location([0.9, 0.9])\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "        elif 'antmaze' in self.env_name:\n",
        "            if self.start_loc == 'center' or self.start_loc == 'center2':\n",
        "                self.env.set_xy([20, 15])\n",
        "                obs[:2] = [20, 15]\n",
        "            elif self.start_loc == 'original':\n",
        "                pass\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "        return obs\n",
        "\n",
        "class GoalReachingMaze(MazeWrapper):\n",
        "    def __init__(self, env_name):\n",
        "        super().__init__(env_name)\n",
        "        self.observation_space = gym.spaces.Dict({\n",
        "            'observation': self.env.observation_space,\n",
        "            'goal': self.env.observation_space,\n",
        "        })\n",
        "\n",
        "    def step(self, action):\n",
        "        next_obs, r, done, info = self.env.step(action)\n",
        "\n",
        "        if 'antmaze' in self.env_name:\n",
        "            achieved = self.get_xy()\n",
        "            desired = self.target_goal\n",
        "        elif 'maze2d' in self.env_name:\n",
        "            achieved = next_obs[:2]\n",
        "            desired = self.env.get_target()\n",
        "        distance = np.linalg.norm(achieved - desired)\n",
        "        info['x'], info['y'] = achieved\n",
        "        info['achieved_goal'] = np.array(achieved)\n",
        "        info['desired_goal'] = np.copy(desired)\n",
        "        info['success'] = float(distance < 0.5)\n",
        "        done = 'TimeLimit.truncated' in info or info['success']\n",
        "\n",
        "        return self.get_obs(next_obs), r, done, info\n",
        "\n",
        "    def get_obs(self, obs):\n",
        "        if 'antmaze' in self.env_name:\n",
        "            desired = self.target_goal\n",
        "        elif 'maze2d' in self.env_name:\n",
        "            desired = self.env.get_target()\n",
        "        target_goal = obs.copy()\n",
        "        target_goal[:2] = desired\n",
        "        if 'antmaze' in self.env_name:\n",
        "            obs = discretize_obs(obs)\n",
        "            target_goal = discretize_obs(target_goal)\n",
        "        return dict(observation=obs, goal=target_goal)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs = self.env.reset(**kwargs)\n",
        "        if 'maze2d' in self.env_name:\n",
        "            obs = self.env.reset_to_location([0.9, 0.9])\n",
        "        return self.get_obs(obs)\n",
        "\n",
        "    def get_normalized_score(self, score):\n",
        "        return score\n",
        "\n",
        "# ===================================\n",
        "# HELPER FUNCTIONS FOR OB DISCRETIZATION\n",
        "# ===================================\n",
        "\n",
        "def discretize_obs(ob, num_bins=32, disc_type='tanh', disc_temperature=1.0):\n",
        "    min_ob = np.array([0, 0])\n",
        "    max_ob = np.array([35, 35])\n",
        "    disc_dims = 2\n",
        "    bins = np.linspace(min_ob, max_ob, num_bins).T # [num_bins,] values from min_ob to max_ob\n",
        "    bin_size = (max_ob - min_ob) / num_bins\n",
        "    if disc_type == 'twohot':\n",
        "        raise NotImplementedError\n",
        "    elif disc_type == 'tanh':\n",
        "        orig_ob = ob\n",
        "        ob = np.expand_dims(ob, -1)\n",
        "        # Convert each discretized dimension into num_bins dimensions. Value of each dimension is tanh of the distance from the bin center.\n",
        "        bin_diff = ob[..., :disc_dims, :] - bins[:disc_dims]\n",
        "        bin_diff_normalized = bin_diff / np.expand_dims(bin_size[:disc_dims], -1) * disc_temperature\n",
        "        bin_tanh = np.tanh(bin_diff_normalized).reshape(*orig_ob.shape[:-1], -1)\n",
        "        disc_ob = np.concatenate([bin_tanh, orig_ob[..., disc_dims:]], axis=-1)\n",
        "        return disc_ob\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "# ===================================\n",
        "# HELPER FUNCTIONS FOR VISUALIZATION\n",
        "# ===================================\n",
        "\n",
        "def get_canvas_image(canvas):\n",
        "    canvas.draw()\n",
        "    out_image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
        "    out_image = out_image.reshape(canvas.get_width_height()[::-1] + (3,))\n",
        "    return out_image\n",
        "\n",
        "def valid_goal_sampler(self, np_random):\n",
        "    valid_cells = []\n",
        "    goal_cells = []\n",
        "    # print('Hello')\n",
        "\n",
        "    for i in range(len(self._maze_map)):\n",
        "      for j in range(len(self._maze_map[0])):\n",
        "        if self._maze_map[i][j] in [0, 'r', 'g']:\n",
        "          valid_cells.append((i, j))\n",
        "\n",
        "    # If there is a 'goal' designated, use that. Otherwise, any valid cell can\n",
        "    # be a goal.\n",
        "    sample_choices = valid_cells\n",
        "    cell = sample_choices[np_random.choice(len(sample_choices))]\n",
        "    xy = self._rowcol_to_xy(cell, add_random_noise=True)\n",
        "\n",
        "    random_x = np.random.uniform(low=0, high=0.5) * 0.25 * self._maze_size_scaling\n",
        "    random_y = np.random.uniform(low=0, high=0.5) * 0.25 * self._maze_size_scaling\n",
        "\n",
        "    xy = (max(xy[0] + random_x, 0), max(xy[1] + random_y, 0))\n",
        "\n",
        "    return xy\n",
        "\n",
        "\n",
        "def get_inner_env(env):\n",
        "    if hasattr(env, '_maze_size_scaling'):\n",
        "        return env\n",
        "    elif hasattr(env, 'env'):\n",
        "        return get_inner_env(env.env)\n",
        "    elif hasattr(env, 'wrapped_env'):\n",
        "        return get_inner_env(env.wrapped_env)\n",
        "    return env\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# PLOT VALUE FUNCTION\n",
        "# ===================================\n",
        "\n",
        "def value_image(env, dataset, value_fn):\n",
        "    \"\"\"\n",
        "    Visualize the value function.\n",
        "    Args:\n",
        "        env: The environment.\n",
        "        value_fn: a function with signature value_fn([# states, state_dim]) -> [#states, 1]\n",
        "    Returns:\n",
        "        A numpy array of the image.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(2, 2, tight_layout=True)\n",
        "    axs_flat = axs.flatten()\n",
        "    canvas = FigureCanvas(fig)\n",
        "    if type(dataset) is GCDataset:\n",
        "        dataset = dataset.dataset\n",
        "    if 'antmaze' in env.env_name:\n",
        "        goals = env.four_goals()\n",
        "        goal_states = dataset['observations'][0]\n",
        "        goal_states = goal_states[-29:] # Remove discretized observations.\n",
        "        goal_states = np.tile(goal_states, (len(goals), 1))\n",
        "        goal_states[:, :2] = goals\n",
        "        goal_states = discretize_obs(goal_states)\n",
        "    elif 'maze2d' in env.env_name:\n",
        "        goals = np.array([[0.8, 0.8], [1, 9.7], [6.8, 9], [6.8, 1]])\n",
        "        goal_states = dataset['observations'][0]\n",
        "        goal_states = np.tile(goal_states, (len(goals), 1))\n",
        "        goal_states[:, :2] = goals\n",
        "    for i in range(4):\n",
        "        plot_value(goal_states[i], env, dataset, value_fn, axs_flat[i])\n",
        "    image = get_canvas_image(canvas)\n",
        "    plt.close(fig)\n",
        "    return image\n",
        "\n",
        "def plot_value(goal_observation, env, dataset, value_fn, ax):\n",
        "    N = 14\n",
        "    M = 20\n",
        "    ob_xy = env.XY(n=N, m=M)\n",
        "\n",
        "    goal_observation = np.tile(goal_observation, (ob_xy.shape[0], 1)) # (N*M, 29)\n",
        "\n",
        "    base_observation = np.copy(dataset['observations'][0])\n",
        "    xy_observations = np.tile(base_observation, (ob_xy.shape[0], 1)) # (N*M, 29)\n",
        "    if 'antmaze' in env.env_name:\n",
        "        xy_observations = xy_observations[:, -29:] # Remove discretized observations.\n",
        "        xy_observations[:, :2] = ob_xy # Set to XY.\n",
        "        xy_observations = discretize_obs(xy_observations) # Discretize again.\n",
        "        assert xy_observations.shape[1] == 91\n",
        "    elif 'maze2d' in env.env_name:\n",
        "        ob_xy_scaled = ob_xy / 3.5\n",
        "        ob_xy_scaled = ob_xy_scaled[:, [1, 0]]\n",
        "        xy_observations[:, :2] = ob_xy_scaled\n",
        "        assert xy_observations.shape[1] == 4 # (x, y, vx, vy)\n",
        "    values = value_fn(xy_observations, goal_observation) # (N*M, 1)\n",
        "\n",
        "    x, y = ob_xy[:, 0], ob_xy[:, 1]\n",
        "    x = x.reshape(N, M)\n",
        "    y = y.reshape(N, M) * 0.975 + 0.7\n",
        "    values = values.reshape(N, M)\n",
        "    mesh = ax.pcolormesh(x, y, values, cmap='viridis')\n",
        "\n",
        "    env.draw(ax, scale=0.95)\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# PLOT TRAJECTORIES\n",
        "# ===================================\n",
        "\n",
        "# Makes an image of the trajectory the Ant follows.\n",
        "def trajectory_image(env, trajectories, **kwargs):\n",
        "    fig = plt.figure(tight_layout=True)\n",
        "    canvas = FigureCanvas(fig)\n",
        "\n",
        "    plot_trajectories(env, trajectories, fig, plt.gca(), **kwargs)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    image = get_canvas_image(canvas)\n",
        "    plt.close(fig)\n",
        "    return image\n",
        "\n",
        "# Helper that plots the XY coordinates as scatter plots.\n",
        "def plot_trajectories(env, trajectories, fig, ax, color_list=None):\n",
        "    if color_list is None:\n",
        "        from itertools import cycle\n",
        "        color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "        color_list = cycle(color_cycle)\n",
        "\n",
        "    for color, trajectory in zip(color_list, trajectories):\n",
        "        obs = np.array(trajectory['observation'])\n",
        "\n",
        "        # convert back to xy?\n",
        "        if 'ant' in env.env_name:\n",
        "            all_x = []\n",
        "            all_y = []\n",
        "            for info in trajectory['info']:\n",
        "                all_x.append(info['x'])\n",
        "                all_y.append(info['y'])\n",
        "            all_x = np.array(all_x)\n",
        "            all_y = np.array(all_y)\n",
        "        elif 'maze2d' in env.env_name:\n",
        "            all_x = obs[:, 1] * 4 - 3.2\n",
        "            all_y = obs[:, 0] * 4 - 3.2\n",
        "        ax.scatter(all_x, all_y, s=5, c=color, alpha=0.2)\n",
        "        ax.scatter(all_x[-1], all_y[-1], s=50, c=color, marker='*', alpha=1, edgecolors='black')\n",
        "\n",
        "    env.draw(ax)\n",
        "\n",
        "\n",
        "#fre/common/envs/d4rl/d4rl_utils\n",
        "\"\"\"import d4rl\n",
        "import d4rl.gym_mujoco\"\"\"\n",
        "import gym\n",
        "import numpy as np\n",
        "from jax import tree_util\n",
        "\n",
        "\n",
        "\"\"\"import fre.common.envs.d4rl.d4rl_ant as d4rl_ant\n",
        "from fre.common.dataset import Dataset\n",
        "\"\"\"\n",
        "\n",
        "# Note on AntMaze. Reward = 1 at the goal, and Terminal = 1 at the goal.\n",
        "# Masks = Does the episode end due to final state?\n",
        "# Dones_float = Does the episode end due to time limit? OR does the episode end due to final state?\n",
        "def get_dataset(env: gym.Env, env_name: str, clip_to_eps: bool = True,\n",
        "                eps: float = 1e-5, dataset=None, filter_terminals=False, obs_dtype=np.float32):\n",
        "    if dataset is None:\n",
        "        dataset = d4rl.qlearning_dataset(env)\n",
        "\n",
        "    if clip_to_eps:\n",
        "        lim = 1 - eps\n",
        "        dataset['actions'] = np.clip(dataset['actions'], -lim, lim)\n",
        "\n",
        "    # Mask everything that is marked as a terminal state.\n",
        "    # For AntMaze, this should mask the end of each trajectory.\n",
        "    masks = 1.0 - dataset['terminals']\n",
        "\n",
        "    # In the AntMaze data, terminal is 1 when at the goal. But the episode doesn't end.\n",
        "    # This just ensures that we treat AntMaze trajectories as non-ending.\n",
        "    if \"antmaze\" in env_name or \"maze2d\" in env_name:\n",
        "        dataset['terminals'] = np.zeros_like(dataset['terminals'])\n",
        "\n",
        "    # if 'antmaze' in env_name:\n",
        "    #     print(\"Discretizing AntMaze observations.\")\n",
        "    #     print(\"Raw observations looks like\", dataset['observations'].shape[1:])\n",
        "    #     dataset['observations'] = d4rl_ant.discretize_obs(dataset['observations'])\n",
        "    #     dataset['next_observations'] = d4rl_ant.discretize_obs(dataset['next_observations'])\n",
        "    #     print(\"Discretized observations looks like\", dataset['observations'].shape[1:])\n",
        "\n",
        "    # Compute dones if terminal OR orbservation jumps.\n",
        "    dones_float = np.zeros_like(dataset['rewards'])\n",
        "\n",
        "    imputed_next_observations = np.roll(dataset['observations'], -1, axis=0)\n",
        "    same_obs = np.all(np.isclose(imputed_next_observations, dataset['next_observations'], atol=1e-5), axis=-1)\n",
        "    dones_float = 1.0 - same_obs.astype(np.float32)\n",
        "    dones_float += dataset['terminals']\n",
        "    dones_float[-1] = 1.0\n",
        "    dones_float = np.clip(dones_float, 0.0, 1.0)\n",
        "\n",
        "    observations = dataset['observations'].astype(obs_dtype)\n",
        "    next_observations = dataset['next_observations'].astype(obs_dtype)\n",
        "\n",
        "    return Dataset.create(\n",
        "        observations=observations,\n",
        "        actions=dataset['actions'].astype(np.float32),\n",
        "        rewards=dataset['rewards'].astype(np.float32),\n",
        "        masks=masks.astype(np.float32),\n",
        "        dones_float=dones_float.astype(np.float32),\n",
        "        next_observations=next_observations,\n",
        "    )\n",
        "\n",
        "def get_normalization(dataset):\n",
        "    returns = []\n",
        "    ret = 0\n",
        "    for r, term in zip(dataset['rewards'], dataset['dones_float']):\n",
        "        ret += r\n",
        "        if term:\n",
        "            returns.append(ret)\n",
        "            ret = 0\n",
        "    return (max(returns) - min(returns)) / 1000\n",
        "\n",
        "def normalize_dataset(env_name, dataset):\n",
        "    print(\"Normalizing\", env_name)\n",
        "    if 'antmaze' in env_name or 'maze2d' in env_name:\n",
        "        return dataset.copy({'rewards': dataset['rewards']- 1.0})\n",
        "    else:\n",
        "        normalizing_factor = get_normalization(dataset)\n",
        "        print(f'Normalizing factor: {normalizing_factor}')\n",
        "        dataset = dataset.copy({'rewards': dataset['rewards'] / normalizing_factor})\n",
        "        return dataset\n",
        "\n",
        "# Flattens environment with a dictionary of observation,goal to a single concatenated observation.\n",
        "class GoalReachingFlat(gym.Wrapper):\n",
        "    \"\"\"A wrapper that maps actions from [-1,1] to [low, hgih].\"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(self.observation_space['observation'].shape[0] + self.observation_space['goal'].shape[0],), dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        ob_flat = np.concatenate([ob['observation'], ob['goal']])\n",
        "        return ob_flat, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        ob = self.env.reset(**kwargs)\n",
        "        ob_flat = np.concatenate([ob['observation'], ob['goal']])\n",
        "        return ob_flat\n",
        "\n",
        "def parse_trajectories(dataset):\n",
        "    trajectory_ids = np.where(dataset['dones_float'] == 1)[0] + 1\n",
        "    trajectory_ids = np.concatenate([[0], trajectory_ids])\n",
        "    num_trajectories = trajectory_ids.shape[0] - 1\n",
        "    print(\"There are {} trajectories. Some traj lens are {}\".format(num_trajectories, [trajectory_ids[i + 1] - trajectory_ids[i] for i in range(min(5, num_trajectories))]))\n",
        "    trajectories = []\n",
        "    for i in range(len(trajectory_ids) - 1):\n",
        "        trajectories.append(tree_util.tree_map(lambda arr: arr[trajectory_ids[i]:trajectory_ids[i + 1]], dataset._dict))\n",
        "    return trajectories\n",
        "\n",
        "class KitchenRenderWrapper(gym.Wrapper):\n",
        "    def render(self, *args, **kwargs):\n",
        "        from dm_control.mujoco import engine\n",
        "        camera = engine.MovableCamera(self.sim, 1920, 2560)\n",
        "        camera.set_pose(distance=2.2, lookat=[-0.2, .5, 2.], azimuth=70, elevation=-35)\n",
        "        img = camera.render()\n",
        "        return img\n",
        "\n",
        "#fre/common/envs/dmc/__init__.py\n",
        "\n",
        "import gym\n",
        "from gym.envs.registration import register\n",
        "\n",
        "\n",
        "def make(\n",
        "        domain_name,\n",
        "        task_name,\n",
        "        seed=1,\n",
        "        visualize_reward=True,\n",
        "        from_pixels=False,\n",
        "        height=84,\n",
        "        width=84,\n",
        "        camera_id=0,\n",
        "        frame_skip=1,\n",
        "        episode_length=1000,\n",
        "        environment_kwargs=None,\n",
        "        time_limit=None,\n",
        "        channels_first=True\n",
        "):\n",
        "    env_id = 'dmc_%s_%s_%s-v1' % (domain_name, task_name, seed)\n",
        "\n",
        "    if from_pixels:\n",
        "        assert not visualize_reward, 'cannot use visualize reward when learning from pixels'\n",
        "\n",
        "    # shorten episode length\n",
        "    max_episode_steps = (episode_length + frame_skip - 1) // frame_skip\n",
        "\n",
        "    if not env_id in gym.envs.registry.env_specs:\n",
        "        task_kwargs = {}\n",
        "        if seed is not None:\n",
        "            task_kwargs['random'] = seed\n",
        "        if time_limit is not None:\n",
        "            task_kwargs['time_limit'] = time_limit\n",
        "        register(\n",
        "            id=env_id,\n",
        "            # entry_point='dmc2gym.wrappers:DMCWrapper',\n",
        "            entry_point='fre.common.envs.dmc.wrappers:DMCWrapper',\n",
        "            kwargs=dict(\n",
        "                domain_name=domain_name,\n",
        "                task_name=task_name,\n",
        "                task_kwargs=task_kwargs,\n",
        "                environment_kwargs=environment_kwargs,\n",
        "                visualize_reward=visualize_reward,\n",
        "                from_pixels=from_pixels,\n",
        "                height=height,\n",
        "                width=width,\n",
        "                camera_id=camera_id,\n",
        "                frame_skip=frame_skip,\n",
        "                channels_first=channels_first,\n",
        "            ),\n",
        "            max_episode_steps=max_episode_steps,\n",
        "        )\n",
        "    return gym.make(env_id)\n",
        "\n",
        "#fre/common/envs/dmc/jaco\n",
        "\"\"\"A task where the goal is to move the hand close to a target prop or site.\"\"\"\n",
        "\n",
        "import collections\n",
        "\n",
        "from dm_control import composer\n",
        "from dm_control.composer import initializers\n",
        "from dm_control.composer.variation import distributions\n",
        "from dm_control.entities import props\n",
        "from dm_control.manipulation.shared import arenas\n",
        "from dm_control.manipulation.shared import cameras\n",
        "from dm_control.manipulation.shared import constants\n",
        "from dm_control.manipulation.shared import observations\n",
        "from dm_control.manipulation.shared import robots\n",
        "from dm_control.manipulation.shared import workspaces\n",
        "from dm_control.utils import rewards\n",
        "from dm_env import specs\n",
        "import numpy as np\n",
        "\n",
        "_ReachWorkspace = collections.namedtuple(\n",
        "    '_ReachWorkspace', ['target_bbox', 'tcp_bbox', 'arm_offset'])\n",
        "\n",
        "# Ensures that the props are not touching the table before settling.\n",
        "_PROP_Z_OFFSET = 0.001\n",
        "\n",
        "_DUPLO_WORKSPACE = _ReachWorkspace(\n",
        "    target_bbox=workspaces.BoundingBox(lower=(-0.1, -0.1, _PROP_Z_OFFSET),\n",
        "                                       upper=(0.1, 0.1, _PROP_Z_OFFSET)),\n",
        "    tcp_bbox=workspaces.BoundingBox(lower=(-0.1, -0.1, 0.2),\n",
        "                                    upper=(0.1, 0.1, 0.4)),\n",
        "    arm_offset=robots.ARM_OFFSET)\n",
        "\n",
        "_SITE_WORKSPACE = _ReachWorkspace(\n",
        "    target_bbox=workspaces.BoundingBox(lower=(-0.2, -0.2, 0.02),\n",
        "                                       upper=(0.2, 0.2, 0.4)),\n",
        "    tcp_bbox=workspaces.BoundingBox(lower=(-0.2, -0.2, 0.02),\n",
        "                                    upper=(0.2, 0.2, 0.4)),\n",
        "    arm_offset=robots.ARM_OFFSET)\n",
        "\n",
        "_TARGET_RADIUS = 0.05\n",
        "_TIME_LIMIT = 10.\n",
        "\n",
        "TASKS = [('reach_top_left', np.array([-0.09, 0.09, _PROP_Z_OFFSET])),\n",
        "         ('reach_top_right', np.array([0.09, 0.09, _PROP_Z_OFFSET])),\n",
        "         ('reach_bottom_left', np.array([-0.09, -0.09, _PROP_Z_OFFSET])),\n",
        "         ('reach_bottom_right', np.array([0.09, -0.09, _PROP_Z_OFFSET]))]\n",
        "\n",
        "\n",
        "def make(task_id, obs_type, seed):\n",
        "    obs_settings = observations.VISION if obs_type == 'pixels' else observations.PERFECT_FEATURES\n",
        "    task = _reach(task_id, obs_settings=obs_settings, use_site=True)\n",
        "    return composer.Environment(task,\n",
        "                                time_limit=_TIME_LIMIT,\n",
        "                                random_state=seed)\n",
        "\n",
        "\n",
        "class MultiTaskReach(composer.Task):\n",
        "    \"\"\"Bring the hand close to a target prop or site.\"\"\"\n",
        "\n",
        "    def __init__(self, task_id, arena, arm, hand, prop, obs_settings,\n",
        "                 workspace, control_timestep):\n",
        "        \"\"\"Initializes a new `Reach` task.\n",
        "    Args:\n",
        "      arena: `composer.Entity` instance.\n",
        "      arm: `robot_base.RobotArm` instance.\n",
        "      hand: `robot_base.RobotHand` instance.\n",
        "      prop: `composer.Entity` instance specifying the prop to reach to, or None\n",
        "        in which case the target is a fixed site whose position is specified by\n",
        "        the workspace.\n",
        "      obs_settings: `observations.ObservationSettings` instance.\n",
        "      workspace: `_ReachWorkspace` specifying the placement of the prop and TCP.\n",
        "      control_timestep: Float specifying the control timestep in seconds.\n",
        "    \"\"\"\n",
        "        self._arena = arena\n",
        "        self._arm = arm\n",
        "        self._hand = hand\n",
        "        self._arm.attach(self._hand)\n",
        "        self._arena.attach_offset(self._arm, offset=workspace.arm_offset)\n",
        "        self.control_timestep = control_timestep\n",
        "        self._tcp_initializer = initializers.ToolCenterPointInitializer(\n",
        "            self._hand,\n",
        "            self._arm,\n",
        "            position=distributions.Uniform(*workspace.tcp_bbox),\n",
        "            quaternion=workspaces.DOWN_QUATERNION)\n",
        "\n",
        "        # Add custom camera observable.\n",
        "        self._task_observables = cameras.add_camera_observables(\n",
        "            arena, obs_settings, cameras.FRONT_CLOSE)\n",
        "\n",
        "        if task_id == 'reach_multitask':\n",
        "            self._targets = [target for (_, target) in TASKS]\n",
        "        else:\n",
        "            self._targets = [\n",
        "                target for (task, target) in TASKS if task == task_id\n",
        "            ]\n",
        "            assert len(self._targets) > 0\n",
        "\n",
        "        #target_pos_distribution = distributions.Uniform(*TASKS[task_id])\n",
        "        self._prop = prop\n",
        "        if prop:\n",
        "            # The prop itself is used to visualize the target location.\n",
        "            self._make_target_site(parent_entity=prop, visible=False)\n",
        "            self._target = self._arena.add_free_entity(prop)\n",
        "            self._prop_placer = initializers.PropPlacer(\n",
        "                props=[prop],\n",
        "                position=target_pos_distribution,\n",
        "                quaternion=workspaces.uniform_z_rotation,\n",
        "                settle_physics=True)\n",
        "        else:\n",
        "            if len(self._targets) == 1:\n",
        "                self._target = self._make_target_site(parent_entity=arena,\n",
        "                                                      visible=True)\n",
        "\n",
        "            #obs = observable.MJCFFeature('pos', self._target)\n",
        "            # obs.configure(**obs_settings.prop_pose._asdict())\n",
        "            #self._task_observables['target_position'] = obs\n",
        "\n",
        "        # Add sites for visualizing the prop and target bounding boxes.\n",
        "        workspaces.add_bbox_site(body=self.root_entity.mjcf_model.worldbody,\n",
        "                                 lower=workspace.tcp_bbox.lower,\n",
        "                                 upper=workspace.tcp_bbox.upper,\n",
        "                                 rgba=constants.GREEN,\n",
        "                                 name='tcp_spawn_area')\n",
        "        workspaces.add_bbox_site(body=self.root_entity.mjcf_model.worldbody,\n",
        "                                 lower=workspace.target_bbox.lower,\n",
        "                                 upper=workspace.target_bbox.upper,\n",
        "                                 rgba=constants.BLUE,\n",
        "                                 name='target_spawn_area')\n",
        "\n",
        "    def _make_target_site(self, parent_entity, visible):\n",
        "        return workspaces.add_target_site(\n",
        "            body=parent_entity.mjcf_model.worldbody,\n",
        "            radius=_TARGET_RADIUS,\n",
        "            visible=visible,\n",
        "            rgba=constants.RED,\n",
        "            name='target_site')\n",
        "\n",
        "    @property\n",
        "    def root_entity(self):\n",
        "        return self._arena\n",
        "\n",
        "    @property\n",
        "    def arm(self):\n",
        "        return self._arm\n",
        "\n",
        "    @property\n",
        "    def hand(self):\n",
        "        return self._hand\n",
        "\n",
        "    def get_reward_spec(self):\n",
        "        n = len(self._targets)\n",
        "        return specs.Array(shape=(n,), dtype=np.float32, name='reward')\n",
        "\n",
        "    @property\n",
        "    def task_observables(self):\n",
        "        return self._task_observables\n",
        "\n",
        "    def get_reward(self, physics):\n",
        "        hand_pos = physics.bind(self._hand.tool_center_point).xpos\n",
        "        rews = []\n",
        "        for target_pos in self._targets:\n",
        "            distance = np.linalg.norm(hand_pos - target_pos)\n",
        "            reward = rewards.tolerance(distance,\n",
        "                                       bounds=(0, _TARGET_RADIUS),\n",
        "                                       margin=_TARGET_RADIUS)\n",
        "            rews.append(reward)\n",
        "        rews = np.array(rews).astype(np.float32)\n",
        "        if len(self._targets) == 1:\n",
        "            return rews[0]\n",
        "        return rews\n",
        "\n",
        "    def initialize_episode(self, physics, random_state):\n",
        "        self._hand.set_grasp(physics, close_factors=random_state.uniform())\n",
        "        self._tcp_initializer(physics, random_state)\n",
        "        if self._prop:\n",
        "            self._prop_placer(physics, random_state)\n",
        "        else:\n",
        "            if len(self._targets) == 1:\n",
        "                physics.bind(self._target).pos = self._targets[0]\n",
        "\n",
        "\n",
        "def _reach(task_id, obs_settings, use_site):\n",
        "    \"\"\"Configure and instantiate a `Reach` task.\n",
        "  Args:\n",
        "    obs_settings: An `observations.ObservationSettings` instance.\n",
        "    use_site: Boolean, if True then the target will be a fixed site, otherwise\n",
        "      it will be a moveable Duplo brick.\n",
        "  Returns:\n",
        "    An instance of `reach.Reach`.\n",
        "  \"\"\"\n",
        "    arena = arenas.Standard()\n",
        "    arm = robots.make_arm(obs_settings=obs_settings)\n",
        "    hand = robots.make_hand(obs_settings=obs_settings)\n",
        "    if use_site:\n",
        "        workspace = _SITE_WORKSPACE\n",
        "        prop = None\n",
        "    else:\n",
        "        workspace = _DUPLO_WORKSPACE\n",
        "        prop = props.Duplo(observable_options=observations.make_options(\n",
        "            obs_settings, observations.FREEPROP_OBSERVABLES))\n",
        "    task = MultiTaskReach(task_id,\n",
        "                          arena=arena,\n",
        "                          arm=arm,\n",
        "                          hand=hand,\n",
        "                          prop=prop,\n",
        "                          obs_settings=obs_settings,\n",
        "                          workspace=workspace,\n",
        "                          control_timestep=constants.CONTROL_TIMESTEP)\n",
        "    return task\n",
        "\n",
        "#fre/common/envs/dmc/wrappers\n",
        "from gym import core, spaces\n",
        "from dm_control import suite\n",
        "from dm_env import specs\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _spec_to_box(spec, dtype):\n",
        "    def extract_min_max(s):\n",
        "        assert s.dtype == np.float64 or s.dtype == np.float32\n",
        "        dim = int(np.prod(s.shape))\n",
        "        if type(s) == specs.Array:\n",
        "            bound = np.inf * np.ones(dim, dtype=np.float32)\n",
        "            return -bound, bound\n",
        "        elif type(s) == specs.BoundedArray:\n",
        "            zeros = np.zeros(dim, dtype=np.float32)\n",
        "            return s.minimum + zeros, s.maximum + zeros\n",
        "\n",
        "    mins, maxs = [], []\n",
        "    for s in spec:\n",
        "        mn, mx = extract_min_max(s)\n",
        "        mins.append(mn)\n",
        "        maxs.append(mx)\n",
        "    low = np.concatenate(mins, axis=0).astype(dtype)\n",
        "    high = np.concatenate(maxs, axis=0).astype(dtype)\n",
        "    assert low.shape == high.shape\n",
        "    return spaces.Box(low, high, dtype=dtype)\n",
        "\n",
        "\n",
        "def _flatten_obs(obs):\n",
        "    obs_pieces = []\n",
        "    for v in obs.values():\n",
        "        flat = np.array([v]) if np.isscalar(v) else v.ravel()\n",
        "        obs_pieces.append(flat)\n",
        "    return np.concatenate(obs_pieces, axis=0)\n",
        "\n",
        "\n",
        "class DMCWrapper(core.Env):\n",
        "    def __init__(\n",
        "        self,\n",
        "        domain_name,\n",
        "        task_name,\n",
        "        task_kwargs=None,\n",
        "        visualize_reward={},\n",
        "        from_pixels=False,\n",
        "        height=84,\n",
        "        width=84,\n",
        "        camera_id=0,\n",
        "        frame_skip=1,\n",
        "        environment_kwargs=None,\n",
        "        channels_first=True\n",
        "    ):\n",
        "        assert 'random' in task_kwargs, 'please specify a seed, for deterministic behaviour'\n",
        "        self._from_pixels = from_pixels\n",
        "        self._height = height\n",
        "        self._width = width\n",
        "        self._camera_id = camera_id\n",
        "        self._frame_skip = frame_skip\n",
        "        self._channels_first = channels_first\n",
        "\n",
        "        # create task\n",
        "        if domain_name == 'jaco':\n",
        "            #import fre.common.envs.dmc.jaco as jaco\n",
        "            self._env = jaco.make(task_id=task_name, obs_type=jaco.observations.PERFECT_FEATURES, seed=1)\n",
        "        else:\n",
        "            self._env = suite.load(\n",
        "                domain_name=domain_name,\n",
        "                task_name=task_name,\n",
        "                task_kwargs=task_kwargs,\n",
        "                visualize_reward=visualize_reward,\n",
        "                environment_kwargs=environment_kwargs\n",
        "            )\n",
        "\n",
        "        # true and normalized action spaces\n",
        "        self._true_action_space = _spec_to_box([self._env.action_spec()], np.float32)\n",
        "        self._norm_action_space = spaces.Box(\n",
        "            low=-1.0,\n",
        "            high=1.0,\n",
        "            shape=self._true_action_space.shape,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # create observation space\n",
        "        if from_pixels:\n",
        "            shape = [3, height, width] if channels_first else [height, width, 3]\n",
        "            self._observation_space = spaces.Box(\n",
        "                low=0, high=255, shape=shape, dtype=np.uint8\n",
        "            )\n",
        "        else:\n",
        "            self._observation_space = _spec_to_box(\n",
        "                self._env.observation_spec().values(),\n",
        "                np.float64\n",
        "            )\n",
        "\n",
        "        self._state_space = _spec_to_box(\n",
        "            self._env.observation_spec().values(),\n",
        "            np.float64\n",
        "        )\n",
        "\n",
        "        self.current_state = None\n",
        "\n",
        "        # set seed\n",
        "        self.seed(seed=task_kwargs.get('random', 1))\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self._env, name)\n",
        "\n",
        "    def _get_obs(self, time_step):\n",
        "        if self._from_pixels:\n",
        "            obs = self.render(\n",
        "                height=self._height,\n",
        "                width=self._width,\n",
        "                camera_id=self._camera_id\n",
        "            )\n",
        "            if self._channels_first:\n",
        "                obs = obs.transpose(2, 0, 1).copy()\n",
        "        else:\n",
        "            obs = _flatten_obs(time_step.observation)\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def _convert_action(self, action):\n",
        "        action = action.astype(np.float32)\n",
        "        true_delta = self._true_action_space.high - self._true_action_space.low\n",
        "        norm_delta = self._norm_action_space.high - self._norm_action_space.low\n",
        "        action = (action - self._norm_action_space.low) / norm_delta\n",
        "        action = action * true_delta + self._true_action_space.low\n",
        "        action = action.astype(np.float32)\n",
        "        return action\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return self._observation_space\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        return self._state_space\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return self._norm_action_space\n",
        "\n",
        "    @property\n",
        "    def reward_range(self):\n",
        "        return 0, self._frame_skip\n",
        "\n",
        "    def seed(self, seed):\n",
        "        self._true_action_space.seed(seed)\n",
        "        self._norm_action_space.seed(seed)\n",
        "        self._observation_space.seed(seed)\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self._norm_action_space.contains(action)\n",
        "        action = self._convert_action(action)\n",
        "        assert self._true_action_space.contains(action)\n",
        "        reward = 0\n",
        "        extra = {'internal_state': self._env.physics.get_state().copy()}\n",
        "\n",
        "        for _ in range(self._frame_skip):\n",
        "            time_step = self._env.step(action)\n",
        "            reward += time_step.reward or 0\n",
        "            done = time_step.last()\n",
        "            if done:\n",
        "                break\n",
        "        obs = self._get_obs(time_step)\n",
        "        self.current_state = _flatten_obs(time_step.observation)\n",
        "        extra['discount'] = time_step.discount\n",
        "        return obs, reward, done, extra\n",
        "\n",
        "    def reset(self):\n",
        "        time_step = self._env.reset()\n",
        "        self.current_state = _flatten_obs(time_step.observation)\n",
        "        obs = self._get_obs(time_step)\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode='rgb_array', height=None, width=None, camera_id=0):\n",
        "        assert mode == 'rgb_array', 'only support rgb_array mode, given %s' % mode\n",
        "        height = height or self._height\n",
        "        width = width or self._width\n",
        "        camera_id = camera_id or self._camera_id\n",
        "        return self._env.physics.render(\n",
        "            height=height, width=width, camera_id=camera_id\n",
        "        )\n",
        "\n",
        "#fre/common/envs/exorl/custom_dmc_tasks/__init__.py\n",
        "\n",
        "import typing as tp\n",
        "\"\"\"from . import cheetah\n",
        "from . import walker\n",
        "from . import hopper\n",
        "from . import quadruped\n",
        "from . import jaco\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def make(domain, task,\n",
        "         task_kwargs=None,\n",
        "         environment_kwargs=None,\n",
        "         visualize_reward: bool = False):\n",
        "\n",
        "    if domain == 'cheetah':\n",
        "        return cheetah.make(task,\n",
        "                            task_kwargs=task_kwargs,\n",
        "                            environment_kwargs=environment_kwargs,\n",
        "                            visualize_reward=visualize_reward)\n",
        "    elif domain == 'walker':\n",
        "        return walker.make(task,\n",
        "                           task_kwargs=task_kwargs,\n",
        "                           environment_kwargs=environment_kwargs,\n",
        "                           visualize_reward=visualize_reward)\n",
        "    elif domain == 'hopper':\n",
        "        return hopper.make(task,\n",
        "                           task_kwargs=task_kwargs,\n",
        "                           environment_kwargs=environment_kwargs,\n",
        "                           visualize_reward=visualize_reward)\n",
        "    elif domain == 'quadruped':\n",
        "        return quadruped.make(task,\n",
        "                              task_kwargs=task_kwargs,\n",
        "                              environment_kwargs=environment_kwargs,\n",
        "                              visualize_reward=visualize_reward)\n",
        "    elif domain == 'point_mass_maze':\n",
        "        return point_mass_maze.make(task,\n",
        "                                    task_kwargs=task_kwargs,\n",
        "                                    environment_kwargs=environment_kwargs,\n",
        "                                    visualize_reward=visualize_reward)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f'{task} not found')\n",
        "\n",
        "    assert None\n",
        "\n",
        "\n",
        "def make_jaco(task, obs_type, seed) -> tp.Any:\n",
        "    return jaco.make(task, obs_type, seed)\n",
        "\n",
        "#fre\\common\\envs\\exorl\\custom_dmc_tasks\\cheetah\n",
        "\"\"\"Cheetah Domain.\"\"\"\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import typing as tp\n",
        "from typing import Any, Tuple\n",
        "\n",
        "from dm_control import mujoco\n",
        "from dm_control.rl import control\n",
        "from dm_control.suite import base\n",
        "from dm_control.suite import common\n",
        "from dm_control.utils import containers\n",
        "from dm_control.utils import rewards\n",
        "from dm_control.utils import io as resources\n",
        "\n",
        "_DEFAULT_TIME_LIMIT: int\n",
        "_RUN_SPEED: int\n",
        "_SPIN_SPEED: int\n",
        "\n",
        "# How long the simulation will run, in seconds.\n",
        "_DEFAULT_TIME_LIMIT = 10\n",
        "\n",
        "# Running speed above which reward is 1.\n",
        "_RUN_SPEED = 10\n",
        "_WALK_SPEED = 2\n",
        "_SPIN_SPEED = 5\n",
        "\n",
        "SUITE = containers.TaggedTasks()\n",
        "\n",
        "\n",
        "def make(task,\n",
        "         task_kwargs=None,\n",
        "         environment_kwargs=None,\n",
        "         visualize_reward: bool = False):\n",
        "    task_kwargs = task_kwargs or {}\n",
        "    if environment_kwargs is not None:\n",
        "        task_kwargs = task_kwargs.copy()\n",
        "        task_kwargs['environment_kwargs'] = environment_kwargs\n",
        "    env = SUITE[task](**task_kwargs)\n",
        "    env.task.visualize_reward = visualize_reward\n",
        "    return env\n",
        "\n",
        "\n",
        "def get_model_and_assets() -> Tuple[Any, Any]:\n",
        "    \"\"\"Returns a tuple containing the model XML string and a dict of assets.\"\"\"\n",
        "    root_dir = os.path.dirname(os.path.dirname(__file__))\n",
        "    xml = resources.GetResource(\n",
        "        os.path.join(root_dir, 'custom_dmc_tasks', 'cheetah.xml'))\n",
        "    return xml, common.ASSETS\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def walk(time_limit: int = _DEFAULT_TIME_LIMIT,\n",
        "         random=None,\n",
        "         environment_kwargs=None):\n",
        "    \"\"\"Returns the run task.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = Cheetah(move_speed=_WALK_SPEED, forward=True, flip=False, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def walk_backward(time_limit: int = _DEFAULT_TIME_LIMIT,\n",
        "                  random=None,\n",
        "                  environment_kwargs=None):\n",
        "    \"\"\"Returns the run task.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = Cheetah(move_speed=_WALK_SPEED, forward=False, flip=False, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def run_backward(time_limit: int = _DEFAULT_TIME_LIMIT,\n",
        "                 random=None,\n",
        "                 environment_kwargs=None):\n",
        "    \"\"\"Returns the run task.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = Cheetah(forward=False, flip=False, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def flip(time_limit: int = _DEFAULT_TIME_LIMIT,\n",
        "         random=None,\n",
        "         environment_kwargs=None):\n",
        "    \"\"\"Returns the run task.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = Cheetah(move_speed=_WALK_SPEED, forward=True, flip=True, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def flip_backward(time_limit: int = _DEFAULT_TIME_LIMIT,\n",
        "                  random=None,\n",
        "                  environment_kwargs=None):\n",
        "    \"\"\"Returns the run task.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = Cheetah(move_speed=_WALK_SPEED, forward=False, flip=True, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "class Physics(mujoco.Physics):\n",
        "    \"\"\"Physics simulation with additional features for the Cheetah domain.\"\"\"\n",
        "\n",
        "    def speed(self) -> Any:\n",
        "        \"\"\"Returns the horizontal speed of the Cheetah.\"\"\"\n",
        "        return self.named.data.sensordata['torso_subtreelinvel'][0]\n",
        "\n",
        "    def angmomentum(self) -> Any:\n",
        "        \"\"\"Returns the angular momentum of torso of the Cheetah about Y axis.\"\"\"\n",
        "        return self.named.data.subtree_angmom['torso'][1]\n",
        "\n",
        "\n",
        "class Cheetah(base.Task):\n",
        "    \"\"\"A `Task` to train a running Cheetah.\"\"\"\n",
        "\n",
        "    def __init__(self, move_speed=_RUN_SPEED, forward=True, flip=False, random=None) -> None:\n",
        "        self._move_speed = move_speed\n",
        "        self._forward = 1 if forward else -1\n",
        "        self._flip = flip\n",
        "        super(Cheetah, self).__init__(random=random)\n",
        "        self._timeout_progress = 0\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\"\"\"\n",
        "        # The indexing below assumes that all joints have a single DOF.\n",
        "        assert physics.model.nq == physics.model.njnt\n",
        "        is_limited = physics.model.jnt_limited == 1\n",
        "        lower, upper = physics.model.jnt_range[is_limited].T\n",
        "        physics.data.qpos[is_limited] = self.random.uniform(lower, upper)\n",
        "\n",
        "        # Stabilize the model before the actual simulation.\n",
        "        for _ in range(200):\n",
        "            physics.step()\n",
        "\n",
        "        physics.data.time = 0\n",
        "        self._timeout_progress = 0\n",
        "        super().initialize_episode(physics)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation of the state, ignoring horizontal position.\"\"\"\n",
        "        obs = collections.OrderedDict()\n",
        "        # Ignores horizontal position to maintain translational invariance.\n",
        "        obs['position'] = physics.data.qpos[1:].copy()\n",
        "        obs['velocity'] = physics.velocity()\n",
        "        return obs\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward to the agent.\"\"\"\n",
        "        if self._flip:\n",
        "            reward = rewards.tolerance(self._forward * physics.angmomentum(),\n",
        "                                       bounds=(_SPIN_SPEED, float('inf')),\n",
        "                                       margin=_SPIN_SPEED,\n",
        "                                       value_at_margin=0,\n",
        "                                       sigmoid='linear')\n",
        "\n",
        "        else:\n",
        "            reward = rewards.tolerance(self._forward * physics.speed(),\n",
        "                                       bounds=(self._move_speed, float('inf')),\n",
        "                                       margin=self._move_speed,\n",
        "                                       value_at_margin=0,\n",
        "                                       sigmoid='linear')\n",
        "        return reward\n",
        "\n",
        "#fre\\common\\envs\\exorl\\custom_dmc_tasks\\hopper\n",
        "\"\"\"Hopper domain.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import typing as tp\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from dm_control import mujoco\n",
        "from dm_control.rl import control\n",
        "from dm_control.suite import base\n",
        "from dm_control.suite import common\n",
        "from dm_control.suite.utils import randomizers\n",
        "from dm_control.utils import containers\n",
        "from dm_control.utils import rewards\n",
        "from dm_control.utils import io as resources\n",
        "\n",
        "_CONTROL_TIMESTEP: float\n",
        "_DEFAULT_TIME_LIMIT: int\n",
        "_HOP_SPEED: int\n",
        "_SPIN_SPEED: int\n",
        "_STAND_HEIGHT: float\n",
        "\n",
        "SUITE = containers.TaggedTasks()\n",
        "\n",
        "_CONTROL_TIMESTEP = .02  # (Seconds)\n",
        "\n",
        "# Default duration of an episode, in seconds.\n",
        "_DEFAULT_TIME_LIMIT = 20\n",
        "\n",
        "# Minimal height of torso over foot above which stand reward is 1.\n",
        "_STAND_HEIGHT = 0.6\n",
        "\n",
        "# Hopping speed above which hop reward is 1.\n",
        "_HOP_SPEED = 2\n",
        "_SPIN_SPEED = 5\n",
        "\n",
        "\n",
        "def make(task,\n",
        "         task_kwargs=None,\n",
        "         environment_kwargs=None,\n",
        "         visualize_reward: bool = False):\n",
        "    task_kwargs = task_kwargs or {}\n",
        "    if environment_kwargs is not None:\n",
        "        task_kwargs = task_kwargs.copy()\n",
        "        task_kwargs['environment_kwargs'] = environment_kwargs\n",
        "    env = SUITE[task](**task_kwargs)\n",
        "    env.task.visualize_reward = visualize_reward\n",
        "    return env\n",
        "\n",
        "\n",
        "def get_model_and_assets() -> Tuple[Any, Any]:\n",
        "    \"\"\"Returns a tuple containing the model XML string and a dict of assets.\"\"\"\n",
        "    root_dir = os.path.dirname(os.path.dirname(__file__))\n",
        "    xml = resources.GetResource(\n",
        "        os.path.join(root_dir, 'custom_dmc_tasks', 'hopper.xml'))\n",
        "    return xml, common.ASSETS\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def hop_backward(time_limit: int = _DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):\n",
        "    \"\"\"Returns a Hopper that strives to hop forward.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = Hopper(hopping=True, forward=False, flip=False, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def flip(time_limit: int = _DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):\n",
        "    \"\"\"Returns a Hopper that strives to hop forward.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = Hopper(hopping=True, forward=True, flip=True, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def flip_backward(time_limit: int = _DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):\n",
        "    \"\"\"Returns a Hopper that strives to hop forward.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = Hopper(hopping=True, forward=False, flip=True, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "class Physics(mujoco.Physics):\n",
        "    \"\"\"Physics simulation with additional features for the Hopper domain.\"\"\"\n",
        "\n",
        "    def height(self) -> Any:\n",
        "        \"\"\"Returns height of torso with respect to foot.\"\"\"\n",
        "        return (self.named.data.xipos['torso', 'z'] -\n",
        "                self.named.data.xipos['foot', 'z'])\n",
        "\n",
        "    def speed(self) -> Any:\n",
        "        \"\"\"Returns horizontal speed of the Hopper.\"\"\"\n",
        "        return self.named.data.sensordata['torso_subtreelinvel'][0]\n",
        "\n",
        "    def touch(self) -> Any:\n",
        "        \"\"\"Returns the signals from two foot touch sensors.\"\"\"\n",
        "        return np.log1p(self.named.data.sensordata[['touch_toe',\n",
        "                                                    'touch_heel']])\n",
        "\n",
        "    def angmomentum(self) -> Any:\n",
        "        \"\"\"Returns the angular momentum of torso of the Cheetah about Y axis.\"\"\"\n",
        "        return self.named.data.subtree_angmom['torso'][1]\n",
        "\n",
        "\n",
        "class Hopper(base.Task):\n",
        "    \"\"\"A Hopper's `Task` to train a standing and a jumping Hopper.\"\"\"\n",
        "\n",
        "    def __init__(self, hopping, forward=True, flip=False, random=None) -> None:\n",
        "        \"\"\"Initialize an instance of `Hopper`.\n",
        "\n",
        "    Args:\n",
        "      hopping: Boolean, if True the task is to hop forwards, otherwise it is to\n",
        "        balance upright.\n",
        "      random: Optional, either a `numpy.random.RandomState` instance, an\n",
        "        integer seed for creating a new `RandomState`, or None to select a seed\n",
        "        automatically (default).\n",
        "    \"\"\"\n",
        "        self._hopping = hopping\n",
        "        self._forward = 1 if forward else -1\n",
        "        self._flip = flip\n",
        "        self._timeout_progress = 0\n",
        "        super(Hopper, self).__init__(random=random)\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\"\"\"\n",
        "        randomizers.randomize_limited_and_rotational_joints(\n",
        "            physics, self.random)\n",
        "        self._timeout_progress = 0\n",
        "        super(Hopper, self).initialize_episode(physics)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation of positions, velocities and touch sensors.\"\"\"\n",
        "        obs = collections.OrderedDict()\n",
        "        # Ignores horizontal position to maintain translational invariance:\n",
        "        obs['position'] = physics.data.qpos[1:].copy()\n",
        "        obs['velocity'] = physics.velocity()\n",
        "        obs['touch'] = physics.touch()\n",
        "        return obs\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward applicable to the performed task.\"\"\"\n",
        "        standing = rewards.tolerance(physics.height(), (_STAND_HEIGHT, 2))\n",
        "        assert self._hopping\n",
        "        if self._flip:\n",
        "            hopping = rewards.tolerance(self._forward * physics.angmomentum(),\n",
        "                                        bounds=(_SPIN_SPEED, float('inf')),\n",
        "                                        margin=_SPIN_SPEED,\n",
        "                                        value_at_margin=0,\n",
        "                                        sigmoid='linear')\n",
        "        else:\n",
        "            hopping = rewards.tolerance(self._forward * physics.speed(),\n",
        "                                        bounds=(_HOP_SPEED, float('inf')),\n",
        "                                        margin=_HOP_SPEED / 2,\n",
        "                                        value_at_margin=0.5,\n",
        "                                        sigmoid='linear')\n",
        "        return standing * hopping\n",
        "\n",
        "\n",
        "#fre\\common\\envs\\exorl\\custom_dmc_tasks\\jaco\n",
        "\"\"\"A task where the goal is to move the hand close to a target prop or site.\"\"\"\n",
        "\n",
        "import collections\n",
        "\n",
        "from dm_control import composer\n",
        "from dm_control.composer import initializers\n",
        "from dm_control.composer.variation import distributions\n",
        "from dm_control.entities import props\n",
        "from dm_control.manipulation.shared import arenas\n",
        "from dm_control.manipulation.shared import cameras\n",
        "from dm_control.manipulation.shared import constants\n",
        "from dm_control.manipulation.shared import observations\n",
        "from dm_control.manipulation.shared import robots\n",
        "from dm_control.manipulation.shared import workspaces\n",
        "from dm_control.utils import rewards\n",
        "from dm_env import specs\n",
        "import numpy as np\n",
        "\n",
        "_ReachWorkspace = collections.namedtuple(\n",
        "    '_ReachWorkspace', ['target_bbox', 'tcp_bbox', 'arm_offset'])\n",
        "\n",
        "# Ensures that the props are not touching the table before settling.\n",
        "_PROP_Z_OFFSET = 0.001\n",
        "\n",
        "_DUPLO_WORKSPACE = _ReachWorkspace(\n",
        "    target_bbox=workspaces.BoundingBox(lower=(-0.1, -0.1, _PROP_Z_OFFSET),\n",
        "                                       upper=(0.1, 0.1, _PROP_Z_OFFSET)),\n",
        "    tcp_bbox=workspaces.BoundingBox(lower=(-0.1, -0.1, 0.2),\n",
        "                                    upper=(0.1, 0.1, 0.4)),\n",
        "    arm_offset=robots.ARM_OFFSET)\n",
        "\n",
        "_SITE_WORKSPACE = _ReachWorkspace(\n",
        "    target_bbox=workspaces.BoundingBox(lower=(-0.2, -0.2, 0.02),\n",
        "                                       upper=(0.2, 0.2, 0.4)),\n",
        "    tcp_bbox=workspaces.BoundingBox(lower=(-0.2, -0.2, 0.02),\n",
        "                                    upper=(0.2, 0.2, 0.4)),\n",
        "    arm_offset=robots.ARM_OFFSET)\n",
        "\n",
        "_TARGET_RADIUS = 0.05\n",
        "_TIME_LIMIT = 10.\n",
        "\n",
        "TASKS = [('reach_top_left', np.array([-0.09, 0.09, _PROP_Z_OFFSET])),\n",
        "         ('reach_top_right', np.array([0.09, 0.09, _PROP_Z_OFFSET])),\n",
        "         ('reach_bottom_left', np.array([-0.09, -0.09, _PROP_Z_OFFSET])),\n",
        "         ('reach_bottom_right', np.array([0.09, -0.09, _PROP_Z_OFFSET]))]\n",
        "\n",
        "\n",
        "def make(task_id, obs_type, seed):\n",
        "    obs_settings = observations.VISION if obs_type == 'pixels' else observations.PERFECT_FEATURES\n",
        "    task = _reach(task_id, obs_settings=obs_settings, use_site=True)\n",
        "    return composer.Environment(task,\n",
        "                                time_limit=_TIME_LIMIT,\n",
        "                                random_state=seed)\n",
        "\n",
        "\n",
        "class MultiTaskReach(composer.Task):\n",
        "    \"\"\"Bring the hand close to a target prop or site.\"\"\"\n",
        "\n",
        "    def __init__(self, task_id, arena, arm, hand, prop, obs_settings,\n",
        "                 workspace, control_timestep):\n",
        "        \"\"\"Initializes a new `Reach` task.\n",
        "\n",
        "    Args:\n",
        "      arena: `composer.Entity` instance.\n",
        "      arm: `robot_base.RobotArm` instance.\n",
        "      hand: `robot_base.RobotHand` instance.\n",
        "      prop: `composer.Entity` instance specifying the prop to reach to, or None\n",
        "        in which case the target is a fixed site whose position is specified by\n",
        "        the workspace.\n",
        "      obs_settings: `observations.ObservationSettings` instance.\n",
        "      workspace: `_ReachWorkspace` specifying the placement of the prop and TCP.\n",
        "      control_timestep: Float specifying the control timestep in seconds.\n",
        "    \"\"\"\n",
        "        self._arena = arena\n",
        "        self._arm = arm\n",
        "        self._hand = hand\n",
        "        self._arm.attach(self._hand)\n",
        "        self._arena.attach_offset(self._arm, offset=workspace.arm_offset)\n",
        "        self.control_timestep = control_timestep\n",
        "        self._tcp_initializer = initializers.ToolCenterPointInitializer(\n",
        "            self._hand,\n",
        "            self._arm,\n",
        "            position=distributions.Uniform(*workspace.tcp_bbox),\n",
        "            quaternion=workspaces.DOWN_QUATERNION)\n",
        "\n",
        "        # Add custom camera observable.\n",
        "        self._task_observables = cameras.add_camera_observables(\n",
        "            arena, obs_settings, cameras.FRONT_CLOSE)\n",
        "\n",
        "        if task_id == 'reach_multitask':\n",
        "            self._targets = [target for (_, target) in TASKS]\n",
        "        else:\n",
        "            self._targets = [\n",
        "                target for (task, target) in TASKS if task == task_id\n",
        "            ]\n",
        "            assert len(self._targets) > 0\n",
        "\n",
        "        #target_pos_distribution = distributions.Uniform(*TASKS[task_id])\n",
        "        self._prop = prop\n",
        "        if prop:\n",
        "            # The prop itself is used to visualize the target location.\n",
        "            self._make_target_site(parent_entity=prop, visible=False)\n",
        "            self._target = self._arena.add_free_entity(prop)\n",
        "            self._prop_placer = initializers.PropPlacer(\n",
        "                props=[prop],\n",
        "                position=target_pos_distribution,\n",
        "                quaternion=workspaces.uniform_z_rotation,\n",
        "                settle_physics=True)\n",
        "        else:\n",
        "            if len(self._targets) == 1:\n",
        "                self._target = self._make_target_site(parent_entity=arena,\n",
        "                                                      visible=True)\n",
        "\n",
        "            #obs = observable.MJCFFeature('pos', self._target)\n",
        "            # obs.configure(**obs_settings.prop_pose._asdict())\n",
        "            #self._task_observables['target_position'] = obs\n",
        "\n",
        "        # Add sites for visualizing the prop and target bounding boxes.\n",
        "        workspaces.add_bbox_site(body=self.root_entity.mjcf_model.worldbody,\n",
        "                                 lower=workspace.tcp_bbox.lower,\n",
        "                                 upper=workspace.tcp_bbox.upper,\n",
        "                                 rgba=constants.GREEN,\n",
        "                                 name='tcp_spawn_area')\n",
        "        workspaces.add_bbox_site(body=self.root_entity.mjcf_model.worldbody,\n",
        "                                 lower=workspace.target_bbox.lower,\n",
        "                                 upper=workspace.target_bbox.upper,\n",
        "                                 rgba=constants.BLUE,\n",
        "                                 name='target_spawn_area')\n",
        "\n",
        "    def _make_target_site(self, parent_entity, visible):\n",
        "        return workspaces.add_target_site(\n",
        "            body=parent_entity.mjcf_model.worldbody,\n",
        "            radius=_TARGET_RADIUS,\n",
        "            visible=visible,\n",
        "            rgba=constants.RED,\n",
        "            name='target_site')\n",
        "\n",
        "    @property\n",
        "    def root_entity(self):\n",
        "        return self._arena\n",
        "\n",
        "    @property\n",
        "    def arm(self):\n",
        "        return self._arm\n",
        "\n",
        "    @property\n",
        "    def hand(self):\n",
        "        return self._hand\n",
        "\n",
        "    def get_reward_spec(self):\n",
        "        n = len(self._targets)\n",
        "        return specs.Array(shape=(n,), dtype=np.float32, name='reward')\n",
        "\n",
        "    @property\n",
        "    def task_observables(self):\n",
        "        return self._task_observables\n",
        "\n",
        "    def get_reward(self, physics):\n",
        "        hand_pos = physics.bind(self._hand.tool_center_point).xpos\n",
        "        rews = []\n",
        "        for target_pos in self._targets:\n",
        "            distance = np.linalg.norm(hand_pos - target_pos)\n",
        "            reward = rewards.tolerance(distance,\n",
        "                                       bounds=(0, _TARGET_RADIUS),\n",
        "                                       margin=_TARGET_RADIUS)\n",
        "            rews.append(reward)\n",
        "        rews = np.array(rews).astype(np.float32)\n",
        "        if len(self._targets) == 1:\n",
        "            return rews[0]\n",
        "        return rews\n",
        "\n",
        "    def initialize_episode(self, physics, random_state):\n",
        "        self._hand.set_grasp(physics, close_factors=random_state.uniform())\n",
        "        self._tcp_initializer(physics, random_state)\n",
        "        if self._prop:\n",
        "            self._prop_placer(physics, random_state)\n",
        "        else:\n",
        "            if len(self._targets) == 1:\n",
        "                physics.bind(self._target).pos = self._targets[0]\n",
        "\n",
        "\n",
        "def _reach(task_id, obs_settings, use_site):\n",
        "    \"\"\"Configure and instantiate a `Reach` task.\n",
        "\n",
        "  Args:\n",
        "    obs_settings: An `observations.ObservationSettings` instance.\n",
        "    use_site: Boolean, if True then the target will be a fixed site, otherwise\n",
        "      it will be a moveable Duplo brick.\n",
        "\n",
        "  Returns:\n",
        "    An instance of `reach.Reach`.\n",
        "  \"\"\"\n",
        "    arena = arenas.Standard()\n",
        "    arm = robots.make_arm(obs_settings=obs_settings)\n",
        "    hand = robots.make_hand(obs_settings=obs_settings)\n",
        "    if use_site:\n",
        "        workspace = _SITE_WORKSPACE\n",
        "        prop = None\n",
        "    else:\n",
        "        workspace = _DUPLO_WORKSPACE\n",
        "        prop = props.Duplo(observable_options=observations.make_options(\n",
        "            obs_settings, observations.FREEPROP_OBSERVABLES))\n",
        "    task = MultiTaskReach(task_id,\n",
        "                          arena=arena,\n",
        "                          arm=arm,\n",
        "                          hand=hand,\n",
        "                          prop=prop,\n",
        "                          obs_settings=obs_settings,\n",
        "                          workspace=workspace,\n",
        "                          control_timestep=constants.CONTROL_TIMESTEP)\n",
        "    return task\n",
        "\n",
        "#fre/common/envs/exorl/custom_dmc_tasks/quadrupled\n",
        "\"\"\"Quadruped Domain.\"\"\"\n",
        "\n",
        "import collections\n",
        "import typing as tp\n",
        "from typing import Any\n",
        "import os\n",
        "\n",
        "from dm_control import mujoco\n",
        "from dm_control.mujoco.wrapper import mjbindings\n",
        "from dm_control.rl import control\n",
        "from dm_control.suite import base\n",
        "from dm_control.suite import common\n",
        "from dm_control.utils import containers\n",
        "from dm_control.utils import rewards\n",
        "from dm_control.utils import xml_tools\n",
        "from lxml import etree\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "enums = mjbindings.enums\n",
        "mjlib = mjbindings.mjlib\n",
        "\n",
        "\n",
        "_DEFAULT_TIME_LIMIT = 20\n",
        "_CONTROL_TIMESTEP = .02\n",
        "\n",
        "# Horizontal speeds above which the move reward is 1.\n",
        "_RUN_SPEED = 5\n",
        "_WALK_SPEED = 0.5\n",
        "\n",
        "_JUMP_HEIGHT = 1.0\n",
        "\n",
        "# Constants related to terrain generation.\n",
        "_HEIGHTFIELD_ID = 0\n",
        "_TERRAIN_SMOOTHNESS = 0.15  # 0.0: maximally bumpy; 1.0: completely smooth.\n",
        "_TERRAIN_BUMP_SCALE = 2  # Spatial scale of terrain bumps (in meters).\n",
        "\n",
        "# Named model elements.\n",
        "_TOES = ['toe_front_left', 'toe_back_left', 'toe_back_right', 'toe_front_right']\n",
        "_WALLS = ['wall_px', 'wall_py', 'wall_nx', 'wall_ny']\n",
        "\n",
        "SUITE = containers.TaggedTasks()\n",
        "\n",
        "\n",
        "def make(task,\n",
        "         task_kwargs=None,\n",
        "         environment_kwargs=None,\n",
        "         visualize_reward: bool = False):\n",
        "    task_kwargs = task_kwargs or {}\n",
        "    if environment_kwargs is not None:\n",
        "        task_kwargs = task_kwargs.copy()\n",
        "        task_kwargs['environment_kwargs'] = environment_kwargs\n",
        "    env = SUITE[task](**task_kwargs)\n",
        "    env.task.visualize_reward = visualize_reward\n",
        "    return env\n",
        "\n",
        "\n",
        "# REMOVED since resources is undefined\n",
        "# def get_model_and_assets() -> Tuple[Any, Any]:\n",
        "#     \"\"\"Returns a tuple containing the model XML string and a dict of assets.\"\"\"\n",
        "#     root_dir = os.path.dirname(os.path.dirname(__file__))\n",
        "#     xml = resources.GetResource(\n",
        "#         os.path.join(root_dir, 'custom_dmc_tasks', 'quadruped.xml'))\n",
        "#     return xml, common.ASSETS\n",
        "\n",
        "\n",
        "def make_model(floor_size=None, terrain: bool = False, rangefinders: bool = False,\n",
        "               walls_and_ball: bool = False):\n",
        "    \"\"\"Returns the model XML string.\"\"\"\n",
        "    root_dir = os.path.dirname(os.path.dirname(__file__))\n",
        "    xml_string = common.read_model(os.path.join(root_dir, 'custom_dmc_tasks', 'quadruped.xml'))\n",
        "    parser = etree.XMLParser(remove_blank_text=True)\n",
        "    mjcf = etree.XML(xml_string, parser)\n",
        "\n",
        "    # Set floor size.\n",
        "    if floor_size is not None:\n",
        "        floor_geom = mjcf.find('.//geom[@name=\\'floor\\']')\n",
        "        floor_geom.attrib['size'] = f'{floor_size} {floor_size} .5'\n",
        "\n",
        "    # Remove walls, ball and target.\n",
        "    if not walls_and_ball:\n",
        "        for wall in _WALLS:\n",
        "            wall_geom = xml_tools.find_element(mjcf, 'geom', wall)\n",
        "            wall_geom.getparent().remove(wall_geom)\n",
        "\n",
        "        # Remove ball.\n",
        "        ball_body = xml_tools.find_element(mjcf, 'body', 'ball')\n",
        "        ball_body.getparent().remove(ball_body)\n",
        "\n",
        "        # Remove target.\n",
        "        target_site = xml_tools.find_element(mjcf, 'site', 'target')\n",
        "        target_site.getparent().remove(target_site)\n",
        "\n",
        "    # Remove terrain.\n",
        "    if not terrain:\n",
        "        terrain_geom = xml_tools.find_element(mjcf, 'geom', 'terrain')\n",
        "        terrain_geom.getparent().remove(terrain_geom)\n",
        "\n",
        "    # Remove rangefinders if they're not used, as range computations can be\n",
        "    # expensive, especially in a scene with heightfields.\n",
        "    if not rangefinders:\n",
        "        rangefinder_sensors = mjcf.findall('.//rangefinder')\n",
        "        for rf in rangefinder_sensors:\n",
        "            rf.getparent().remove(rf)\n",
        "\n",
        "    return etree.tostring(mjcf, pretty_print=True)\n",
        "\n",
        "\n",
        "@SUITE.add()\n",
        "def stand(time_limit: int = _DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):\n",
        "    \"\"\"Returns the Walk task.\"\"\"\n",
        "    xml_string = make_model(floor_size=_DEFAULT_TIME_LIMIT * _WALK_SPEED)\n",
        "    physics = Physics.from_xml_string(xml_string, common.ASSETS)\n",
        "    task = Stand(random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics, task, time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add()\n",
        "def jump(time_limit: int = _DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):\n",
        "    \"\"\"Returns the Walk task.\"\"\"\n",
        "    xml_string = make_model(floor_size=_DEFAULT_TIME_LIMIT * _WALK_SPEED)\n",
        "    physics = Physics.from_xml_string(xml_string, common.ASSETS)\n",
        "    task = Jump(desired_height=_JUMP_HEIGHT, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics, task, time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add()\n",
        "def roll(time_limit: int = _DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):\n",
        "    \"\"\"Returns the Walk task.\"\"\"\n",
        "    xml_string = make_model(floor_size=_DEFAULT_TIME_LIMIT * _WALK_SPEED)\n",
        "    physics = Physics.from_xml_string(xml_string, common.ASSETS)\n",
        "    task = Roll(desired_speed=_WALK_SPEED, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics, task, time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add()\n",
        "def roll_fast(time_limit: int = _DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):\n",
        "    \"\"\"Returns the Walk task.\"\"\"\n",
        "    xml_string = make_model(floor_size=_DEFAULT_TIME_LIMIT * _WALK_SPEED)\n",
        "    physics = Physics.from_xml_string(xml_string, common.ASSETS)\n",
        "    task = Roll(desired_speed=_RUN_SPEED, random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics, task, time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add()\n",
        "def escape(time_limit: int = _DEFAULT_TIME_LIMIT, random=None,\n",
        "           environment_kwargs=None):\n",
        "    \"\"\"Returns the Escape task.\"\"\"\n",
        "    xml_string = make_model(floor_size=40, terrain=True, rangefinders=True)\n",
        "    physics = Physics.from_xml_string(xml_string, common.ASSETS)\n",
        "    task = Escape(random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics, task, time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "@SUITE.add()\n",
        "def fetch(time_limit: int = _DEFAULT_TIME_LIMIT, random=None, environment_kwargs=None):\n",
        "    \"\"\"Returns the Fetch task.\"\"\"\n",
        "    xml_string = make_model(walls_and_ball=True)\n",
        "    physics = Physics.from_xml_string(xml_string, common.ASSETS)\n",
        "    task = Fetch(random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics, task, time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "# pylint: disable=attribute-defined-outside-init\n",
        "class Physics(mujoco.Physics):\n",
        "    \"\"\"Physics simulation with additional features for the Quadruped domain.\"\"\"\n",
        "\n",
        "    def _reload_from_data(self, data) -> None:\n",
        "        super()._reload_from_data(data)\n",
        "        # Clear cached sensor names when the physics is reloaded.\n",
        "        self._sensor_types_to_names: tp.Dict[tp.Tuple[tp.Any, ...], tp.List[str]] = {}\n",
        "        self._hinge_names: tp.List[str] = []\n",
        "\n",
        "    def _get_sensor_names(self, *sensor_types) -> Any:\n",
        "        try:\n",
        "            sensor_names = self._sensor_types_to_names[sensor_types]\n",
        "        except KeyError:\n",
        "            [sensor_ids] = np.where(np.in1d(self.model.sensor_type, sensor_types))\n",
        "            sensor_names = [self.model.id2name(s_id, 'sensor') for s_id in sensor_ids]\n",
        "            self._sensor_types_to_names[sensor_types] = sensor_names\n",
        "        return sensor_names\n",
        "\n",
        "    def torso_upright(self) -> np.ndarray:\n",
        "        \"\"\"Returns the dot-product of the torso z-axis and the global z-axis.\"\"\"\n",
        "        return np.asarray(self.named.data.xmat['torso', 'zz'])\n",
        "\n",
        "    def torso_velocity(self) -> Any:\n",
        "        \"\"\"Returns the velocity of the torso, in the local frame.\"\"\"\n",
        "        return self.named.data.sensordata['velocimeter'].copy()\n",
        "\n",
        "    def com_height(self) -> Any:\n",
        "        return self.named.data.sensordata['center_of_mass'].copy()[2]\n",
        "\n",
        "    def egocentric_state(self) -> Any:\n",
        "        \"\"\"Returns the state without global orientation or position.\"\"\"\n",
        "        if not self._hinge_names:\n",
        "            [hinge_ids] = np.nonzero(self.model.jnt_type ==\n",
        "                                     enums.mjtJoint.mjJNT_HINGE)\n",
        "            self._hinge_names = [self.model.id2name(j_id, 'joint')\n",
        "                                 for j_id in hinge_ids]\n",
        "        return np.hstack((self.named.data.qpos[self._hinge_names],\n",
        "                          self.named.data.qvel[self._hinge_names],\n",
        "                          self.data.act))\n",
        "\n",
        "    def toe_positions(self) -> Any:\n",
        "        \"\"\"Returns toe positions in egocentric frame.\"\"\"\n",
        "        torso_frame = self.named.data.xmat['torso'].reshape(3, 3)\n",
        "        torso_pos = self.named.data.xpos['torso']\n",
        "        torso_to_toe = self.named.data.xpos[_TOES] - torso_pos\n",
        "        return torso_to_toe.dot(torso_frame)\n",
        "\n",
        "    def force_torque(self) -> Any:\n",
        "        \"\"\"Returns scaled force/torque sensor readings at the toes.\"\"\"\n",
        "        force_torque_sensors = self._get_sensor_names(enums.mjtSensor.mjSENS_FORCE,\n",
        "                                                      enums.mjtSensor.mjSENS_TORQUE)\n",
        "        return np.arcsinh(self.named.data.sensordata[force_torque_sensors])\n",
        "\n",
        "    def imu(self) -> Any:\n",
        "        \"\"\"Returns IMU-like sensor readings.\"\"\"\n",
        "        imu_sensors = self._get_sensor_names(enums.mjtSensor.mjSENS_GYRO,\n",
        "                                             enums.mjtSensor.mjSENS_ACCELEROMETER)\n",
        "        return self.named.data.sensordata[imu_sensors]\n",
        "\n",
        "    def rangefinder(self) -> Any:\n",
        "        \"\"\"Returns scaled rangefinder sensor readings.\"\"\"\n",
        "        rf_sensors = self._get_sensor_names(enums.mjtSensor.mjSENS_RANGEFINDER)\n",
        "        rf_readings = self.named.data.sensordata[rf_sensors]\n",
        "        no_intersection = -1.0\n",
        "        return np.where(rf_readings == no_intersection, 1.0, np.tanh(rf_readings))\n",
        "\n",
        "    def origin_distance(self) -> np.ndarray:\n",
        "        \"\"\"Returns the distance from the origin to the workspace.\"\"\"\n",
        "        return np.asarray(np.linalg.norm(self.named.data.site_xpos['workspace']))\n",
        "\n",
        "    def origin(self) -> Any:\n",
        "        \"\"\"Returns origin position in the torso frame.\"\"\"\n",
        "        torso_frame = self.named.data.xmat['torso'].reshape(3, 3)\n",
        "        torso_pos = self.named.data.xpos['torso']\n",
        "        return -torso_pos.dot(torso_frame)\n",
        "\n",
        "    def ball_state(self) -> Any:\n",
        "        \"\"\"Returns ball position and velocity relative to the torso frame.\"\"\"\n",
        "        data = self.named.data\n",
        "        torso_frame = data.xmat['torso'].reshape(3, 3)\n",
        "        ball_rel_pos = data.xpos['ball'] - data.xpos['torso']\n",
        "        ball_rel_vel = data.qvel['ball_root'][:3] - data.qvel['root'][:3]\n",
        "        ball_rot_vel = data.qvel['ball_root'][3:]\n",
        "        ball_state = np.vstack((ball_rel_pos, ball_rel_vel, ball_rot_vel))\n",
        "        return ball_state.dot(torso_frame).ravel()\n",
        "\n",
        "    def target_position(self) -> Any:\n",
        "        \"\"\"Returns target position in torso frame.\"\"\"\n",
        "        torso_frame = self.named.data.xmat['torso'].reshape(3, 3)\n",
        "        torso_pos = self.named.data.xpos['torso']\n",
        "        torso_to_target = self.named.data.site_xpos['target'] - torso_pos\n",
        "        return torso_to_target.dot(torso_frame)\n",
        "\n",
        "    def ball_to_target_distance(self) -> Any:\n",
        "        \"\"\"Returns horizontal distance from the ball to the target.\"\"\"\n",
        "        ball_to_target = (self.named.data.site_xpos['target'] -\n",
        "                          self.named.data.xpos['ball'])\n",
        "        return np.linalg.norm(ball_to_target[:2])\n",
        "\n",
        "    def self_to_ball_distance(self) -> Any:\n",
        "        \"\"\"Returns horizontal distance from the quadruped workspace to the ball.\"\"\"\n",
        "        self_to_ball = (self.named.data.site_xpos['workspace']\n",
        "                        - self.named.data.xpos['ball'])\n",
        "        return np.linalg.norm(self_to_ball[:2])\n",
        "\n",
        "\n",
        "def _find_non_contacting_height(physics, orientation, x_pos: float = 0.0, y_pos: float = 0.0) -> None:\n",
        "    \"\"\"Find a height with no contacts given a body orientation.\n",
        "    Args:\n",
        "      physics: An instance of `Physics`.\n",
        "      orientation: A quaternion.\n",
        "      x_pos: A float. Position along global x-axis.\n",
        "      y_pos: A float. Position along global y-axis.\n",
        "    Raises:\n",
        "      RuntimeError: If a non-contacting configuration has not been found after\n",
        "      10,000 attempts.\n",
        "    \"\"\"\n",
        "    z_pos = 0.0  # Start embedded in the floor.\n",
        "    num_contacts = 1\n",
        "    num_attempts = 0\n",
        "    # Move up in 1cm increments until no contacts.\n",
        "    while num_contacts > 0:\n",
        "        try:\n",
        "            with physics.reset_context():\n",
        "                physics.named.data.qpos['root'][:3] = x_pos, y_pos, z_pos\n",
        "                physics.named.data.qpos['root'][3:] = orientation\n",
        "        except control.PhysicsError:\n",
        "            # We may encounter a PhysicsError here due to filling the contact\n",
        "            # buffer, in which case we simply increment the height and continue.\n",
        "            pass\n",
        "        num_contacts = physics.data.ncon\n",
        "        z_pos += 0.01\n",
        "        num_attempts += 1\n",
        "        if num_attempts > 10000:\n",
        "            raise RuntimeError('Failed to find a non-contacting configuration.')\n",
        "\n",
        "\n",
        "def _common_observations(physics) -> tp.Dict[str, Any]:\n",
        "    \"\"\"Returns the observations common to all tasks.\"\"\"\n",
        "    obs = collections.OrderedDict()\n",
        "    obs['egocentric_state'] = physics.egocentric_state()\n",
        "    obs['torso_velocity'] = physics.torso_velocity()\n",
        "    obs['torso_upright'] = physics.torso_upright()\n",
        "    obs['imu'] = physics.imu()\n",
        "    obs['force_torque'] = physics.force_torque()\n",
        "    return obs\n",
        "\n",
        "\n",
        "def _upright_reward(physics, deviation_angle: int = 0):\n",
        "    \"\"\"Returns a reward proportional to how upright the torso is.\n",
        "    Args:\n",
        "      physics: an instance of `Physics`.\n",
        "      deviation_angle: A float, in degrees. The reward is 0 when the torso is\n",
        "        exactly upside-down and 1 when the torso's z-axis is less than\n",
        "        `deviation_angle` away from the global z-axis.\n",
        "    \"\"\"\n",
        "    deviation = np.cos(np.deg2rad(deviation_angle))\n",
        "    return rewards.tolerance(\n",
        "        physics.torso_upright(),\n",
        "        bounds=(deviation, float('inf')),\n",
        "        sigmoid='linear',\n",
        "        margin=1 + deviation,\n",
        "        value_at_margin=0)\n",
        "\n",
        "\n",
        "class Move(base.Task):\n",
        "    \"\"\"A quadruped task solved by moving forward at a designated speed.\"\"\"\n",
        "\n",
        "    def __init__(self, desired_speed, random=None) -> None:\n",
        "        \"\"\"Initializes an instance of `Move`.\n",
        "        Args:\n",
        "          desired_speed: A float. If this value is zero, reward is given simply\n",
        "            for standing upright. Otherwise this specifies the horizontal velocity\n",
        "            at which the velocity-dependent reward component is maximized.\n",
        "          random: Optional, either a `numpy.random.RandomState` instance, an\n",
        "            integer seed for creating a new `RandomState`, or None to select a seed\n",
        "            automatically (default).\n",
        "        \"\"\"\n",
        "        self._desired_speed = desired_speed\n",
        "        super().__init__(random=random)\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\n",
        "        Args:\n",
        "          physics: An instance of `Physics`.\n",
        "        \"\"\"\n",
        "        # Initial configuration.\n",
        "        orientation = self.random.randn(4)\n",
        "        orientation /= np.linalg.norm(orientation)\n",
        "        _find_non_contacting_height(physics, orientation)\n",
        "        super().initialize_episode(physics)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation to the agent.\"\"\"\n",
        "        return _common_observations(physics)\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward to the agent.\"\"\"\n",
        "\n",
        "        # Move reward term.\n",
        "        move_reward = rewards.tolerance(\n",
        "            physics.torso_velocity()[0],\n",
        "            bounds=(self._desired_speed, float('inf')),\n",
        "            margin=self._desired_speed,\n",
        "            value_at_margin=0.5,\n",
        "            sigmoid='linear')\n",
        "\n",
        "        return _upright_reward(physics) * move_reward\n",
        "\n",
        "\n",
        "class Stand(base.Task):\n",
        "    \"\"\"A quadruped task solved by moving forward at a designated speed.\"\"\"\n",
        "\n",
        "    def __init__(self, random=None) -> None:\n",
        "        \"\"\"Initializes an instance of `Move`.\n",
        "        Args:\n",
        "          desired_speed: A float. If this value is zero, reward is given simply\n",
        "            for standing upright. Otherwise this specifies the horizontal velocity\n",
        "            at which the velocity-dependent reward component is maximized.\n",
        "          random: Optional, either a `numpy.random.RandomState` instance, an\n",
        "            integer seed for creating a new `RandomState`, or None to select a seed\n",
        "            automatically (default).\n",
        "        \"\"\"\n",
        "        super().__init__(random=random)\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\n",
        "        Args:\n",
        "          physics: An instance of `Physics`.\n",
        "        \"\"\"\n",
        "        # Initial configuration.\n",
        "        orientation = self.random.randn(4)\n",
        "        orientation /= np.linalg.norm(orientation)\n",
        "        _find_non_contacting_height(physics, orientation)\n",
        "        super().initialize_episode(physics)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation to the agent.\"\"\"\n",
        "        return _common_observations(physics)\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward to the agent.\"\"\"\n",
        "\n",
        "        return _upright_reward(physics)\n",
        "\n",
        "\n",
        "class Jump(base.Task):\n",
        "    \"\"\"A quadruped task solved by moving forward at a designated speed.\"\"\"\n",
        "\n",
        "    def __init__(self, desired_height, random=None) -> None:\n",
        "        \"\"\"Initializes an instance of `Move`.\n",
        "        Args:\n",
        "          desired_speed: A float. If this value is zero, reward is given simply\n",
        "            for standing upright. Otherwise this specifies the horizontal velocity\n",
        "            at which the velocity-dependent reward component is maximized.\n",
        "          random: Optional, either a `numpy.random.RandomState` instance, an\n",
        "            integer seed for creating a new `RandomState`, or None to select a seed\n",
        "            automatically (default).\n",
        "        \"\"\"\n",
        "        self._desired_height = desired_height\n",
        "        super().__init__(random=random)\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\n",
        "        Args:\n",
        "          physics: An instance of `Physics`.\n",
        "        \"\"\"\n",
        "        # Initial configuration.\n",
        "        orientation = self.random.randn(4)\n",
        "        orientation /= np.linalg.norm(orientation)\n",
        "        _find_non_contacting_height(physics, orientation)\n",
        "        super().initialize_episode(physics)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation to the agent.\"\"\"\n",
        "        return _common_observations(physics)\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward to the agent.\"\"\"\n",
        "\n",
        "        # Move reward term.\n",
        "        jump_up = rewards.tolerance(\n",
        "            physics.com_height(),\n",
        "            bounds=(self._desired_height, float('inf')),\n",
        "            margin=self._desired_height,\n",
        "            value_at_margin=0.5,\n",
        "            sigmoid='linear')\n",
        "\n",
        "        return _upright_reward(physics) * jump_up\n",
        "\n",
        "\n",
        "class Roll(base.Task):\n",
        "    \"\"\"A quadruped task solved by moving forward at a designated speed.\"\"\"\n",
        "\n",
        "    def __init__(self, desired_speed, random=None) -> None:\n",
        "        \"\"\"Initializes an instance of `Move`.\n",
        "        Args:\n",
        "          desired_speed: A float. If this value is zero, reward is given simply\n",
        "            for standing upright. Otherwise this specifies the horizontal velocity\n",
        "            at which the velocity-dependent reward component is maximized.\n",
        "          random: Optional, either a `numpy.random.RandomState` instance, an\n",
        "            integer seed for creating a new `RandomState`, or None to select a seed\n",
        "            automatically (default).\n",
        "        \"\"\"\n",
        "        self._desired_speed = desired_speed\n",
        "        super().__init__(random=random)\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\n",
        "        Args:\n",
        "          physics: An instance of `Physics`.\n",
        "        \"\"\"\n",
        "        # Initial configuration.\n",
        "        orientation = self.random.randn(4)\n",
        "        orientation /= np.linalg.norm(orientation)\n",
        "        _find_non_contacting_height(physics, orientation)\n",
        "        super().initialize_episode(physics)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation to the agent.\"\"\"\n",
        "        return _common_observations(physics)\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward to the agent.\"\"\"\n",
        "        # Move reward term.\n",
        "        move_reward = rewards.tolerance(\n",
        "            np.linalg.norm(physics.torso_velocity()),\n",
        "            bounds=(self._desired_speed, float('inf')),\n",
        "            margin=self._desired_speed,\n",
        "            value_at_margin=0.5,\n",
        "            sigmoid='linear')\n",
        "\n",
        "        return _upright_reward(physics) * move_reward\n",
        "\n",
        "\n",
        "class Escape(base.Task):\n",
        "    \"\"\"A quadruped task solved by escaping a bowl-shaped terrain.\"\"\"\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\n",
        "        Args:\n",
        "          physics: An instance of `Physics`.\n",
        "        \"\"\"\n",
        "        # Get heightfield resolution, assert that it is square.\n",
        "        res = physics.model.hfield_nrow[_HEIGHTFIELD_ID]\n",
        "        assert res == physics.model.hfield_ncol[_HEIGHTFIELD_ID]\n",
        "        # Sinusoidal bowl shape.\n",
        "        row_grid, col_grid = np.ogrid[-1:1:res * 1j, -1:1:res * 1j]\n",
        "        radius = np.clip(np.sqrt(col_grid**2 + row_grid**2), .04, 1)\n",
        "        bowl_shape = .5 - np.cos(2 * np.pi * radius) / 2\n",
        "        # Random smooth bumps.\n",
        "        terrain_size = 2 * physics.model.hfield_size[_HEIGHTFIELD_ID, 0]\n",
        "        bump_res = int(terrain_size / _TERRAIN_BUMP_SCALE)\n",
        "        bumps = self.random.uniform(_TERRAIN_SMOOTHNESS, 1, (bump_res, bump_res))\n",
        "        smooth_bumps = ndimage.zoom(bumps, res / float(bump_res))\n",
        "        # Terrain is elementwise product.\n",
        "        terrain = bowl_shape * smooth_bumps\n",
        "        start_idx = physics.model.hfield_adr[_HEIGHTFIELD_ID]\n",
        "        physics.model.hfield_data[start_idx:start_idx + res**2] = terrain.ravel()\n",
        "        super().initialize_episode(physics)\n",
        "\n",
        "        # If we have a rendering context, we need to re-upload the modified\n",
        "        # heightfield data.\n",
        "        if physics.contexts:\n",
        "            with physics.contexts.gl.make_current() as ctx:\n",
        "                ctx.call(mjlib.mjr_uploadHField,\n",
        "                         physics.model.ptr,\n",
        "                         physics.contexts.mujoco.ptr,\n",
        "                         _HEIGHTFIELD_ID)\n",
        "\n",
        "        # Initial configuration.\n",
        "        orientation = self.random.randn(4)\n",
        "        orientation /= np.linalg.norm(orientation)\n",
        "        _find_non_contacting_height(physics, orientation)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation to the agent.\"\"\"\n",
        "        obs = _common_observations(physics)\n",
        "        obs['origin'] = physics.origin()\n",
        "        obs['rangefinder'] = physics.rangefinder()\n",
        "        return obs\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward to the agent.\"\"\"\n",
        "\n",
        "        # Escape reward term.\n",
        "        terrain_size = physics.model.hfield_size[_HEIGHTFIELD_ID, 0]\n",
        "        escape_reward = rewards.tolerance(\n",
        "            physics.origin_distance(),\n",
        "            bounds=(terrain_size, float('inf')),\n",
        "            margin=terrain_size,\n",
        "            value_at_margin=0,\n",
        "            sigmoid='linear')\n",
        "\n",
        "        return _upright_reward(physics, deviation_angle=20) * escape_reward\n",
        "\n",
        "\n",
        "class Fetch(base.Task):\n",
        "    \"\"\"A quadruped task solved by bringing a ball to the origin.\"\"\"\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\n",
        "        Args:\n",
        "          physics: An instance of `Physics`.\n",
        "        \"\"\"\n",
        "        # Initial configuration, random azimuth and horizontal position.\n",
        "        azimuth = self.random.uniform(0, 2 * np.pi)\n",
        "        orientation = np.array((np.cos(azimuth / 2), 0, 0, np.sin(azimuth / 2)))\n",
        "        spawn_radius = 0.9 * physics.named.model.geom_size['floor', 0]\n",
        "        x_pos, y_pos = self.random.uniform(-spawn_radius, spawn_radius, size=(2,))\n",
        "        _find_non_contacting_height(physics, orientation, x_pos, y_pos)\n",
        "\n",
        "        # Initial ball state.\n",
        "        physics.named.data.qpos['ball_root'][:2] = self.random.uniform(\n",
        "            -spawn_radius, spawn_radius, size=(2,))\n",
        "        physics.named.data.qpos['ball_root'][2] = 2\n",
        "        physics.named.data.qvel['ball_root'][:2] = 5 * self.random.randn(2)\n",
        "        super().initialize_episode(physics)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation to the agent.\"\"\"\n",
        "        obs = _common_observations(physics)\n",
        "        obs['ball_state'] = physics.ball_state()\n",
        "        obs['target_position'] = physics.target_position()\n",
        "        return obs\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward to the agent.\"\"\"\n",
        "\n",
        "        # Reward for moving close to the ball.\n",
        "        arena_radius = physics.named.model.geom_size['floor', 0] * np.sqrt(2)\n",
        "        workspace_radius = physics.named.model.site_size['workspace', 0]\n",
        "        ball_radius = physics.named.model.geom_size['ball', 0]\n",
        "        reach_reward = rewards.tolerance(\n",
        "            physics.self_to_ball_distance(),\n",
        "            bounds=(0, workspace_radius + ball_radius),\n",
        "            sigmoid='linear',\n",
        "            margin=arena_radius, value_at_margin=0)\n",
        "\n",
        "        # Reward for bringing the ball to the target.\n",
        "        target_radius = physics.named.model.site_size['target', 0]\n",
        "        fetch_reward = rewards.tolerance(\n",
        "            physics.ball_to_target_distance(),\n",
        "            bounds=(0, target_radius),\n",
        "            sigmoid='linear',\n",
        "            margin=arena_radius, value_at_margin=0)\n",
        "\n",
        "        reach_then_fetch = reach_reward * (0.5 + 0.5 * fetch_reward)\n",
        "\n",
        "        return _upright_reward(physics) * reach_then_fetch\n",
        "\n",
        "#fre/common/envs/exorl/custom_dmc_tasks/walker\n",
        "\"\"\"Planar Walker Domain.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "from typing import Any, Tuple\n",
        "import typing as tp\n",
        "import os\n",
        "\n",
        "from dm_control import mujoco\n",
        "from dm_control.rl import control\n",
        "from dm_control.suite import base\n",
        "from dm_control.suite import common\n",
        "from dm_control.suite.utils import randomizers\n",
        "from dm_control.utils import containers\n",
        "from dm_control.utils import rewards\n",
        "from dm_control.utils import io as resources\n",
        "\n",
        "_CONTROL_TIMESTEP: float\n",
        "_DEFAULT_TIME_LIMIT: int\n",
        "_RUN_SPEED: int\n",
        "_SPIN_SPEED: int\n",
        "_STAND_HEIGHT: float\n",
        "_WALK_SPEED: int\n",
        "# from dm_control import suite  # TODO useless?\n",
        "\n",
        "_DEFAULT_TIME_LIMIT = 25\n",
        "_CONTROL_TIMESTEP = .025\n",
        "\n",
        "# Minimal height of torso over foot above which stand reward is 1.\n",
        "_STAND_HEIGHT = 1.2\n",
        "\n",
        "# Horizontal speeds (meters/second) above which move reward is 1.\n",
        "_WALK_SPEED = 1\n",
        "_RUN_SPEED = 8\n",
        "_SPIN_SPEED = 5\n",
        "\n",
        "SUITE = containers.TaggedTasks()\n",
        "\n",
        "\n",
        "def make(task,\n",
        "         task_kwargs=None,\n",
        "         environment_kwargs=None,\n",
        "         visualize_reward: bool = False):\n",
        "    task_kwargs = task_kwargs or {}\n",
        "    if environment_kwargs is not None:\n",
        "        task_kwargs = task_kwargs.copy()\n",
        "        task_kwargs['environment_kwargs'] = environment_kwargs\n",
        "    env = SUITE[task](**task_kwargs)\n",
        "    env.task.visualize_reward = visualize_reward\n",
        "    return env\n",
        "\n",
        "\n",
        "def get_model_and_assets() -> Tuple[Any, Any]:\n",
        "    \"\"\"Returns a tuple containing the model XML string and a dict of assets.\"\"\"\n",
        "    root_dir = os.path.dirname(os.path.dirname(__file__))\n",
        "    xml = resources.GetResource(os.path.join(root_dir, 'custom_dmc_tasks',\n",
        "                                             'walker.xml'))\n",
        "    return xml, common.ASSETS\n",
        "\n",
        "\n",
        "@SUITE.add('benchmarking')\n",
        "def flip(time_limit: int = _DEFAULT_TIME_LIMIT,\n",
        "         random=None,\n",
        "         environment_kwargs=None):\n",
        "    \"\"\"Returns the Run task.\"\"\"\n",
        "    physics = Physics.from_xml_string(*get_model_and_assets())\n",
        "    task = PlanarWalker(move_speed=_RUN_SPEED,\n",
        "                        forward=True,\n",
        "                        flip=True,\n",
        "                        random=random)\n",
        "    environment_kwargs = environment_kwargs or {}\n",
        "    return control.Environment(physics,\n",
        "                               task,\n",
        "                               time_limit=time_limit,\n",
        "                               control_timestep=_CONTROL_TIMESTEP,\n",
        "                               **environment_kwargs)\n",
        "\n",
        "\n",
        "class Physics(mujoco.Physics):\n",
        "    \"\"\"Physics simulation with additional features for the Walker domain.\"\"\"\n",
        "\n",
        "    def torso_upright(self) -> Any:\n",
        "        \"\"\"Returns projection from z-axes of torso to the z-axes of world.\"\"\"\n",
        "        return self.named.data.xmat['torso', 'zz']\n",
        "\n",
        "    def torso_height(self) -> Any:\n",
        "        \"\"\"Returns the height of the torso.\"\"\"\n",
        "        return self.named.data.xpos['torso', 'z']\n",
        "\n",
        "    def horizontal_velocity(self) -> Any:\n",
        "        \"\"\"Returns the horizontal velocity of the center-of-mass.\"\"\"\n",
        "        return self.named.data.sensordata['torso_subtreelinvel'][0]\n",
        "\n",
        "    def orientations(self) -> Any:\n",
        "        \"\"\"Returns planar orientations of all bodies.\"\"\"\n",
        "        return self.named.data.xmat[1:, ['xx', 'xz']].ravel()\n",
        "\n",
        "    def angmomentum(self) -> Any:\n",
        "        \"\"\"Returns the angular momentum of torso of the Cheetah about Y axis.\"\"\"\n",
        "        return self.named.data.subtree_angmom['torso'][1]\n",
        "\n",
        "\n",
        "class PlanarWalker(base.Task):\n",
        "    \"\"\"A planar walker task.\"\"\"\n",
        "\n",
        "    def __init__(self, move_speed, forward=True, flip=False, random=None) -> None:\n",
        "        \"\"\"Initializes an instance of `PlanarWalker`.\n",
        "\n",
        "    Args:\n",
        "      move_speed: A float. If this value is zero, reward is given simply for\n",
        "        standing up. Otherwise this specifies a target horizontal velocity for\n",
        "        the walking task.\n",
        "      random: Optional, either a `numpy.random.RandomState` instance, an\n",
        "        integer seed for creating a new `RandomState`, or None to select a seed\n",
        "        automatically (default).\n",
        "    \"\"\"\n",
        "        self._move_speed = move_speed\n",
        "        self._forward = 1 if forward else -1\n",
        "        self._flip = flip\n",
        "        super(PlanarWalker, self).__init__(random=random)\n",
        "\n",
        "    def initialize_episode(self, physics) -> None:\n",
        "        \"\"\"Sets the state of the environment at the start of each episode.\n",
        "\n",
        "    In 'standing' mode, use initial orientation and small velocities.\n",
        "    In 'random' mode, randomize joint angles and let fall to the floor.\n",
        "\n",
        "    Args:\n",
        "      physics: An instance of `Physics`.\n",
        "\n",
        "    \"\"\"\n",
        "        randomizers.randomize_limited_and_rotational_joints(\n",
        "            physics, self.random)\n",
        "        super(PlanarWalker, self).initialize_episode(physics)\n",
        "\n",
        "    def get_observation(self, physics) -> tp.Dict[str, Any]:\n",
        "        \"\"\"Returns an observation of body orientations, height and velocites.\"\"\"\n",
        "        obs = collections.OrderedDict()\n",
        "        obs['orientations'] = physics.orientations()\n",
        "        obs['height'] = physics.torso_height()\n",
        "        obs['velocity'] = physics.velocity()\n",
        "        return obs\n",
        "\n",
        "    def get_reward(self, physics) -> Any:\n",
        "        \"\"\"Returns a reward to the agent.\"\"\"\n",
        "        standing = rewards.tolerance(physics.torso_height(),\n",
        "                                     bounds=(_STAND_HEIGHT, float('inf')),\n",
        "                                     margin=_STAND_HEIGHT / 2)\n",
        "        upright = (1 + physics.torso_upright()) / 2\n",
        "        stand_reward = (3 * standing + upright) / 4\n",
        "\n",
        "        if self._flip:\n",
        "            move_reward = rewards.tolerance(self._forward *\n",
        "                                            physics.angmomentum(),\n",
        "                                            bounds=(_SPIN_SPEED, float('inf')),\n",
        "                                            margin=_SPIN_SPEED,\n",
        "                                            value_at_margin=0,\n",
        "                                            sigmoid='linear')\n",
        "        else:\n",
        "            move_reward = rewards.tolerance(\n",
        "                self._forward * physics.horizontal_velocity(),\n",
        "                bounds=(self._move_speed, float('inf')),\n",
        "                margin=self._move_speed / 2,\n",
        "                value_at_margin=0.5,\n",
        "                sigmoid='linear')\n",
        "\n",
        "        return stand_reward * (5 * move_reward + 1) / 6\n",
        "\n",
        "#fre/common/envs/exorl/dmc.py\n",
        "import unittest\n",
        "import dataclasses\n",
        "from collections import OrderedDict, deque\n",
        "import typing as tp\n",
        "from typing import Any\n",
        "\n",
        "from gym import core, spaces\n",
        "from dm_env import Environment\n",
        "from dm_env import StepType, specs\n",
        "from dm_control import suite\n",
        "from dm_control.suite.wrappers import action_scale, pixels\n",
        "\"\"\"import fre.common.envs.exorl.custom_dmc_tasks as cdmc\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "S = tp.TypeVar(\"S\", bound=\"TimeStep\")\n",
        "Env = tp.Union[\"EnvWrapper\", Environment]\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class TimeStep:\n",
        "    step_type: StepType\n",
        "    reward: float\n",
        "    discount: float\n",
        "    observation: np.ndarray\n",
        "    physics: np.ndarray = dataclasses.field(default=np.ndarray([]), init=False)\n",
        "\n",
        "    def first(self) -> bool:\n",
        "        return self.step_type == StepType.FIRST  # type: ignore\n",
        "\n",
        "    def mid(self) -> bool:\n",
        "        return self.step_type == StepType.MID  # type: ignore\n",
        "\n",
        "    def last(self) -> bool:\n",
        "        return self.step_type == StepType.LAST  # type: ignore\n",
        "\n",
        "    def __getitem__(self, attr: str) -> tp.Any:\n",
        "        return getattr(self, attr)\n",
        "\n",
        "    def _replace(self: S, **kwargs: tp.Any) -> S:\n",
        "        for name, val in kwargs.items():\n",
        "            setattr(self, name, val)\n",
        "        return self\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class ExtendedTimeStep(TimeStep):\n",
        "    action: tp.Any\n",
        "\n",
        "\n",
        "class EnvWrapper:\n",
        "    def __init__(self, env: Env) -> None:\n",
        "        self._env = env\n",
        "\n",
        "    def _augment_time_step(self, time_step: TimeStep, action: tp.Optional[np.ndarray] = None) -> TimeStep:\n",
        "        if not isinstance(time_step, TimeStep):\n",
        "            # dm_env time step is a named tuple\n",
        "            time_step = TimeStep(**time_step._asdict())\n",
        "        if self.physics is not None:\n",
        "            return time_step._replace(physics=self.physics.get_state())\n",
        "        else:\n",
        "            return time_step\n",
        "\n",
        "    def reset(self) -> TimeStep:\n",
        "        time_step = self._env.reset()\n",
        "        return self._augment_time_step(time_step)\n",
        "\n",
        "    def step(self, action: np.ndarray) -> TimeStep:\n",
        "        time_step = self._env.step(action)\n",
        "        return self._augment_time_step(time_step, action)\n",
        "\n",
        "    def observation_spec(self) -> tp.Any:\n",
        "        assert isinstance(self, EnvWrapper)\n",
        "        return self._env.observation_spec()\n",
        "\n",
        "    def action_spec(self) -> specs.Array:\n",
        "        return self._env.action_spec()\n",
        "\n",
        "    def render(self, *args: tp.Any, **kwargs: tp.Any) -> np.ndarray:\n",
        "        return self._env.render(*args, **kwargs)  # type: ignore\n",
        "\n",
        "    @property\n",
        "    def base_env(self) -> tp.Any:\n",
        "        env = self._env\n",
        "        if isinstance(env, EnvWrapper):\n",
        "            return self.base_env\n",
        "        return env\n",
        "\n",
        "    @property\n",
        "    def physics(self) -> tp.Any:\n",
        "        if hasattr(self._env, \"physics\"):\n",
        "            return self._env.physics\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self._env, name)\n",
        "\n",
        "\n",
        "class FlattenJacoObservationWrapper(EnvWrapper):\n",
        "    def __init__(self, env: Env) -> None:\n",
        "        super().__init__(env)\n",
        "        self._obs_spec = OrderedDict()\n",
        "        wrapped_obs_spec = env.observation_spec().copy()\n",
        "        if 'front_close' in wrapped_obs_spec:\n",
        "            spec = wrapped_obs_spec['front_close']\n",
        "            # drop batch dim\n",
        "            self._obs_spec['pixels'] = specs.BoundedArray(shape=spec.shape[1:],\n",
        "                                                          dtype=spec.dtype,\n",
        "                                                          minimum=spec.minimum,\n",
        "                                                          maximum=spec.maximum,\n",
        "                                                          name='pixels')\n",
        "            wrapped_obs_spec.pop('front_close')\n",
        "\n",
        "        for spec in wrapped_obs_spec.values():\n",
        "            assert spec.dtype == np.float64\n",
        "            assert type(spec) == specs.Array\n",
        "        dim = np.sum(\n",
        "            np.fromiter((int(np.prod(spec.shape))  # type: ignore\n",
        "                         for spec in wrapped_obs_spec.values()), np.int32))\n",
        "\n",
        "        self._obs_spec['observations'] = specs.Array(shape=(dim,),\n",
        "                                                     dtype=np.float32,\n",
        "                                                     name='observations')\n",
        "\n",
        "    def observation_spec(self) -> tp.Any:\n",
        "        return self._obs_spec\n",
        "\n",
        "    def _augment_time_step(self, time_step: TimeStep, action: tp.Optional[np.ndarray] = None) -> TimeStep:\n",
        "        super()._augment_time_step(time_step=time_step, action=action)\n",
        "        obs = OrderedDict()\n",
        "\n",
        "        # TODO: this is badly typed since observation is a dict in this case\n",
        "        if 'front_close' in time_step.observation:\n",
        "            pixels = time_step.observation['front_close']\n",
        "            time_step.observation.pop('front_close')  # type: ignore\n",
        "            pixels = np.squeeze(pixels)\n",
        "            obs['pixels'] = pixels\n",
        "\n",
        "        features = []\n",
        "        for feature in time_step.observation.values():  # type: ignore\n",
        "            features.append(feature.ravel())\n",
        "        obs['observations'] = np.concatenate(features, axis=0)\n",
        "        return time_step._replace(observation=obs)\n",
        "\n",
        "\n",
        "class ActionRepeatWrapper(EnvWrapper):\n",
        "    def __init__(self, env: tp.Any, num_repeats: int) -> None:\n",
        "        super().__init__(env)\n",
        "        self._num_repeats = num_repeats\n",
        "\n",
        "    def step(self, action: np.ndarray) -> TimeStep:\n",
        "        reward = 0.0\n",
        "        discount = 1.0\n",
        "        for _ in range(self._num_repeats):\n",
        "            time_step = self._env.step(action)\n",
        "            reward += (time_step.reward or 0.0) * discount\n",
        "            discount *= time_step.discount\n",
        "            if time_step.last():\n",
        "                break\n",
        "\n",
        "        return time_step._replace(reward=reward, discount=discount)\n",
        "\n",
        "\n",
        "class FrameStackWrapper(EnvWrapper):\n",
        "    def __init__(self, env: Env, num_frames: int, pixels_key: str = 'pixels') -> None:\n",
        "        super().__init__(env)\n",
        "        self._num_frames = num_frames\n",
        "        self._frames: tp.Deque[np.ndarray] = deque([], maxlen=num_frames)\n",
        "        self._pixels_key = pixels_key\n",
        "\n",
        "        wrapped_obs_spec = env.observation_spec()\n",
        "        assert pixels_key in wrapped_obs_spec\n",
        "\n",
        "        pixels_shape = wrapped_obs_spec[pixels_key].shape\n",
        "        # remove batch dim\n",
        "        if len(pixels_shape) == 4:\n",
        "            pixels_shape = pixels_shape[1:]\n",
        "        self._obs_spec = specs.BoundedArray(shape=np.concatenate(\n",
        "            [[pixels_shape[2] * num_frames], pixels_shape[:2]], axis=0),\n",
        "            dtype=np.uint8,\n",
        "            minimum=0,\n",
        "            maximum=255,\n",
        "            name='observation')\n",
        "\n",
        "    def _augment_time_step(self, time_step: TimeStep, action: tp.Optional[np.ndarray] = None) -> TimeStep:\n",
        "        super()._augment_time_step(time_step=time_step, action=action)\n",
        "        assert len(self._frames) == self._num_frames\n",
        "        obs = np.concatenate(list(self._frames), axis=0)\n",
        "        return time_step._replace(observation=obs)\n",
        "\n",
        "    def _extract_pixels(self, time_step: TimeStep) -> np.ndarray:\n",
        "        pixels_ = time_step.observation[self._pixels_key]\n",
        "        # remove batch dim\n",
        "        if len(pixels_.shape) == 4:\n",
        "            pixels_ = pixels_[0]\n",
        "        return pixels_.transpose(2, 0, 1).copy()\n",
        "\n",
        "    def reset(self) -> TimeStep:\n",
        "        time_step = self._env.reset()\n",
        "        pixels_ = self._extract_pixels(time_step)\n",
        "        for _ in range(self._num_frames):\n",
        "            self._frames.append(pixels_)\n",
        "        return self._augment_time_step(time_step)\n",
        "\n",
        "    def step(self, action: np.ndarray) -> TimeStep:\n",
        "        time_step = self._env.step(action)\n",
        "        pixels_ = self._extract_pixels(time_step)\n",
        "        self._frames.append(pixels_)\n",
        "        return self._augment_time_step(time_step)\n",
        "\n",
        "\n",
        "class ActionDTypeWrapper(EnvWrapper):\n",
        "    def __init__(self, env: Env, dtype) -> None:\n",
        "        super().__init__(env)\n",
        "        wrapped_action_spec = env.action_spec()\n",
        "        self._action_spec = specs.BoundedArray(wrapped_action_spec.shape,\n",
        "                                               dtype,\n",
        "                                               wrapped_action_spec.minimum,\n",
        "                                               wrapped_action_spec.maximum,\n",
        "                                               'action')\n",
        "\n",
        "    def action_spec(self) -> specs.BoundedArray:\n",
        "        return self._action_spec\n",
        "\n",
        "    def step(self, action) -> Any:\n",
        "        action = action.astype(self._env.action_spec().dtype)\n",
        "        return self._env.step(action)\n",
        "\n",
        "\n",
        "class ObservationDTypeWrapper(EnvWrapper):\n",
        "    def __init__(self, env: Env, dtype) -> None:\n",
        "        super().__init__(env)\n",
        "        self._dtype = dtype\n",
        "        wrapped_obs_spec = env.observation_spec()['observations']\n",
        "        self._obs_spec = specs.Array(wrapped_obs_spec.shape, dtype,\n",
        "                                     'observation')\n",
        "\n",
        "    def _augment_time_step(self, time_step: TimeStep, action: tp.Optional[np.ndarray] = None) -> TimeStep:\n",
        "        obs = time_step.observation['observations'].astype(self._dtype)\n",
        "        return time_step._replace(observation=obs)\n",
        "\n",
        "    def observation_spec(self) -> Any:\n",
        "        return self._obs_spec\n",
        "\n",
        "\n",
        "class ExtendedTimeStepWrapper(EnvWrapper):\n",
        "\n",
        "    def _augment_time_step(self, time_step: TimeStep, action: tp.Optional[np.ndarray] = None) -> TimeStep:\n",
        "        if action is None:\n",
        "            action_spec = self.action_spec()\n",
        "            action = np.zeros(action_spec.shape, dtype=action_spec.dtype)\n",
        "        ts = ExtendedTimeStep(observation=time_step.observation,\n",
        "                              step_type=time_step.step_type,\n",
        "                              action=action,\n",
        "                              reward=time_step.reward or 0.0,\n",
        "                              discount=time_step.discount or 1.0)\n",
        "        return super()._augment_time_step(time_step=ts, action=action)\n",
        "\n",
        "\n",
        "def _make_jaco(obs_type, domain, task, frame_stack, action_repeat, seed) -> FlattenJacoObservationWrapper:\n",
        "    env = cdmc.make_jaco(task, obs_type, seed)\n",
        "    env = ActionDTypeWrapper(env, np.float32)\n",
        "    env = ActionRepeatWrapper(env, action_repeat)\n",
        "    env = FlattenJacoObservationWrapper(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "def _make_dmc(obs_type, domain, task, frame_stack, action_repeat, seed):\n",
        "    visualize_reward = False\n",
        "    if (domain, task) in suite.ALL_TASKS:\n",
        "        env = suite.load(domain,\n",
        "                         task,\n",
        "                         task_kwargs=dict(random=seed),\n",
        "                         environment_kwargs=dict(flat_observation=True),\n",
        "                         visualize_reward=visualize_reward)\n",
        "    else:\n",
        "        env = cdmc.make(domain,\n",
        "                        task,\n",
        "                        task_kwargs=dict(random=seed),\n",
        "                        environment_kwargs=dict(flat_observation=True),\n",
        "                        visualize_reward=visualize_reward)\n",
        "    env = ActionDTypeWrapper(env, np.float32)\n",
        "    env = ActionRepeatWrapper(env, action_repeat)\n",
        "    if obs_type == 'pixels':\n",
        "        # zoom in camera for quadruped\n",
        "        camera_id = dict(quadruped=2).get(domain, 0)\n",
        "        render_kwargs = dict(height=84, width=84, camera_id=camera_id)\n",
        "        env = pixels.Wrapper(env,\n",
        "                             pixels_only=True,\n",
        "                             render_kwargs=render_kwargs)\n",
        "    return env\n",
        "\n",
        "\n",
        "def make(\n",
        "    name: str, obs_type='states', frame_stack=1, action_repeat=1,\n",
        "    seed=1,\n",
        ") -> EnvWrapper:\n",
        "    assert obs_type in ['states', 'pixels']\n",
        "    if name.startswith('point_mass_maze'):\n",
        "        domain = 'point_mass_maze'\n",
        "        _, _, _, task = name.split('_', 3)\n",
        "    else:\n",
        "        domain, task = name.split('_', 1)\n",
        "    domain = dict(cup='ball_in_cup').get(domain, domain)\n",
        "\n",
        "    make_fn = _make_jaco if domain == 'jaco' else _make_dmc\n",
        "    env = make_fn(obs_type, domain, task, frame_stack, action_repeat, seed)\n",
        "\n",
        "    if obs_type == 'pixels':\n",
        "        env = FrameStackWrapper(env, frame_stack)\n",
        "    else:\n",
        "        env = ObservationDTypeWrapper(env, np.float32)\n",
        "\n",
        "    env = action_scale.Wrapper(env, minimum=-1.0, maximum=+1.0)\n",
        "    env = ExtendedTimeStepWrapper(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "def extract_physics(env: Env) -> tp.Dict[str, float]:\n",
        "    \"\"\"Extract some physics available in the env\"\"\"\n",
        "    output = {}\n",
        "    names = [\"torso_height\", \"torso_upright\", \"horizontal_velocity\", \"torso_velocity\"]\n",
        "    for name in names:\n",
        "        if not hasattr(env.physics, name):\n",
        "            continue\n",
        "        val: tp.Union[float, np.ndarray] = getattr(env.physics, name)()\n",
        "        if isinstance(val, (int, float)) or not val.ndim:\n",
        "            output[name] = float(val)\n",
        "        else:\n",
        "            for k, v in enumerate(val):\n",
        "                output[f\"{name}#{k}\"] = float(v)\n",
        "    return output\n",
        "\n",
        "\n",
        "class FloatStats:\n",
        "    \"\"\"Handle for keeping track of the statistics of a float variable\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.min = np.inf\n",
        "        self.max = -np.inf\n",
        "        self.mean = 0.0\n",
        "        self._count = 0\n",
        "\n",
        "    def add(self, value: float) -> \"FloatStats\":\n",
        "        self.min = min(value, self.min)\n",
        "        self.max = max(value, self.max)\n",
        "        self._count += 1\n",
        "        self.mean = (self._count - 1) / self._count * self.mean + 1 / self._count * value\n",
        "        return self\n",
        "\n",
        "    def items(self) -> tp.Iterator[tp.Tuple[str, float]]:\n",
        "        for name, val in self.__dict__.items():\n",
        "            if not name.startswith(\"_\"):\n",
        "                yield name, val\n",
        "\n",
        "\n",
        "class PhysicsAggregator:\n",
        "    \"\"\"Aggregate stats on the physics of an environment\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.stats: tp.Dict[str, FloatStats] = {}\n",
        "\n",
        "    def add(self, env: Env) -> \"PhysicsAggregator\":\n",
        "        phy = extract_physics(env)\n",
        "        for key, val in phy.items():\n",
        "            self.stats.setdefault(key, FloatStats()).add(val)\n",
        "        return self\n",
        "\n",
        "    def dump(self) -> tp.Iterator[tp.Tuple[str, float]]:\n",
        "        \"\"\"Exports all statistics and reset the statistics\"\"\"\n",
        "        for key, stats in self.stats.items():\n",
        "            for stat, val in stats.items():\n",
        "                yield (f'{key}/{stat}', val)\n",
        "        self.stats.clear()\n",
        "\n",
        "\n",
        "def _spec_to_box(spec, dtype):\n",
        "    def extract_min_max(s):\n",
        "        assert s.dtype == np.float64 or s.dtype == np.float32\n",
        "        dim = int(np.prod(s.shape))\n",
        "        if type(s) == specs.Array:\n",
        "            bound = np.inf * np.ones(dim, dtype=np.float32)\n",
        "            return -bound, bound\n",
        "        elif type(s) == specs.BoundedArray:\n",
        "            zeros = np.zeros(dim, dtype=np.float32)\n",
        "            return s.minimum + zeros, s.maximum + zeros\n",
        "\n",
        "    mins, maxs = [], []\n",
        "    for s in spec:\n",
        "        mn, mx = extract_min_max(s)\n",
        "        mins.append(mn)\n",
        "        maxs.append(mx)\n",
        "    low = np.concatenate(mins, axis=0).astype(dtype)\n",
        "    high = np.concatenate(maxs, axis=0).astype(dtype)\n",
        "    assert low.shape == high.shape\n",
        "    return spaces.Box(low, high, dtype=dtype)\n",
        "\n",
        "\n",
        "def _flatten_obs(obs):\n",
        "    obs_pieces = []\n",
        "    v = obs\n",
        "    flat = np.array([v]) if np.isscalar(v) else v.ravel()\n",
        "    obs_pieces.append(flat)\n",
        "    return np.concatenate(obs_pieces, axis=0)\n",
        "\n",
        "\n",
        "class DMCWrapper(core.Env):\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            seed,\n",
        "            from_pixels=False,\n",
        "            height=84,\n",
        "            width=84,\n",
        "            camera_id=0,\n",
        "            frame_skip=1,\n",
        "            channels_first=True,\n",
        "    ):\n",
        "        self._from_pixels = from_pixels\n",
        "        self._height = height\n",
        "        self._width = width\n",
        "        self._camera_id = camera_id\n",
        "        self._frame_skip = frame_skip\n",
        "        self._channels_first = channels_first\n",
        "\n",
        "        self._env = env\n",
        "\n",
        "        # true and normalized action spaces\n",
        "        self._true_action_space = _spec_to_box([self._env.action_spec()], np.float32)\n",
        "        self._norm_action_space = spaces.Box(\n",
        "            low=-1.0,\n",
        "            high=1.0,\n",
        "            shape=self._true_action_space.shape,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # create observation space\n",
        "        if from_pixels:\n",
        "            shape = [3, height, width] if channels_first else [height, width, 3]\n",
        "            self._observation_space = spaces.Box(\n",
        "                low=0, high=255, shape=shape, dtype=np.uint8\n",
        "            )\n",
        "        else:\n",
        "            self._observation_space = _spec_to_box([self._env.observation_spec()], np.float32)\n",
        "\n",
        "        self._state_space = _spec_to_box([self._env.observation_spec()], np.float32)\n",
        "\n",
        "        self.current_state = None\n",
        "\n",
        "        self.seed(seed)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self._env, name)\n",
        "\n",
        "    def _get_obs(self, time_step):\n",
        "        if self._from_pixels:\n",
        "            obs = self.render(\n",
        "                height=self._height,\n",
        "                width=self._width,\n",
        "                camera_id=self._camera_id\n",
        "            )\n",
        "            if self._channels_first:\n",
        "                obs = obs.transpose(2, 0, 1).copy()\n",
        "        else:\n",
        "            obs = _flatten_obs(time_step.observation)\n",
        "        return obs\n",
        "\n",
        "    def _convert_action(self, action):\n",
        "        true_delta = self._true_action_space.high - self._true_action_space.low\n",
        "        norm_delta = self._norm_action_space.high - self._norm_action_space.low\n",
        "        action = (action - self._norm_action_space.low) / norm_delta\n",
        "        action = action * true_delta + self._true_action_space.low\n",
        "        return action\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return self._observation_space\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        return self._state_space\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return self._norm_action_space\n",
        "\n",
        "    @property\n",
        "    def reward_range(self):\n",
        "        return 0, self._frame_skip\n",
        "\n",
        "    def seed(self, seed):\n",
        "        self._true_action_space.seed(seed)\n",
        "        self._norm_action_space.seed(seed)\n",
        "        self._observation_space.seed(seed)\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self._norm_action_space.contains(action)\n",
        "        action = self._convert_action(action)\n",
        "        assert self._true_action_space.contains(action)\n",
        "        reward = 0\n",
        "        extra = {'internal_state': self._env.physics.get_state().copy()}\n",
        "\n",
        "        for _ in range(self._frame_skip):\n",
        "            time_step = self._env.step(action)\n",
        "            reward += time_step.reward or 0\n",
        "            done = time_step.last()\n",
        "            if done:\n",
        "                break\n",
        "        obs = self._get_obs(time_step)\n",
        "        self.current_state = _flatten_obs(time_step.observation)\n",
        "        extra['discount'] = time_step.discount\n",
        "        return obs, reward, done, extra\n",
        "\n",
        "    def reset(self):\n",
        "        time_step = self._env.reset()\n",
        "        self.current_state = _flatten_obs(time_step.observation)\n",
        "        obs = self._get_obs(time_step)\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode='rgb_array', height=None, width=None, camera_id=0):\n",
        "        assert mode == 'rgb_array', 'only support rgb_array mode, given %s' % mode\n",
        "        height = height or self._height\n",
        "        width = width or self._width\n",
        "        camera_id = camera_id or self._camera_id\n",
        "        return self._env.physics.render(\n",
        "            height=height, width=width, camera_id=camera_id\n",
        "        )\n",
        "\n",
        "#fre/common/envs/exorl/exorl_utils.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# from fre.common.envs.d4rl import d4rl_utils\n",
        "#from fre.common.dataset import Dataset\n",
        "\n",
        "# get path relative to 'fre' package\n",
        "data_path = open('/content/download.py')\n",
        "print(\"Path to exorl data is\", data_path)\n",
        "\n",
        "def get_dataset(env, env_name, method='rnd', dmc_dataset_size=10000000, use_task_reward=True):\n",
        "\n",
        "    # dmc_dataset_size /= 10\n",
        "    # print(\"WARNING: Only using 10 percent of exorl data.\")\n",
        "\n",
        "    domain_name, task_name = env_name.split('_', 1)\n",
        "\n",
        "    path = os.path.join(data_path, domain_name, method)\n",
        "    if not os.path.exists(path):\n",
        "        print(\"Downloading exorl data.\")\n",
        "        os.makedirs(path)\n",
        "        url = \"https://dl.fbaipublicfiles.com/exorl/\" + domain_name + \"/\" + method + \".zip\"\n",
        "        print(\"Downloading from\", url)\n",
        "        os.system(\"wget \" + url + \" -P \" + path)\n",
        "        os.system(\"unzip \" + path + \"/\" + method + \".zip -d \" + path)\n",
        "\n",
        "    # process data into Dataset object.\n",
        "    path = os.path.join(data_path, domain_name, method, 'buffer')\n",
        "    npzs = sorted(glob.glob(f'{path}/*.npz'))\n",
        "    dataset_npy = os.path.join(data_path, domain_name, method, task_name + '.npy')\n",
        "    if os.path.exists(dataset_npy):\n",
        "        dataset = np.load(dataset_npy, allow_pickle=True).item()\n",
        "    else:\n",
        "        print(\"Calculating exorl rewards.\")\n",
        "        dataset = defaultdict(list)\n",
        "        num_steps = 0\n",
        "        for i, npz in tqdm.tqdm(enumerate(npzs)):\n",
        "            traj_data = dict(np.load(npz))\n",
        "            dataset['observations'].append(traj_data['observation'][:-1, :])\n",
        "            dataset['next_observations'].append(traj_data['observation'][1:, :])\n",
        "            dataset['actions'].append(traj_data['action'][1:, :])\n",
        "            dataset['physics'].append(traj_data['physics'][1:, :])  # Note that this corresponds to next_observations (i.e., r(s, a, s') = r(s') -- following the original DMC rewards)\n",
        "\n",
        "            if use_task_reward:\n",
        "                # TODO: make this faster and sanity check it\n",
        "                rewards = []\n",
        "                reward_spec = env.reward_spec()\n",
        "                states = traj_data['physics']\n",
        "                for j in range(states.shape[0]):\n",
        "                    with env.physics.reset_context():\n",
        "                        env.physics.set_state(states[j])\n",
        "                    reward = env.task.get_reward(env.physics)\n",
        "                    reward = np.full(reward_spec.shape, reward, reward_spec.dtype)\n",
        "                    rewards.append(reward)\n",
        "                traj_data['reward'] = np.array(rewards, dtype=reward_spec.dtype)\n",
        "                dataset['rewards'].append(traj_data['reward'][1:])\n",
        "            else:\n",
        "                dataset['rewards'].append(traj_data['reward'][1:, 0])\n",
        "\n",
        "            terminals = np.full((len(traj_data['observation']) - 1,), False)\n",
        "            dataset['terminals'].append(terminals)\n",
        "            num_steps += len(traj_data['observation']) - 1\n",
        "            if num_steps >= dmc_dataset_size:\n",
        "                break\n",
        "        print(\"Loaded {} steps\".format(num_steps))\n",
        "        for k, v in dataset.items():\n",
        "            dataset[k] = np.concatenate(v, axis=0)\n",
        "        np.save(dataset_npy, dataset)\n",
        "\n",
        "\n",
        "\n",
        "    # Processing\n",
        "    masks = 1.0 - dataset['terminals']\n",
        "    dones_float = dataset['terminals']\n",
        "\n",
        "    return Dataset.create(\n",
        "        observations=dataset['observations'],\n",
        "        actions=dataset['actions'],\n",
        "        rewards=dataset['rewards'],\n",
        "        masks=masks,\n",
        "        dones_float=dones_float,\n",
        "        next_observations=dataset['next_observations'],\n",
        "    )\n",
        "\n",
        "#fre/common/networks/basic.py\n",
        "###############################\n",
        "#\n",
        "#  Common Flax Networks.\n",
        "#\n",
        "###############################\n",
        "#from fre.common.typing import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install distrax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plt6RQLGlgOg",
        "outputId": "d25496de-a8b7-4fea-bd5e-cd3272efc4a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting distrax\n",
            "  Downloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/319.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m307.2/319.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from distrax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from distrax) (0.1.86)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from distrax) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.1.67 in /usr/local/lib/python3.10/dist-packages (from distrax) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from distrax) (1.25.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from distrax) (0.23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.8->distrax) (4.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.8->distrax) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->distrax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->distrax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->distrax) (1.11.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.1.8)\n",
            "Installing collected packages: distrax\n",
            "Successfully installed distrax-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import distrax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from dataclasses import field\n",
        "\n",
        "###############################\n",
        "#\n",
        "#  Common Networks\n",
        "#\n",
        "###############################\n",
        "\n",
        "def mish(x):\n",
        "    return x * jnp.tanh(nn.softplus(x))\n",
        "\n",
        "def default_init(scale: Optional[float] = 1.0):\n",
        "    return nn.initializers.variance_scaling(scale, \"fan_avg\", \"uniform\")\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = mish\n",
        "    activate_final: int = False\n",
        "    use_layer_norm: bool = True\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_init()\n",
        "\n",
        "    def setup(self):\n",
        "        self.layers = [\n",
        "            nn.Dense(size, kernel_init=self.kernel_init) for size in self.hidden_dims\n",
        "        ]\n",
        "        if self.use_layer_norm:\n",
        "            self.layer_norms = [nn.LayerNorm() for _ in self.hidden_dims]\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i + 1 < len(self.layers) and self.use_layer_norm:\n",
        "                x = self.layer_norms[i](x)\n",
        "            if i + 1 < len(self.layers) or self.activate_final:\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "###############################\n",
        "#\n",
        "#  Common RL Networks\n",
        "#\n",
        "###############################\n",
        "\n",
        "\n",
        "# DQN-style critic.\n",
        "class DiscreteCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    n_actions: int\n",
        "    mlp_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
        "        return MLP((*self.hidden_dims, self.n_actions), **self.mlp_kwargs)(\n",
        "            observations\n",
        "        )\n",
        "\n",
        "# Q(s,a) critic.\n",
        "class Critic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    mlp_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray, actions: jnp.ndarray) -> jnp.ndarray:\n",
        "        inputs = jnp.concatenate([observations, actions], -1)\n",
        "        critic = MLP((*self.hidden_dims, 1), **self.mlp_kwargs)(inputs)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "# V(s) critic.\n",
        "class ValueCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    mlp_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
        "        critic = MLP((*self.hidden_dims, 1), **self.mlp_kwargs)(observations)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "# pi(a|s). Returns a distrax distribution.\n",
        "class Policy(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    mlp_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    is_discrete: bool = False\n",
        "    log_std_min: Optional[float] = -20\n",
        "    log_std_max: Optional[float] = 2\n",
        "    mean_min: Optional[float] = -5\n",
        "    mean_max: Optional[float] = 5\n",
        "    tanh_squash_distribution: bool = False\n",
        "    state_dependent_std: bool = True\n",
        "    final_fc_init_scale: float = 1e-2\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observations: jnp.ndarray, temperature: float = 1.0\n",
        "    ) -> distrax.Distribution:\n",
        "        outputs = MLP(\n",
        "            self.hidden_dims,\n",
        "            activate_final=True,\n",
        "            **self.mlp_kwargs\n",
        "        )(observations)\n",
        "\n",
        "        if self.is_discrete:\n",
        "            logits = nn.Dense(\n",
        "                self.action_dim, kernel_init=default_init(self.final_fc_init_scale)\n",
        "            )(outputs)\n",
        "            distribution = distrax.Categorical(logits=logits / jnp.maximum(1e-6, temperature))\n",
        "        else:\n",
        "            means = nn.Dense(\n",
        "                self.action_dim, kernel_init=default_init(self.final_fc_init_scale)\n",
        "            )(outputs)\n",
        "            if self.state_dependent_std:\n",
        "                log_stds = nn.Dense(\n",
        "                    self.action_dim, kernel_init=default_init(self.final_fc_init_scale)\n",
        "                )(outputs)\n",
        "            else:\n",
        "                log_stds = self.param(\"log_stds\", nn.initializers.zeros, (self.action_dim,))\n",
        "\n",
        "            log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n",
        "            means = jnp.clip(means, self.mean_min, self.mean_max)\n",
        "\n",
        "            distribution = distrax.MultivariateNormalDiag(\n",
        "                loc=means, scale_diag=jnp.exp(log_stds) * temperature\n",
        "            )\n",
        "            if self.tanh_squash_distribution:\n",
        "                distribution = TransformedWithMode(\n",
        "                    distribution, distrax.Block(distrax.Tanh(), ndims=1)\n",
        "                )\n",
        "        return distribution\n",
        "\n",
        "###############################\n",
        "#\n",
        "#   Helper Things\n",
        "#\n",
        "###############################\n",
        "\n",
        "\n",
        "class TransformedWithMode(distrax.Transformed):\n",
        "    def mode(self) -> jnp.ndarray:\n",
        "        return self.bijector.forward(self.distribution.mode())\n",
        "\n",
        "def ensemblize(cls, num_qs, out_axes=0, **kwargs):\n",
        "    \"\"\"\n",
        "    Useful for making ensembles of Q functions (e.g. double Q in SAC).\n",
        "\n",
        "    Usage:\n",
        "\n",
        "        critic_def = ensemblize(Critic, 2)(hidden_dims=hidden_dims)\n",
        "\n",
        "    \"\"\"\n",
        "    return nn.vmap(\n",
        "        cls,\n",
        "        variable_axes={\"params\": 0},\n",
        "        split_rngs={\"params\": True},\n",
        "        in_axes=None,\n",
        "        out_axes=out_axes,\n",
        "        axis_size=num_qs,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "#fre/common/networks/transformers\n",
        "from typing import Any, Callable, Optional, Tuple, Type\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "Array = Any\n",
        "PRNGKey = Any\n",
        "Shape = Tuple[int]\n",
        "Dtype = Any\n",
        "\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    \"\"\"Identity layer, convenient for giving a name to an array.\"\"\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class AddPositionEmbs(nn.Module):\n",
        "    # Need to define function that adds the poisition embeddings to the input.\n",
        "    posemb_init: Callable[[PRNGKey, Shape, Dtype], Array]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"\n",
        "            inputs.shape is (batch_size, timesteps, emb_dim).\n",
        "            Output tensor with shape `(batch_size, timesteps, in_dim)`.\n",
        "        \"\"\"\n",
        "        assert inputs.ndim == 3, ('Number of dimensions should be 3, but it is: %d' % inputs.ndim)\n",
        "\n",
        "        position_ids = jnp.arange(inputs.shape[1])[None] # (1, timesteps)\n",
        "        pos_embeddings = nn.Embed(\n",
        "            128, # Max Positional Embeddings\n",
        "            inputs.shape[2],\n",
        "            embedding_init=self.posemb_init,\n",
        "            dtype=inputs.dtype,\n",
        "        )(position_ids)\n",
        "        print(\"For Input Shape {}, Pos Embes Shape is {}\".format(inputs.shape, pos_embeddings.shape))\n",
        "        return inputs + pos_embeddings\n",
        "\n",
        "        # pos_emb_shape = (1, inputs.shape[1], inputs.shape[2])\n",
        "        # pe = self.param('pos_embedding', self.posemb_init, pos_emb_shape)\n",
        "        # return inputs + pe\n",
        "\n",
        "\n",
        "class MlpBlock(nn.Module):\n",
        "    \"\"\"Transformer MLP / feed-forward block.\"\"\"\n",
        "\n",
        "    mlp_dim: int\n",
        "    dtype: Dtype = jnp.float32\n",
        "    out_dim: Optional[int] = None\n",
        "    dropout_rate: float = None\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.xavier_uniform()\n",
        "    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.normal(stddev=1e-6)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs, *, deterministic):\n",
        "        \"\"\"It's just an MLP, so the input shape is (batch, len, emb).\"\"\"\n",
        "        actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim\n",
        "        x = nn.Dense(\n",
        "                features=self.mlp_dim,\n",
        "                dtype=self.dtype,\n",
        "                kernel_init=self.kernel_init,\n",
        "                bias_init=self.bias_init)(inputs)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "        output = nn.Dense(\n",
        "                features=actual_out_dim,\n",
        "                dtype=self.dtype,\n",
        "                kernel_init=self.kernel_init,\n",
        "                bias_init=self.bias_init)(x)\n",
        "        output = nn.Dropout(\n",
        "                rate=self.dropout_rate)(output, deterministic=deterministic)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder1DBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder layer.\n",
        "    Given a sequence, it passes it through an attention layer, then through a mlp layer.\n",
        "    In each case it is a residual block with a layer norm.\n",
        "    \"\"\"\n",
        "\n",
        "    mlp_dim: int\n",
        "    num_heads: int\n",
        "    causal: bool\n",
        "    dropout_rate: float\n",
        "    attention_dropout_rate: float\n",
        "    dtype: Dtype = jnp.float32\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs, *, deterministic, train=True):\n",
        "\n",
        "        if self.causal:\n",
        "            causal_mask = nn.make_causal_mask(jnp.ones((inputs.shape[0], inputs.shape[1]),\n",
        "                                                        dtype=\"bool\"), dtype=\"bool\")\n",
        "            print(\"Using Causal Mask with shape\", causal_mask.shape, \"and inputs shape\", inputs.shape, \".\")\n",
        "        else:\n",
        "            causal_mask = None\n",
        "\n",
        "        # Attention block.\n",
        "        assert inputs.ndim == 3, f'Expected (batch, seq, hidden) got {inputs.shape}'\n",
        "        x = nn.LayerNorm(dtype=self.dtype)(inputs)\n",
        "        x = nn.MultiHeadDotProductAttention(\n",
        "            dtype=self.dtype,\n",
        "            kernel_init=nn.initializers.xavier_uniform(),\n",
        "            broadcast_dropout=False,\n",
        "            deterministic=deterministic,\n",
        "            dropout_rate=self.attention_dropout_rate,\n",
        "            decode=False,\n",
        "            num_heads=self.num_heads)(x, x, causal_mask)\n",
        "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "        x = x + inputs\n",
        "\n",
        "        # MLP block. This does NOT change the embedding dimension!\n",
        "        y = nn.LayerNorm(dtype=self.dtype)(x)\n",
        "        y = MlpBlock(mlp_dim=self.mlp_dim, dtype=self.dtype, dropout_rate=self.dropout_rate)(y, deterministic=deterministic)\n",
        "\n",
        "        return x + y\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\n",
        "    \"\"\"\n",
        "\n",
        "    num_layers: int\n",
        "    emb_dim: int\n",
        "    mlp_dim: int\n",
        "    num_heads: int\n",
        "    dropout_rate: float\n",
        "    attention_dropout_rate: float\n",
        "    causal: bool = True\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, *, train):\n",
        "        assert x.ndim == 3  # (batch, len, emb)\n",
        "        assert x.shape[-1] == self.emb_dim\n",
        "\n",
        "        # Input Encoder. Each layer processes x, but the shape of x does not change.\n",
        "        for lyr in range(self.num_layers):\n",
        "            x = Encoder1DBlock(\n",
        "                    mlp_dim=self.mlp_dim,\n",
        "                    dropout_rate=self.dropout_rate,\n",
        "                    attention_dropout_rate=self.attention_dropout_rate,\n",
        "                    name=f'encoderblock_{lyr}',\n",
        "                    causal=self.causal,\n",
        "                    num_heads=self.num_heads)(\n",
        "                            x, deterministic=not train, train=train)\n",
        "        encoded = nn.LayerNorm(name='encoder_norm')(x)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "def get_default_config():\n",
        "    import ml_collections\n",
        "\n",
        "    config = ml_collections.ConfigDict({\n",
        "        'num_layers': 4,\n",
        "        'emb_dim': 256,\n",
        "        'mlp_dim': 256,\n",
        "        'num_heads': 4,\n",
        "        'dropout_rate': 0.0,\n",
        "        'attention_dropout_rate': 0.0,\n",
        "        'causal': True,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "#fre/experiment/ant_helper.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "def get_canvas_image(canvas):\n",
        "    canvas.draw()\n",
        "    out_image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
        "    out_image = out_image.reshape(canvas.get_width_height()[::-1] + (3,))\n",
        "    return out_image\n",
        "\n",
        "def value_image(env, dataset, value_fn, mask, clip=False):\n",
        "    \"\"\"\n",
        "    Visualize the value function.\n",
        "    Args:\n",
        "        env: The environment.\n",
        "        value_fn: a function with signature value_fn([# states, state_dim]) -> [#states, 1]\n",
        "    Returns:\n",
        "        A numpy array of the image.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(tight_layout=True)\n",
        "    canvas = FigureCanvas(fig)\n",
        "    plot_value(env, dataset, value_fn, mask, fig, plt.gca(), clip=clip)\n",
        "    image = get_canvas_image(canvas)\n",
        "    plt.close(fig)\n",
        "    return image\n",
        "\n",
        "def plot_value(env, dataset, value_fn, mask, fig, ax, title=None, clip=True):\n",
        "    N = 14\n",
        "    M = 20\n",
        "    ob_xy = env.XY(n=N, m=M)\n",
        "\n",
        "    base_observation = np.copy(dataset['observations'][0])\n",
        "    base_observations = np.tile(base_observation, (5, ob_xy.shape[0], 1))\n",
        "    base_observations[:, :, :2] = ob_xy\n",
        "    base_observations[:, :, 15:17] = 0.0\n",
        "    base_observations[0, :, 15] = 1.0\n",
        "    base_observations[1, :, 16] = 1.0\n",
        "    base_observations[2, :, 15] = -1.0\n",
        "    base_observations[3, :, 16] = -1.0\n",
        "    print(\"Base observations, \", base_observations.shape)\n",
        "\n",
        "\n",
        "    values = []\n",
        "    for i in range(5):\n",
        "        values.append(value_fn(base_observations[i]))\n",
        "    values = np.stack(values, axis=0)\n",
        "    print(\"Values\", values.shape)\n",
        "\n",
        "    x, y = ob_xy[:, 0], ob_xy[:, 1]\n",
        "    x = x.reshape(N, M)\n",
        "    y = y.reshape(N, M) * 0.975 + 0.7\n",
        "    values = values.reshape(5, N, M)\n",
        "    values[-1, 10, 0] = np.min(values[-1]) + 0.3 # Hack to make the scaling not show small errors.\n",
        "    print(\"Clip:\", clip)\n",
        "    if clip:\n",
        "        mesh = ax.pcolormesh(x, y, values[-1], cmap='viridis', vmin=-0.1, vmax=1.0)\n",
        "    else:\n",
        "        mesh = ax.pcolormesh(x, y, values[-1], cmap='viridis')\n",
        "\n",
        "    v = (values[1] - values[3]) / 2\n",
        "    u = (values[0] - values[2]) / 2\n",
        "    uv_dist = np.sqrt(u**2 + v**2) + 1e-6\n",
        "    # Normalize u,v\n",
        "    un = u / uv_dist\n",
        "    vn = v / uv_dist\n",
        "    un[uv_dist < 0.1] = 0\n",
        "    vn[uv_dist < 0.1] = 0\n",
        "\n",
        "    plt.quiver(x, y, un, vn, color='r', pivot='mid', scale=0.75, scale_units='xy')\n",
        "\n",
        "    if mask is not None and type(mask) == np.ndarray:\n",
        "        # mask = NxM array of things to unmask.\n",
        "        from matplotlib.colors import LinearSegmentedColormap\n",
        "        colors = [(0,0,0,c) for c in np.linspace(0,1,100)]\n",
        "        cmapred = LinearSegmentedColormap.from_list('mycmap', colors, N=5)\n",
        "        mask_mesh_ax = ax.pcolormesh(x, y, mask, cmap=cmapred)\n",
        "    elif mask is not None and type(mask) is list:\n",
        "        maskmesh = np.ones((N, M))\n",
        "        for xy in mask:\n",
        "            for xi in range(N):\n",
        "                for yi in range(M):\n",
        "                    if np.linalg.norm(np.array(xy) - np.array([x[xi, yi], y[xi, yi]])) < 1.4:\n",
        "                        # print(xy, x[xi, yi], y[xi, yi])\n",
        "                        maskmesh[xi,yi] = 0\n",
        "        from matplotlib.colors import LinearSegmentedColormap\n",
        "        colors = [(0,0,0,c) for c in np.linspace(0,1,100)]\n",
        "        cmapred = LinearSegmentedColormap.from_list('mycmap', colors, N=5)\n",
        "        mask_mesh_ax = ax.pcolormesh(x, y, maskmesh, cmap=cmapred)\n",
        "\n",
        "    env.draw(ax, scale=0.95)\n",
        "\n",
        "\n",
        "\n",
        "    # env.draw(ax, scale=1.0)\n",
        "\n",
        "    # divider = make_axes_locatable(ax)\n",
        "    # cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    # fig.colorbar(mesh, cax=cax, orientation='vertical')\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "\n"
      ],
      "metadata": {
        "id": "JWuquO5WsHrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0f0a4a-4217-4dd9-a493-bbaf8e81acab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opensimplex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNUNs9wclupv",
        "outputId": "d42df662-eedc-45ca-c383-2509893305df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opensimplex\n",
            "  Downloading opensimplex-0.4.5.1-py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from opensimplex) (1.25.2)\n",
            "Installing collected packages: opensimplex\n",
            "Successfully installed opensimplex-0.4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fre/experiment/rewards_unsupervised\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import opensimplex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from functools import partial\n",
        "\n",
        "class RewardFunction():\n",
        "    # Given a batch of trajectory states and random states, generate a reward function.\n",
        "    # Return the labelled state-reward pairs. (batch_size, num_pairs, obs_dim + 1)\n",
        "    def generate_params_and_pairs(self, traj_states, random_states):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Given a batch of states and a batch of parameters, compute the reward.\n",
        "    def compute_reward(self, states, params):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class GoalReachingRewardFunction(RewardFunction):\n",
        "    def __init__(self):\n",
        "        self.p_current = 0.2\n",
        "        self.p_trajectory = 0.5\n",
        "        self.p_random = 0.3\n",
        "\n",
        "    # TODO: If this is slow, we can try and JIT it.\n",
        "    # Select a random goal from the provided states.\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        all_states = np.concatenate([traj_states, random_states], axis=1)\n",
        "        batch_size = all_states.shape[0]\n",
        "        p_trajectory_normalized = self.p_trajectory / traj_states.shape[1]\n",
        "        p_random_normalized = self.p_random / random_states.shape[1]\n",
        "        probabilities = [self.p_current] + [p_trajectory_normalized] * (traj_states.shape[1]-1) \\\n",
        "            + [p_random_normalized] * random_states.shape[1]\n",
        "        probabilities = np.array(probabilities) / np.sum(probabilities)\n",
        "        selected_goal_idx = np.random.choice(len(probabilities), size=(batch_size,), p=probabilities)\n",
        "        selected_goal = all_states[np.arange(batch_size), selected_goal_idx]\n",
        "\n",
        "        params = selected_goal # (batch_size, obs_dim)\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_pairs[:, 0] = params # Decode the goal state too.\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1] # (batch_size,)\n",
        "        masks = -rewards # If (rew=-1, mask=1), else (rew=0, mask=0)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def compute_reward(self, states, params, delta=False):\n",
        "        assert len(states.shape) == len(params.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "        if states.shape[-1] == 29: # AntMaze\n",
        "            if delta:\n",
        "                dists = np.linalg.norm(states - params, axis=-1)\n",
        "                is_goal = (dists < 0.1)\n",
        "            else:\n",
        "                dists = np.linalg.norm(states[..., :2] - params[..., :2], axis=-1)\n",
        "                is_goal = (dists < 2)\n",
        "            return -1 + is_goal.astype(float) # (batch_size,)\n",
        "        elif states.shape[-1] == 18: # Cheetah\n",
        "            std = np.array([[0.4407440506721877, 10.070289916801876, 0.5172332956856273, 0.5601041145815341, 0.518947027289748, 0.3204431592542281, 0.5501848643154092, 0.3856393812067661, 1.9882502334402663, 1.6377168569884073, 4.308505013609855, 12.144181770553105, 13.537567521831702, 16.88983033626308, 7.715009572436841, 14.345667964212357, 10.6904255152284, 100]])\n",
        "            assert len(states.shape) == len(params.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "            # if len(states.shape) == 3:\n",
        "            #     breakpoint()\n",
        "            dists_per_dim = states - params\n",
        "            dists_per_dim = dists_per_dim / std\n",
        "            dists = np.linalg.norm(dists_per_dim, axis=-1) / states.shape[-1]\n",
        "            is_goal = (dists < 0.08)\n",
        "            # print(dists_per_dim)\n",
        "            # print(dists, is_goal)\n",
        "            return -1 + is_goal.astype(float) # (batch_size,)\n",
        "        elif states.shape[-1] == 27: # Walker\n",
        "            std = np.array([[0.7212967364054736, 0.6775020895964047, 0.7638155887842976, 0.6395721376821286, 0.6849394775886244, 0.7078581708129903, 0.7113168519036742, 0.6753408522523937, 0.6818095329625652, 0.7133958718133511, 0.65227578338642, 0.757622576816855, 0.7311826446274479, 0.6745824928740024, 0.36822491550384456, 2.1134839667805805, 1.813353841099317, 10.594648894374815, 17.41041469033713, 17.836743227082106, 22.399097178637533, 16.1492222730888, 15.693574546557201, 18.539929326905067, 100, 100, 100]])\n",
        "            assert len(states.shape) == len(params.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "            dists_per_dim = states - params\n",
        "            dists_per_dim = dists_per_dim / std\n",
        "            dists = np.linalg.norm(dists_per_dim, axis=-1) / states.shape[-1]\n",
        "            is_goal = (dists < 0.2)\n",
        "            return -1 + is_goal.astype(float) # e6yfwsc ebnev (batch_size,)\n",
        "        elif states.shape[-1] == 30: # Kitchen\n",
        "            dists_per_dim = states - params\n",
        "            dists_per_dim = dists_per_dim\n",
        "            dists = np.linalg.norm(dists_per_dim, axis=-1) / states.shape[-1]\n",
        "            is_goal = (dists < 1e-6)\n",
        "            return -1 + is_goal.astype(float)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape # (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        if random_states.shape[-1] == 29: # AntMaze\n",
        "            random_states[:, 0, :2] = params[:, :2] # Make sure to include the goal.\n",
        "        else:\n",
        "            random_states[:, 0] = params\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs # (batch_size, reward_pairs, obs_dim + 1)\n",
        "\n",
        "class LinearRewardFunction(RewardFunction):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # Randomly generate a linear weighting over state features.\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        assert len(traj_states.shape) == 3, traj_states.shape # (batch_size, traj_len, obs_dim)\n",
        "        batch_size = traj_states.shape[0]\n",
        "        state_len = traj_states.shape[-1]\n",
        "\n",
        "        params = np.random.uniform(-1, 1, size=(batch_size, state_len)) # Uniform weighting.\n",
        "        random_mask = np.random.uniform(size=(batch_size,state_len)) < 0.9\n",
        "        if state_len == 29:\n",
        "            random_mask[:, :2] = True # Zero out the XY position for antmaze.\n",
        "        random_mask_positive = np.random.randint(2, state_len, size=(batch_size))\n",
        "        random_mask[np.arange(batch_size), random_mask_positive] = False # Force at least one positive weight.\n",
        "        params[random_mask] = 0 # Zero out some of the weights.\n",
        "        # if state_len == 29:\n",
        "        #     params = params / np.linalg.norm(params, axis=-1, keepdims=True) # Normalize XY\n",
        "\n",
        "        # Remove auxilliary features during training.\n",
        "        if state_len == 27:\n",
        "            params[:, -3:] = 0\n",
        "        if state_len == 18:\n",
        "            params[:, -1:] = 0\n",
        "\n",
        "        clip_bit = np.random.uniform(size=(batch_size,)) < 0.5\n",
        "        params = np.concatenate([params, clip_bit[:, None]], axis=-1) # (batch_size, obs_dim + 1)\n",
        "\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1] # (batch_size,)\n",
        "        masks = np.ones_like(rewards) # (batch_size,)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def compute_reward(self, states, params):\n",
        "        params_raw = params[..., :-1]\n",
        "        assert len(states.shape) == len(params_raw.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "        r = np.sum(states * params_raw, axis=-1) # (batch_size,)\n",
        "        r = np.where(params[..., -1] > 0, np.clip(r, 0, 1), np.clip(r, -1, 1))\n",
        "        return r\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape # (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs # (batch_size, reward_pairs, obs_dim + 1)\n",
        "\n",
        "class RandomRewardFunction(RewardFunction):\n",
        "    def __init__(self, num_simplex, obs_len=29):\n",
        "        # Pre-compute parameter matrices.\n",
        "        print(\"Generating parameter matrices...\")\n",
        "        self.simplex_size = num_simplex\n",
        "        np_random = np.random.RandomState(0)\n",
        "        self.param_w1 = np_random.normal(size=(self.simplex_size, obs_len, 32)) * np.sqrt(1/32)\n",
        "        self.param_b1 = np_random.normal(size=(self.simplex_size, 1, 32)) * np.sqrt(16)\n",
        "        self.param_w2 = np_random.normal(size=(self.simplex_size, 32, 1)) * np.sqrt(1/16)\n",
        "\n",
        "        # Remove auxilliary features during training.\n",
        "        if obs_len == 27:\n",
        "            self.param_w1[:, -3:] = 0\n",
        "        if obs_len == 18:\n",
        "            self.param_w1[:, -1:] = 0\n",
        "\n",
        "    # Random neural network.\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        batch_size = traj_states.shape[0]\n",
        "        params = np.random.randint(self.simplex_size, size=(batch_size, 1)) # (batch_size, 1)\n",
        "\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1] # (batch_size,)\n",
        "        masks = np.ones_like(rewards) # (batch_size,)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def compute_reward(self, states, params):\n",
        "        assert len(states.shape) == len(params.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        param_id = params[..., 0].astype(int)\n",
        "        param1_w = self.param_w1[param_id]\n",
        "        param1_b = self.param_b1[param_id]\n",
        "        param2_w = self.param_w2[param_id]\n",
        "\n",
        "        obs = states\n",
        "        x = np.expand_dims(obs, -2) # [batch, (pairs), 1, features_in]\n",
        "        x = np.matmul(x, param1_w) # [batch, (pairs), 1, features_out]\n",
        "        x = x + param1_b\n",
        "        x = np.tanh(x)\n",
        "        x = np.matmul(x, param2_w) # [batch, (pairs), 1, 1]\n",
        "        x = x.squeeze(-1).squeeze(-1) # [batch, (pairs)]\n",
        "        x = np.clip(x, -1, 1)\n",
        "        return x\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape # (batch_size, num_pairs, obs_dim)\n",
        "        batch_size = random_states.shape[0]\n",
        "\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs # (batch_size, reward_pairs, obs_dim + 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGQN8uMnmx9V",
        "outputId": "13b913c1-2e51-48ab-8614-b1561a94432f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fre/experiment/rewards_eval.py\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import opensimplex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from functools import partial\n",
        "\n",
        "#from fre.experiment.rewards_unsupervised import RewardFunction\n",
        "\n",
        "\n",
        "class VelocityRewardFunction(RewardFunction):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # Select an XY velocity from a future state in the trajectory.\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        batch_size = traj_states.shape[0]\n",
        "        selected_traj_state_idx = np.random.randint(traj_states.shape[1], size=(batch_size,))\n",
        "        selected_traj_state = traj_states[np.arange(batch_size), selected_traj_state_idx] # (batch_size, obs_dim)\n",
        "        params = selected_traj_state[:, 15:17] # (batch_size, 2)\n",
        "        params[:batch_size//4] = np.random.uniform(-1, 1, size=(batch_size//4, 2)) # Randomize 25% of the time.\n",
        "        params = params / np.linalg.norm(params, axis=-1, keepdims=True) # Normalize XY\n",
        "\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1] # (batch_size,)\n",
        "        masks = np.ones_like(rewards) # (batch_size,)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def compute_reward(self, states, params):\n",
        "        assert len(states.shape) == len(params.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "        xy_vels = states[..., 15:17] * 0.33820298\n",
        "        return np.sum(xy_vels * params, axis=-1) # (batch_size,)\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape # (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs # (batch_size, reward_pairs, obs_dim + 1)\n",
        "\n",
        "class TestRewMatrix(RewardFunction):\n",
        "    def __init__(self):\n",
        "        self.pos = np.zeros((36, 24))\n",
        "        self.xvel = np.zeros((36, 24))\n",
        "        self.yvel = np.zeros((36, 24))\n",
        "\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        batch_size = traj_states.shape[0]\n",
        "        params = np.zeros((batch_size, 1)) # (batch_size, 1)\n",
        "\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1] # (batch_size,)\n",
        "        masks = np.ones_like(rewards) # (batch_size,)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def compute_reward(self, s, params):\n",
        "        rews = np.zeros_like(s[..., 0]) # (batch, examples)\n",
        "        # XY Vel Reward\n",
        "        xy_vels = s[..., 15:17] * 0.33820298\n",
        "\n",
        "        x = s[..., 0].astype(int).clip(0, 35)\n",
        "        y = s[..., 1].astype(int).clip(0, 23)\n",
        "        simplex = self.pos[x, y]\n",
        "        simplex_xvel = self.xvel[x, y]\n",
        "        simplex_yvel = self.yvel[x, y]\n",
        "        rews = (simplex > 0.3).astype(float) * 0.5\n",
        "        rews += xy_vels[...,0] * simplex_xvel + xy_vels[...,1] * simplex_yvel\n",
        "\n",
        "        return rews\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape # (batch_size, num_pairs, obs_dim)\n",
        "        batch_size = random_states.shape[0]\n",
        "\n",
        "        # TODO: Be smarter about the states to use here.\n",
        "\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs # (batch_size, reward_pairs, obs_dim + 1)\n",
        "\n",
        "class SimplexRewardFunction(RewardFunction):\n",
        "    def __init__(self, num_simplex):\n",
        "        self.simplex_size = num_simplex\n",
        "        self.simplex_seeds_pos = np.zeros((self.simplex_size, 36, 24))\n",
        "        self.simplex_seeds_xvel = np.zeros((self.simplex_size, 36, 24))\n",
        "        self.simplex_seeds_yvel = np.zeros((self.simplex_size, 36, 24))\n",
        "        self.simplex_best_xy = np.zeros((self.simplex_size, 10, 2))\n",
        "        print(\"Generating simplex seeds\")\n",
        "        xi = np.arange(36)\n",
        "        yi = np.arange(24)\n",
        "        for r in tqdm.tqdm(range(self.simplex_size)):\n",
        "            opensimplex.seed(r)\n",
        "            self.simplex_seeds_pos[r] = opensimplex.noise2array(x=xi/20.0, y=yi/20.0).T\n",
        "            opensimplex.seed(r + self.simplex_size)\n",
        "            self.simplex_seeds_xvel[r] = opensimplex.noise2array(x=xi/20.0, y=yi/20.0).T\n",
        "            opensimplex.seed(r + self.simplex_size * 2)\n",
        "            self.simplex_seeds_yvel[r] = opensimplex.noise2array(x=xi/20.0, y=yi/20.0).T\n",
        "\n",
        "            best_topn = np.argpartition(self.simplex_seeds_pos[r].flatten(), -10)[-10:] # (10,)\n",
        "            best_xy = np.array(np.unravel_index(best_topn, self.simplex_seeds_pos[r].shape)).T # (10, 2)\n",
        "            self.simplex_best_xy[r] = best_xy\n",
        "        self.simplex_seeds_xvel[np.abs(self.simplex_seeds_xvel) < 0.5] = 0\n",
        "        self.simplex_seeds_yvel[np.abs(self.simplex_seeds_yvel) < 0.5] = 0\n",
        "\n",
        "    # Select an XY velocity from a future state in the trajectory.\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        batch_size = traj_states.shape[0]\n",
        "        params = np.random.randint(self.simplex_size, size=(batch_size, 1)) # (batch_size, 1)\n",
        "\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1] # (batch_size,)\n",
        "        masks = np.ones_like(rewards) # (batch_size,)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def compute_reward(self, states, params):\n",
        "        assert len(states.shape) == len(params.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        simplex_id = params[..., 0].astype(int)\n",
        "        x = states[..., 0].astype(int).clip(0, 35)\n",
        "        y = states[..., 1].astype(int).clip(0, 23)\n",
        "        simplex = self.simplex_seeds_pos[simplex_id, x, y]\n",
        "        simplex_xvel = self.simplex_seeds_xvel[simplex_id, x, y]\n",
        "        simplex_yvel = self.simplex_seeds_yvel[simplex_id, x, y]\n",
        "        rews = -1 + (simplex > 0.3).astype(float) * 0.5\n",
        "        xy_vels = states[..., 15:17] * 0.33820298\n",
        "        rews += xy_vels[...,0] * simplex_xvel + xy_vels[...,1] * simplex_yvel\n",
        "        return rews # (batch_size,)\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape # (batch_size, num_pairs, obs_dim)\n",
        "        batch_size = random_states.shape[0]\n",
        "\n",
        "        # For simplex rewards, make sure to include the top 4 best points.\n",
        "        simplex_id = params[..., 0].astype(int)\n",
        "        random_best_4 = np.random.randint(0, 10, size=(batch_size, 4))\n",
        "        random_states[:, :4, :2] = self.simplex_best_xy[simplex_id[:, None], random_best_4, :]\n",
        "\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs # (batch_size, reward_pairs, obs_dim + 1)\n",
        "\n",
        "class TestRewMatrixEdges(TestRewMatrix):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pos[:3, :] = 1\n",
        "        self.pos[-3:, :] = 1\n",
        "        self.pos[:, :3] = 1\n",
        "        self.pos[:, -3:] = 1\n",
        "\n",
        "class TestRewLoop(TestRewMatrix):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pos[22:33, 14:18] = 1\n",
        "        self.xvel[22:33, 14:18] = -1\n",
        "\n",
        "        self.pos[21:, 0:3] = 1\n",
        "        self.xvel[21:, 0:3] = 1\n",
        "\n",
        "        self.pos[33:, 3:18] = 1\n",
        "        self.yvel[33:, 3:18] = 1\n",
        "\n",
        "        self.pos[18:21, 0:7] = 1\n",
        "        self.yvel[18:21, 0:7] = -1\n",
        "\n",
        "class TestRewPath(TestRewMatrix):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pos[3:21, 7:10] = 1\n",
        "        self.xvel[3:21, 7:10] = -1\n",
        "\n",
        "        self.pos[0:3, 3:10] = 1\n",
        "        self.yvel[0:3, 3:10] = -1\n",
        "\n",
        "        self.pos[0:18, 0:3] = 1\n",
        "        self.xvel[0:18, 0:3] = 1\n",
        "\n",
        "class TestRewLoop2(TestRewMatrix):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pos[22:33, 14:18] = 1\n",
        "        self.pos[21:, 0:3] = 1\n",
        "        self.pos[33:, 3:18] = 1\n",
        "        self.pos[18:21, 0:7] = 1\n",
        "\n",
        "class TestRewPath2(TestRewMatrix):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pos[3:21, 7:10] = 1\n",
        "        self.pos[0:3, 3:10] = 1\n",
        "        self.pos[0:18, 0:3] = 1\n",
        "\n",
        "\n",
        "# =================== For DMC\n",
        "\n",
        "class VelocityRewardFunctionWalker(RewardFunction):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        batch_size = traj_states.shape[0]\n",
        "        params = np.random.uniform(low=0, high=8, size=(batch_size, 1)) # (batch_size, 1)\n",
        "\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1] # (batch_size,)\n",
        "        masks = np.ones_like(rewards) # (batch_size,)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def _sigmoids(self, x, value_at_1, sigmoid):\n",
        "        if sigmoid == 'gaussian':\n",
        "            scale = np.sqrt(-2 * np.log(value_at_1))\n",
        "            return np.exp(-0.5 * (x*scale)**2)\n",
        "\n",
        "        elif sigmoid == 'linear':\n",
        "            scale = 1-value_at_1\n",
        "            scaled_x = x*scale\n",
        "            return np.where(abs(scaled_x) < 1, 1 - scaled_x, 0.0)\n",
        "\n",
        "    def tolerance(self, x, lower, upper, margin=0.0, sigmoid='gaussian', value_at_margin=0.1):\n",
        "        in_bounds = np.logical_and(lower <= x, x <= upper)\n",
        "        d = np.where(x < lower, lower - x, x - upper) / margin\n",
        "        value = np.where(in_bounds, 1.0, self._sigmoids(d, value_at_margin, sigmoid))\n",
        "        return value\n",
        "\n",
        "    def compute_reward(self, states, params):\n",
        "        assert len(states.shape) == len(params.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        _STAND_HEIGHT = 1.2\n",
        "        horizontal_velocity = states[..., 24:25]\n",
        "        torso_upright = states[..., 25:26]\n",
        "        torso_height = states[..., 26:27]\n",
        "        standing = self.tolerance(torso_height, lower=_STAND_HEIGHT, upper=float('inf'), margin=_STAND_HEIGHT/2)\n",
        "        upright = (1 + torso_upright) / 2\n",
        "        stand_reward = (3*standing + upright) / 4\n",
        "        move_reward = self.tolerance(horizontal_velocity,\n",
        "                                        lower=params,\n",
        "                                        upper=float('inf'),\n",
        "                                        margin=params/2,\n",
        "                                        value_at_margin=0.5,\n",
        "                                        sigmoid='linear')\n",
        "        # move_reward[params == 0] = stand_reward[params == 0]\n",
        "        rew = stand_reward * (5*move_reward + 1) / 6\n",
        "        return rew[..., 0]\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape # (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs # (batch_size, reward_pairs, obs_dim + 1)\n",
        "\n",
        "class VelocityRewardFunctionCheetah(RewardFunction):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        batch_size = traj_states.shape[0]\n",
        "        params = np.random.uniform(low=-10, high=10, size=(batch_size, 1)) # (batch_size, 1)\n",
        "\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1] # (batch_size,)\n",
        "        masks = np.ones_like(rewards) # (batch_size,)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def _sigmoids(self, x, value_at_1, sigmoid):\n",
        "        if sigmoid == 'linear':\n",
        "            scale = 1-value_at_1\n",
        "            scaled_x = x*scale\n",
        "            return np.where(abs(scaled_x) < 1, 1 - scaled_x, 0.0)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def tolerance(self, x, lower, upper, margin=0.0, sigmoid='linear', value_at_margin=0):\n",
        "        in_bounds = np.logical_and(lower <= x, x <= upper)\n",
        "        d = np.where(x < lower, lower - x, x - upper) / margin\n",
        "        value = np.where(in_bounds, 1.0, self._sigmoids(d, value_at_margin, sigmoid))\n",
        "        return value\n",
        "\n",
        "    def compute_reward(self, states, params):\n",
        "        assert len(states.shape) == len(params.shape), states.shape # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        horizontal_velocity = states[..., 17:18]\n",
        "        sign_of_param = np.sign(params)\n",
        "        horizontal_velocity = horizontal_velocity * sign_of_param\n",
        "        rew = self.tolerance(horizontal_velocity,\n",
        "                             lower=np.abs(params),\n",
        "                             upper=float('inf'),\n",
        "                             margin=np.abs(params),\n",
        "                             value_at_margin=0,\n",
        "                             sigmoid='linear')\n",
        "        return rew[..., 0]\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape # (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs # (batch_size, reward_pairs, obs_dim + 1)\n",
        "\n",
        "# =================== For Kitchen\n",
        "\n",
        "class SingleTaskRewardFunction(RewardFunction):\n",
        "    def __init__(self):\n",
        "        self.obs_element_indices = {\n",
        "            \"bottom left burner\": np.array([11, 12]),\n",
        "            \"top left burner\": np.array([15, 16]),\n",
        "            \"light switch\": np.array([17, 18]),\n",
        "            \"slide cabinet\": np.array([19]),\n",
        "            \"hinge cabinet\": np.array([20, 21]),\n",
        "            \"microwave\": np.array([22]),\n",
        "            \"kettle\": np.array([23, 24, 25, 26, 27, 28, 29]),\n",
        "        }\n",
        "        self.obs_element_goals = {\n",
        "            \"bottom left burner\": np.array([-0.88, -0.01]),\n",
        "            \"top left burner\": np.array([-0.92, -0.01]),\n",
        "            \"light switch\": np.array([-0.69, -0.05]),\n",
        "            \"slide cabinet\": np.array([0.37]),\n",
        "            \"hinge cabinet\": np.array([0.0, 1.45]),\n",
        "            \"microwave\": np.array([-0.75]),\n",
        "            \"kettle\": np.array([-0.23, 0.75, 1.62, 0.99, 0.0, 0.0, -0.06]),\n",
        "        }\n",
        "        self.dist_thresh = 0.3\n",
        "        self.num_tasks = len(self.obs_element_indices)\n",
        "\n",
        "    def generate_params_and_pairs(self, traj_states, random_states, random_states_decode):\n",
        "        batch_size = traj_states.shape[0]\n",
        "        params = np.random.randint(self.num_tasks, size=(batch_size, 1))  # (batch_size, 1)\n",
        "        params = np.eye(self.num_tasks)[params[:, 0]]  # (batch_size, num_tasks)\n",
        "\n",
        "        encode_pairs = np.concatenate([traj_states, random_states], axis=1)\n",
        "        encode_rewards = self.compute_reward(encode_pairs, params[:, None, :])[:, :, None]\n",
        "        encode_pairs = np.concatenate([encode_pairs, encode_rewards], axis=-1)\n",
        "\n",
        "        decode_pairs = random_states_decode\n",
        "        decode_rewards = self.compute_reward(decode_pairs, params[:, None, :])[:, :, None]\n",
        "        decode_pairs = np.concatenate([random_states_decode, decode_rewards], axis=-1)\n",
        "\n",
        "        rewards = encode_pairs[:, 0, -1]  # (batch_size,)\n",
        "        masks = np.ones_like(rewards)  # (batch_size,)\n",
        "\n",
        "        return params, encode_pairs, decode_pairs, rewards, masks\n",
        "\n",
        "    def compute_reward(self, states, params):\n",
        "        assert len(states.shape) == len(params.shape), states.shape  # (batch_size, obs_dim) OR (batch_size, num_pairs, obs_dim)\n",
        "        task_rewards = []\n",
        "        for task, target_indices in self.obs_element_indices.items():\n",
        "            task_dists = np.linalg.norm(states[..., target_indices] - self.obs_element_goals[task], axis=-1)\n",
        "            task_completes = (task_dists < self.dist_thresh).astype(float)\n",
        "            task_rewards.append(task_completes)\n",
        "        task_rewards = np.stack(task_rewards, axis=-1)\n",
        "\n",
        "        return np.sum(task_rewards * params, axis=-1)\n",
        "\n",
        "    def make_encoder_pairs_testing(self, params, random_states):\n",
        "        assert len(params.shape) == 2, params.shape  # (batch_size, 2)\n",
        "        assert len(random_states.shape) == 3, random_states.shape  # (batch_size, num_pairs, obs_dim)\n",
        "\n",
        "        reward_pair_rews = self.compute_reward(random_states, params[:, None, :])[..., None]\n",
        "        reward_pairs = np.concatenate([random_states, reward_pair_rews], axis=-1)\n",
        "        return reward_pairs  # (batch_size, reward_pairs, obs_dim + 1)\n",
        "\n",
        "\n",
        "#fre\\eperiment\\run_free\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "import functools\n",
        "import ml_collections\n",
        "from ml_collections import config_flags\n",
        "from absl import app, flags\n",
        "import os\n",
        "import pickle\n",
        "import tqdm\n",
        "from flax.training import checkpoints\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "from fre.common.typing import *\n",
        "from fre.common.networks.transformer import Transformer\n",
        "import fre.common.networks.transformer as transformer\n",
        "from fre.common.dataset import Dataset\n",
        "from fre.common.typing import *\n",
        "from fre.common.train_state import TrainState, target_update\n",
        "from fre.common.networks.basic import Policy, ValueCritic, Critic, ensemblize\n",
        "from fre.common.wandb import setup_wandb, default_wandb_config, get_flag_dict\n",
        "from fre.common.envs.gc_utils import GCDataset\n",
        "from fre.common.envs.env_helper import make_env, get_dataset\n",
        "from fre.common.evaluation import evaluate\n",
        "from fre.common.envs.wrappers import EpisodeMonitor, RewardOverride, TruncateObservation\n",
        "from fre.common.utils import supply_rng\n",
        "from fre.experiment.rewards_unsupervised import *\n",
        "from fre.experiment.rewards_eval import *\n",
        "\n",
        "\"\"\"\n",
        "import flax\n",
        "\n",
        "###############################\n",
        "#  Configs\n",
        "###############################\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "      delattr(flags.FLAGS,name)\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_string('env_name', 'antmaze-large-diverse-v2', 'Environment name.')\n",
        "flags.DEFINE_integer('dmc_dataset_size', 5000000, 'ExORL dataset size.')\n",
        "flags.DEFINE_string('name', 'default', '')\n",
        "\n",
        "flags.DEFINE_string('save_dir', None, 'Logging dir (if not None, save params).')\n",
        "flags.DEFINE_string('load_dir', None, 'Logging dir (if not None, load params).')\n",
        "\n",
        "flags.DEFINE_integer('seed', np.random.choice(1000000), 'Random seed.')\n",
        "flags.DEFINE_integer('eval_episodes', 20,\n",
        "                     'Number of episodes used for evaluation.')\n",
        "flags.DEFINE_integer('log_interval', 1000, 'Logging interval.')\n",
        "flags.DEFINE_integer('eval_interval', 200000, 'Eval interval.')\n",
        "flags.DEFINE_integer('save_interval', 200000, 'Eval interval.')\n",
        "flags.DEFINE_integer('video_interval', 10050000, 'Eval interval.')\n",
        "flags.DEFINE_integer('batch_size', 512, 'Mini batch size.')\n",
        "flags.DEFINE_integer('max_steps', int(1e6), 'Number of training steps.')\n",
        "\n",
        "flags.DEFINE_integer('reward_pairs_encode', 32, 'Number of reward pairs to use for encoding.')\n",
        "flags.DEFINE_integer('reward_pairs_decode', 8, 'Number of reward pairs to use for decoding.')\n",
        "flags.DEFINE_integer('reward_pairs_encode_test', 32, 'Number of reward pairs to use for encoding (for testing).')\n",
        "\n",
        "flags.DEFINE_float('rew_ratio_goal', 0.3333, 'Ratio of reward functions that are goal.')\n",
        "flags.DEFINE_float('rew_ratio_linear', 0.3333, 'Ratio of reward functions that are linear.')\n",
        "flags.DEFINE_float('rew_ratio_mlp', 0.3333, 'Ratio of reward functions that are random mlp.')\n",
        "\n",
        "# Env-Specific Settings\n",
        "flags.DEFINE_string('start_loc', 'center2', 'Starting location of the ant')\n",
        "flags.DEFINE_integer('use_discrete_xy', 1, 'Use discrete XY encoding for antmaze?')\n",
        "flags.DEFINE_integer('dmc_use_oracle', 0, 'Use true rewards during training?')\n",
        "\n",
        "agent_config = ml_collections.ConfigDict({\n",
        "    'lr': 1e-4,\n",
        "    'reward_pairs_emb_dim': 128,\n",
        "    'hidden_dims': (512, 512, 512),\n",
        "    'discount': 0.99,\n",
        "    'expectile': 0.8,\n",
        "    'temperature': 3.0, # 0 for behavior cloning.\n",
        "    'tau': 0.001,\n",
        "    'opt_decay_schedule': 'none',\n",
        "    'warmup_steps': 150000,\n",
        "    \"num_discrete_embeddings\": 32,\n",
        "    'kl_weight': 0.01,\n",
        "    'actor_loss_type': 'awr', # awr or ddpg.\n",
        "    'bc_coefficient': 0.0,\n",
        "})\n",
        "\n",
        "wandb_config = default_wandb_config()\n",
        "wandb_config.update({\n",
        "    'project': 'fre_fre',\n",
        "    'name': 'fre_{env_name}',\n",
        "})\n",
        "\n",
        "def GCDataset_get_default_config():\n",
        "        return ml_collections.ConfigDict({\n",
        "            'p_randomgoal': 0.3,\n",
        "            'p_trajgoal': 0.5,\n",
        "            'p_currgoal': 0.2,\n",
        "            'geom_sample': 1,\n",
        "            'discount': 0.99,\n",
        "            'reward_scale': 1.0,\n",
        "            'reward_shift': -1.0,\n",
        "            'mask_terminal': 1,\n",
        "        })\n",
        "\n",
        "def transformer_get_default_config():\n",
        "    config = ml_collections.ConfigDict({\n",
        "        'num_layers': 4,\n",
        "        'emb_dim': 256,\n",
        "        'mlp_dim': 256,\n",
        "        'num_heads': 4,\n",
        "        'dropout_rate': 0.0,\n",
        "        'attention_dropout_rate': 0.0,\n",
        "        'causal': True,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "config_flags.DEFINE_config_dict('wandb', wandb_config, lock_config=False)\n",
        "config_flags.DEFINE_config_dict('agent', agent_config, lock_config=False)\n",
        "config_flags.DEFINE_config_dict('gcdataset', GCDataset_get_default_config(), lock_config=False)\n",
        "config_flags.DEFINE_config_dict('transformer', transformer_get_default_config(), lock_config=False)\n",
        "\n",
        "\n",
        "###############################\n",
        "#  Agent. Contains the neural networks, training logic, and sampling.\n",
        "###############################\n",
        "\n",
        "def expectile_loss(adv, diff, expectile=0.7):\n",
        "    weight = jnp.where(adv >= 0, expectile, (1 - expectile))\n",
        "    return weight * (diff**2)\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    \"\"\"Identity layer, convenient for giving a name to an array.\"\"\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        return jnp.expand_dims(x, axis=-1)\n",
        "\n",
        "\n",
        "class FRENetwork(nn.Module):\n",
        "    transformer_params: dict\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    reward_pairs_emb_dim : int\n",
        "    num_discrete_embeddings: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.encoder_transformer = Transformer(**self.transformer_params)\n",
        "        self.encoder_mean = nn.Dense(self.reward_pairs_emb_dim)\n",
        "        self.encoder_log_std = nn.Dense(self.reward_pairs_emb_dim)\n",
        "\n",
        "        self.reward_embed = nn.Embed(self.num_discrete_embeddings, self.reward_pairs_emb_dim // 2)\n",
        "        self.embed_reward_pairs = nn.Dense(self.reward_pairs_emb_dim // 2)\n",
        "\n",
        "        self.value = ValueCritic(self.hidden_dims)\n",
        "        self.critic = ensemblize(Critic, num_qs=2)(self.hidden_dims)\n",
        "        self.actor = Policy(self.hidden_dims, action_dim=self.action_dim,\n",
        "            log_std_min=-5.0, state_dependent_std=False, tanh_squash_distribution=False)\n",
        "\n",
        "        self.reward_predict = ValueCritic(self.hidden_dims)\n",
        "\n",
        "    def __call__(self, x): # (batch_size, timesteps, emb_dim)\n",
        "        raise None\n",
        "\n",
        "    def get_transformer_encoding(self, reward_state_pairs):\n",
        "        reward_states = reward_state_pairs[:, :, :-1]\n",
        "        reward_values = reward_state_pairs[:, :, -1]\n",
        "        reward_values_idx = jnp.floor((reward_values / 2.0 + 0.5) * self.num_discrete_embeddings).astype(jnp.int32)\n",
        "        reward_values_idx = jnp.clip(reward_values_idx, 0, self.num_discrete_embeddings - 1)\n",
        "\n",
        "        reward_state_emb = self.embed_reward_pairs(reward_states)\n",
        "        reward_state_val = self.reward_embed(reward_values_idx)\n",
        "        reward_state_pairs = jnp.concatenate([reward_state_emb, reward_state_val], axis=-1)\n",
        "\n",
        "        w_pre = self.encoder_transformer(reward_state_pairs, train=True) # [batch, reward_pairs, emb_dim]\n",
        "        w_pair_mean = w_pre.mean(axis=1)\n",
        "        w_mean = self.encoder_mean(w_pair_mean)\n",
        "        w_log_std = self.encoder_log_std(w_pair_mean)\n",
        "\n",
        "        return w_mean, w_log_std # (batch_size, emb_dim)\n",
        "\n",
        "    def get_value(self, w, obs):\n",
        "        w_and_obs = jnp.concatenate([w, obs], axis=-1)\n",
        "        return self.value(w_and_obs)\n",
        "\n",
        "    def get_critic(self, w, obs, actions):\n",
        "        w_and_obs = jnp.concatenate([w, obs], axis=-1)\n",
        "        return self.critic(w_and_obs, actions)\n",
        "\n",
        "    def get_actor(self, w, obs, temperature=1.0):\n",
        "        w_and_obs = jnp.concatenate([w, obs], axis=-1)\n",
        "        return self.actor(w_and_obs, temperature)\n",
        "\n",
        "    def get_reward_pred(self, w, reward_pairs): # Reward Pairs: [batch, reward_pairs, obs_dim + 1]\n",
        "        z_expand = jnp.expand_dims(w, axis=1) # [batch, 1, emb_dim]\n",
        "        z_expand = jnp.repeat(z_expand, repeats=reward_pairs.shape[1], axis=1)\n",
        "        reward_states = reward_pairs[:, :, :-1]\n",
        "        w_and_obs = jnp.concatenate([z_expand, reward_states], axis=-1)\n",
        "        reward_pred = self.reward_predict(w_and_obs)\n",
        "        return reward_pred # [batch, reward_pairs]\n",
        "\n",
        "    def get_all(self, reward_state_pairs, obs, actions):\n",
        "        w_mean, w_log_std = self.get_transformer_encoding(reward_state_pairs)\n",
        "        w = w_mean\n",
        "        w_and_obs = jnp.concatenate([w, obs], axis=-1)\n",
        "        ret = self.value(w_and_obs), self.get_actor(w, obs), self.get_reward_pred(w, reward_state_pairs), self.critic(w_and_obs, actions)\n",
        "        return ret\n",
        "\n",
        "\n",
        "class FREAgent(flax.struct.PyTreeNode):\n",
        "    rng: PRNGKey\n",
        "    fre: TrainState\n",
        "    target_fre: TrainState\n",
        "    config: dict = flax.struct.field(pytree_node=False)\n",
        "\n",
        "    @functools.partial(jax.jit, static_argnames=('train_encoder', 'train_actor', 'train_critic'))\n",
        "    def update(agent, batch: Batch, train_encoder=True, train_actor=True, train_critic=True, apply_updates=True) -> InfoDict:\n",
        "        new_rng, w_key = jax.random.split(agent.rng, 2)\n",
        "        reward_state_pairs = batch['reward_pairs_encode']\n",
        "        reward_pairs_decode = batch['reward_pairs_decode']\n",
        "\n",
        "        def full_loss_fn(params):\n",
        "            if train_encoder:\n",
        "                w_mean, w_log_std = agent.fre.do('get_transformer_encoding')(reward_state_pairs, params=params)\n",
        "            else:\n",
        "                w_mean, w_log_std = agent.fre.do('get_transformer_encoding')(reward_state_pairs)\n",
        "            w_no_grad = jax.lax.stop_gradient(w_mean)\n",
        "\n",
        "            if train_encoder:\n",
        "                # Reward Pred Loss\n",
        "                w = w_mean + jax.random.normal(agent.rng, w_mean.shape) * jnp.exp(w_log_std)\n",
        "                reward_pred = agent.fre.do('get_reward_pred')(w, reward_pairs_decode, params=params)\n",
        "                reward_truths = reward_pairs_decode[:, :, -1]\n",
        "                reward_pred_loss = ((reward_pred - reward_truths)**2).mean()\n",
        "                kl_loss = -0.5 * (1 + w_log_std - w_mean**2 - jnp.exp(w_log_std)).mean()\n",
        "                reward_kl_loss = reward_pred_loss + kl_loss * agent.config['kl_weight']\n",
        "                reward_pred_info = {\n",
        "                    'reward_pred_loss': reward_pred_loss,\n",
        "                    'reward_pred': reward_pred.mean(),\n",
        "                    'kl_loss': kl_loss,\n",
        "                }\n",
        "            else:\n",
        "                reward_kl_loss = 0.0\n",
        "                reward_pred_info = {}\n",
        "\n",
        "            if train_critic:\n",
        "                # Implicit Q-Learning\n",
        "                # Value Loss: Update V towards expectile of min(q1, q2).\n",
        "                w_target_mean = w_no_grad\n",
        "                w_mean = w_no_grad\n",
        "                q1, q2 = agent.target_fre.do(\"get_critic\")(w_target_mean, batch['observations'], batch['actions'])\n",
        "                q = jnp.minimum(q1, q2)\n",
        "                v = agent.fre.do(\"get_value\")(w_mean, batch['observations'], params=params)\n",
        "                adv = q - v\n",
        "                v_loss = expectile_loss(adv, q - v, agent.config['expectile'])\n",
        "                v_loss = (v_loss).mean()\n",
        "\n",
        "                # Critic Loss. Update Q = r\n",
        "                next_v = jax.lax.stop_gradient(agent.fre.do(\"get_value\")(w_mean, batch['next_observations']))\n",
        "                q = batch['rewards'] + agent.config['discount'] * batch['masks'] * next_v\n",
        "\n",
        "                q1, q2 = agent.fre.do(\"get_critic\")(w_mean, batch['observations'], batch['actions'], params=params)\n",
        "                q_loss = (q1 - q) ** 2 + (q2 - q) ** 2\n",
        "                q_loss = (q_loss).mean()\n",
        "\n",
        "                value_loss = v_loss + q_loss\n",
        "                value_info = {\n",
        "                    # 'value_loss': value_loss,\n",
        "                    'v_loss': v_loss,\n",
        "                    'q_loss': q_loss,\n",
        "                    'v': v.mean(),\n",
        "                    'q': q.mean(),\n",
        "                }\n",
        "            else:\n",
        "                value_loss = 0.0\n",
        "                value_info = {}\n",
        "\n",
        "            if train_actor:\n",
        "                # Actor Loss\n",
        "                actor_w = w_mean\n",
        "                if agent.config['actor_loss_type'] == 'awr':\n",
        "                    v = agent.fre.do(\"get_value\")(w_no_grad, batch['observations'])\n",
        "                    q1, q2 = agent.fre.do(\"get_critic\")(w_no_grad, batch['observations'], batch['actions'])\n",
        "                    q = jnp.minimum(q1, q2)\n",
        "                    adv = q - v\n",
        "\n",
        "                    actions = batch['actions']\n",
        "                    exp_a = jnp.exp(adv * agent.config['temperature'])\n",
        "                    exp_a = jnp.minimum(exp_a, 100.0)\n",
        "                    dist = agent.fre.do('get_actor')(actor_w, batch['observations'], params=params)\n",
        "                    log_probs = dist.log_prob(actions)\n",
        "                    assert exp_a.shape == log_probs.shape\n",
        "                    print(\"Log probs shape\", log_probs.shape)\n",
        "                    actor_loss = -(exp_a * log_probs).mean()\n",
        "                elif agent.config['actor_loss_type'] == 'ddpg':\n",
        "                    dist = agent.fre.do(\"get_actor\")(actor_w, batch['observations'], params=params)\n",
        "                    normalized_actions = jnp.tanh(dist.loc)\n",
        "                    q1, q2 = agent.fre.do(\"get_critic\")(w_no_grad, batch['observations'], normalized_actions)\n",
        "                    q = (q1 + q2) / 2\n",
        "\n",
        "                    q_loss = -q.mean()\n",
        "\n",
        "                    log_probs = dist.log_prob(batch['actions'])\n",
        "                    bc_loss = -((agent.config['bc_coefficient'] * log_probs)).mean()\n",
        "\n",
        "                    actor_loss = ((q_loss + bc_loss)).mean()\n",
        "\n",
        "                std = dist.stddev().mean()\n",
        "                mse_error = jnp.square(dist.loc - batch['actions']).mean()\n",
        "                actor_info = {\n",
        "                    'actor_loss': actor_loss,\n",
        "                    'std': std,\n",
        "                    'adv': adv.mean(),\n",
        "                    'mse_error': mse_error,\n",
        "                }\n",
        "            else:\n",
        "                actor_loss = 0.0\n",
        "                actor_info = {}\n",
        "\n",
        "            return value_loss + actor_loss + reward_kl_loss, {**value_info, **actor_info, **reward_pred_info}\n",
        "\n",
        "        new_fre, info = agent.fre.apply_loss_fn(loss_fn=full_loss_fn, has_aux=True)\n",
        "        new_target_fre = target_update(agent.fre, agent.target_fre, agent.config['target_update_rate'])\n",
        "\n",
        "        return agent.replace(fre=new_fre, target_fre=new_target_fre, rng=new_rng), {\n",
        "            **info\n",
        "        }\n",
        "\n",
        "    @jax.jit\n",
        "    def sample_actions(agent,\n",
        "                       observations: np.ndarray, # [obs_dim]\n",
        "                       reward_pairs: np.ndarray, # [1, reward_pairs, obs_dim + 1]\n",
        "                       *,\n",
        "                       seed: PRNGKey,\n",
        "                       temperature: float = 1.0) -> jnp.ndarray:\n",
        "        if type(observations) is dict:\n",
        "            observations = jnp.concatenate([observations['observation'], observations['goal']], axis=-1)\n",
        "        observations = jnp.expand_dims(observations, axis=0)\n",
        "        print(\"Reward pairs shape\", reward_pairs.shape)\n",
        "        w_mean, w_log_std = agent.fre.do('get_transformer_encoding')(reward_pairs)\n",
        "        print(\"W shape\", w_mean.shape)\n",
        "        actions = agent.fre.do('get_actor')(w_mean, observations, temperature=temperature).sample(seed=seed)\n",
        "        actions = jnp.clip(actions, -1, 1)\n",
        "        return actions[0]\n",
        "\n",
        "    def get_reward_pred(agent, observations: np.ndarray, reward_pairs: np.ndarray):\n",
        "        # append a dummy reward to the observations.\n",
        "        decode_pairs = jnp.concatenate([observations, np.ones((observations.shape[0], 1))], axis=-1)[None]\n",
        "        w_mean, w_log_std = agent.fre.do('get_transformer_encoding')(reward_pairs)\n",
        "        return agent.fre.do('get_reward_pred')(w_mean, decode_pairs)\n",
        "\n",
        "    def get_value_pred(agent, observations: np.ndarray, reward_pairs: np.ndarray):\n",
        "        w_mean, w_log_std = agent.fre.do('get_transformer_encoding')(reward_pairs) # [batch, emb_dim]\n",
        "        w_expand = jnp.repeat(w_mean, repeats=observations.shape[0], axis=0)\n",
        "        v = agent.fre.do('get_value')(w_expand, observations)\n",
        "        return v\n",
        "\n",
        "def create_learner(\n",
        "                seed: int,\n",
        "                batch: Batch,\n",
        "                transformer_params: dict,\n",
        "                lr: float,\n",
        "                reward_pairs_emb_dim: int,\n",
        "                num_discrete_embeddings: int,\n",
        "                kl_weight: float,\n",
        "                hidden_dims: Sequence[int],\n",
        "                discount: float,\n",
        "                tau: float,\n",
        "                expectile: float,\n",
        "                temperature: float,\n",
        "                max_steps: Optional[int],\n",
        "                opt_decay_schedule: str,\n",
        "                actor_loss_type: str,\n",
        "                bc_coefficient: float,\n",
        "            **kwargs):\n",
        "\n",
        "        print('Extra kwargs:', kwargs)\n",
        "\n",
        "        rng = jax.random.PRNGKey(seed)\n",
        "        rng, actor_key, critic_key, value_key = jax.random.split(rng, 4)\n",
        "\n",
        "        action_dim = batch['actions'].shape[-1]\n",
        "        transformer_params['causal'] = False\n",
        "        transformer_params['emb_dim'] = reward_pairs_emb_dim\n",
        "        transformer_params['num_heads'] = 2\n",
        "        transformer_params['num_layers'] = 2\n",
        "        fre_def = FRENetwork(transformer_params, hidden_dims, action_dim, reward_pairs_emb_dim=reward_pairs_emb_dim, num_discrete_embeddings=num_discrete_embeddings)\n",
        "\n",
        "        if opt_decay_schedule == \"cosine\":\n",
        "            schedule_fn = optax.cosine_decay_schedule(-lr, max_steps)\n",
        "            tx = optax.chain(optax.scale_by_adam(),\n",
        "                                    optax.scale_by_schedule(schedule_fn))\n",
        "        else:\n",
        "            tx = optax.adam(learning_rate=lr)\n",
        "\n",
        "        params = fre_def.init(actor_key, batch['reward_pairs_encode'], batch['observations'], batch['actions'], method='get_all')['params']\n",
        "        fre = TrainState.create(fre_def, params, tx=tx)\n",
        "        target_fre = TrainState.create(fre_def, params)\n",
        "\n",
        "        config = flax.core.FrozenDict(dict(\n",
        "            discount=discount, temperature=temperature, expectile=expectile, target_update_rate=tau, reward_pairs_emb_dim=reward_pairs_emb_dim, kl_weight=kl_weight, actor_loss_type=actor_loss_type, bc_coefficient=bc_coefficient, num_discrete_embeddings=num_discrete_embeddings\n",
        "        ))\n",
        "\n",
        "        return FREAgent(rng, fre=fre, target_fre=target_fre, config=config)\n",
        "\n",
        "###############################\n",
        "#  Run Script. Loads data, logs to wandb, and runs the training loop.\n",
        "###############################\n",
        "\n",
        "def main(_):\n",
        "    # Create wandb logger\n",
        "    setup_wandb(FLAGS.agent.to_dict(), **FLAGS.wandb)\n",
        "    assert 'ant' in FLAGS.env_name or 'dmc' in FLAGS.env_name or 'kitchen' in FLAGS.env_name\n",
        "\n",
        "    agent = None\n",
        "\n",
        "    if FLAGS.save_dir is not None:\n",
        "        os.makedirs(FLAGS.save_dir, exist_ok=True)\n",
        "        print(f'Saving config to {FLAGS.save_dir}/config.pkl')\n",
        "        with open(os.path.join(FLAGS.save_dir, 'config.pkl'), 'wb') as f:\n",
        "            pickle.dump(get_flag_dict(), f)\n",
        "\n",
        "    if 'ant' in FLAGS.env_name:\n",
        "        #import fre.common.envs.d4rl.d4rl_ant as d4rl_ant\n",
        "        env = d4rl_ant.CenteredMaze(FLAGS.env_name)\n",
        "        dataset = get_dataset(env, FLAGS.env_name)\n",
        "        dataset = dataset.copy({'masks': np.ones_like(dataset['masks'])})\n",
        "        dataset_gc = GCDataset(dataset, **FLAGS.gcdataset.to_dict())\n",
        "        example_batch = dataset.sample(1)\n",
        "        eval_env = EpisodeMonitor(RewardOverride(d4rl_ant.CenteredMaze(FLAGS.env_name)))\n",
        "        ## =============== Reward Functions for Testing =============== ##\n",
        "\n",
        "        base_ob = example_batch['observations'][0]\n",
        "        def goal_at(x,y):\n",
        "            goal = base_ob.copy()\n",
        "            goal[:2] = [x,y]\n",
        "            return goal\n",
        "        reward_fn_ratios = [FLAGS.rew_ratio_goal, FLAGS.rew_ratio_linear, FLAGS.rew_ratio_mlp]\n",
        "        GoalReachingRewards = GoalReachingRewardFunction()\n",
        "        VelocityRewards = VelocityRewardFunction()\n",
        "        LinearRewards = LinearRewardFunction()\n",
        "        SimplexRewards = SimplexRewardFunction(num_simplex=10)\n",
        "        RandomRewards = RandomRewardFunction(num_simplex=10000)\n",
        "        reward_fns = [GoalReachingRewards, LinearRewards, RandomRewards]\n",
        "\n",
        "        linear_states = dataset.sample(5)['observations'][:, None, :]\n",
        "        linear_params = LinearRewards.generate_params_and_pairs(linear_states, linear_states, linear_states)[0] # (5, params_dim)\n",
        "        print(\"Linear Params: \", linear_params)\n",
        "        test_rewards = [\n",
        "            (GoalReachingRewards, 'goal_bottom', goal_at(28, 0)),\n",
        "            (GoalReachingRewards, 'goal_left', goal_at(0, 15)),\n",
        "            (GoalReachingRewards, 'goal_top', goal_at(35, 24)),\n",
        "            (GoalReachingRewards, 'goal_center', goal_at(12, 24)),\n",
        "            (GoalReachingRewards, 'goal_right', goal_at(33, 16)),\n",
        "            (VelocityRewards, 'vel_left', np.array([-1, 0])),\n",
        "            (VelocityRewards, 'vel_up', np.array([0, 1])),\n",
        "            (VelocityRewards, 'vel_down', np.array([0, -1])),\n",
        "            (VelocityRewards, 'vel_right', np.array([1, 0])),\n",
        "            (SimplexRewards, 'simplex_1', np.array([1])),\n",
        "            (SimplexRewards, 'simplex_2', np.array([2])),\n",
        "            (SimplexRewards, 'simplex_3', np.array([3])),\n",
        "            (SimplexRewards, 'simplex_4', np.array([4])),\n",
        "            (SimplexRewards, 'simplex_5', np.array([5])),\n",
        "            (TestRewPath(), 'path_center', np.array([0])),\n",
        "            (TestRewLoop(), 'path_loop', np.array([0])),\n",
        "            (TestRewMatrixEdges(), 'path_edges', np.array([0])),\n",
        "        ]\n",
        "\n",
        "        slices = []\n",
        "        slices.append(0)\n",
        "        for j in range(len(reward_fns)):\n",
        "            slices.append(int(FLAGS.batch_size * reward_fn_ratios[j]))\n",
        "        slices[-1] = FLAGS.batch_size - sum(slices[:-1])\n",
        "        print(\"Number of samples for each reward func: \", slices)\n",
        "        slices = np.cumsum(slices)\n",
        "        print(\"Cumsum of samples for each reward func: \", slices)\n",
        "    elif 'dmc' in FLAGS.env_name:\n",
        "        _, env_name, task_name = FLAGS.env_name.split('_')\n",
        "        env = make_env(f'{env_name}_{task_name}')\n",
        "        env.reset()\n",
        "\n",
        "        # Load dataset.\n",
        "        import pathlib\n",
        "        file_path = str(pathlib.Path().resolve().parents[0])\n",
        "        path = file_path + f'/fre/data/exorl/{env_name}/rnd'\n",
        "        dataset_npy = os.path.join(path, task_name + '.npy')\n",
        "        dataset = np.load(dataset_npy, allow_pickle=True).item()\n",
        "        dataset['dones_float'] = np.zeros_like(dataset['rewards'])\n",
        "        dataset['dones_float'][::1000] = 1.0 # Each exorl trajectory is length 1000.\n",
        "        dataset['dones_float'][-1] = 1.0 # Last state is terminal.\n",
        "\n",
        "        # For evaluating the velocity rewareds, we need an augmented observation that uses the physics state.\n",
        "        if 'walker' in FLAGS.env_name:\n",
        "            aux = np.load(file_path+'/fre/data/aux_walker.npy', allow_pickle=True)\n",
        "        elif 'cheetah' in FLAGS.env_name:\n",
        "            aux = np.load(file_path+'/fre/data/aux_cheetah.npy', allow_pickle=True)\n",
        "        dataset['observations'] = np.concatenate([dataset['observations'], aux], axis=1)\n",
        "        aux_shifted = np.concatenate([aux[1::], aux[-1:]], axis=0)\n",
        "        dataset['next_observations'] = np.concatenate([dataset['next_observations'], aux_shifted], axis=1)\n",
        "\n",
        "        dataset = Dataset(dataset)\n",
        "        dataset_gc = GCDataset(dataset, **FLAGS.gcdataset.to_dict())\n",
        "        eval_env = EpisodeMonitor(RewardOverride(make_env(f'{env_name}_{task_name}')))\n",
        "        eval_env.reset()\n",
        "\n",
        "        def goal_at(seed):\n",
        "            return dataset.sample(1, indx=seed*777)['observations']\n",
        "\n",
        "        VelocityRewardsWalker = VelocityRewardFunctionWalker()\n",
        "        VelocityRewardsCheetah = VelocityRewardFunctionCheetah()\n",
        "        GoalReachingRewards = GoalReachingRewardFunction()\n",
        "        LinearRewards = LinearRewardFunction()\n",
        "        if 'walker' in FLAGS.env_name:\n",
        "            reward_fns = [VelocityRewardsWalker]\n",
        "            RandomRewards = RandomRewardFunction(num_simplex=10000, obs_len=27)\n",
        "            test_rewards = [\n",
        "                (VelocityRewardsWalker, 'vel0.1', np.array([0.1])),\n",
        "                (VelocityRewardsWalker, 'vel1', np.array([1])),\n",
        "                (VelocityRewardsWalker, 'vel4', np.array([4])),\n",
        "                (VelocityRewardsWalker, 'vel8', np.array([8])),\n",
        "                (GoalReachingRewards, 'goal_1', goal_at(1)),\n",
        "                (GoalReachingRewards, 'goal_2', goal_at(2)),\n",
        "                (GoalReachingRewards, 'goal_3', goal_at(3)),\n",
        "                (GoalReachingRewards, 'goal_4', goal_at(4)),\n",
        "                (GoalReachingRewards, 'goal_5', goal_at(5)),\n",
        "            ]\n",
        "        elif 'cheetah' in FLAGS.env_name:\n",
        "            reward_fns = [VelocityRewardsCheetah]\n",
        "            RandomRewards = RandomRewardFunction(num_simplex=10000, obs_len=18)\n",
        "            test_rewards = [\n",
        "                (VelocityRewardsCheetah, 'vel10Back', np.array([-10])),\n",
        "                (VelocityRewardsCheetah, 'vel2Back', np.array([-2])),\n",
        "                (VelocityRewardsCheetah, 'vel2', np.array([2])),\n",
        "                (VelocityRewardsCheetah, 'vel10', np.array([10])),\n",
        "                (GoalReachingRewards, 'goal_1', goal_at(1)),\n",
        "                (GoalReachingRewards, 'goal_2', goal_at(2)),\n",
        "                (GoalReachingRewards, 'goal_3', goal_at(3)),\n",
        "                (GoalReachingRewards, 'goal_4', goal_at(4)),\n",
        "                (GoalReachingRewards, 'goal_5', goal_at(5)),\n",
        "            ]\n",
        "        if FLAGS.dmc_use_oracle:\n",
        "            pass\n",
        "        else:\n",
        "            reward_fns = [GoalReachingRewards, LinearRewards, RandomRewards]\n",
        "\n",
        "        reward_fn_ratios = [FLAGS.rew_ratio_goal, FLAGS.rew_ratio_linear, FLAGS.rew_ratio_mlp]\n",
        "        slices = []\n",
        "        slices.append(0)\n",
        "        for j in range(len(reward_fns)):\n",
        "            slices.append(int(FLAGS.batch_size * reward_fn_ratios[j]))\n",
        "        slices[-1] = FLAGS.batch_size - sum(slices[:-1])\n",
        "        print(\"Number of samples for each reward func: \", slices)\n",
        "        slices = np.cumsum(slices)\n",
        "        print(\"Cumsum of samples for each reward func: \", slices)\n",
        "\n",
        "    elif 'kitchen' in FLAGS.env_name:\n",
        "        # HACK: Monkey patching to make it compatible with Python 3.10.\n",
        "        import collections\n",
        "        if not hasattr(collections, 'Mapping'):\n",
        "            collections.Mapping = collections.abc.Mapping\n",
        "\n",
        "        def make_kitchen_env(env_name):\n",
        "            env = make_env(env_name)\n",
        "            # Only use the first 30 dimensions (because the other half corresponds to the goal).\n",
        "            env = TruncateObservation(env, truncate_size=30)\n",
        "            return env\n",
        "        dataset = get_dataset(make_kitchen_env(FLAGS.env_name), FLAGS.env_name, filter_terminals=True)\n",
        "        dataset = dataset.copy({'observations': dataset['observations'][:, :30], 'next_observations': dataset['next_observations'][:, :30]})\n",
        "        dataset = dataset.copy({'masks': np.ones_like(dataset['masks'])})\n",
        "        dataset_gc = GCDataset(dataset, **FLAGS.gcdataset.to_dict())\n",
        "        eval_env = EpisodeMonitor(RewardOverride(make_kitchen_env(FLAGS.env_name)))\n",
        "\n",
        "        SingleTaskRewards = SingleTaskRewardFunction()\n",
        "        GoalReachingRewards = GoalReachingRewardFunction()\n",
        "        LinearRewards = LinearRewardFunction()\n",
        "        RandomRewards = RandomRewardFunction(num_simplex=10000, obs_len=30)\n",
        "        reward_fns = [GoalReachingRewards, LinearRewards, RandomRewards]\n",
        "\n",
        "        reward_fn_ratios = [FLAGS.rew_ratio_goal, FLAGS.rew_ratio_linear, FLAGS.rew_ratio_mlp]\n",
        "        slices = []\n",
        "        slices.append(0)\n",
        "        for j in range(len(reward_fns)):\n",
        "            slices.append(int(FLAGS.batch_size * reward_fn_ratios[j]))\n",
        "        slices[-1] = FLAGS.batch_size - sum(slices[:-1])\n",
        "        print(\"Number of samples for each reward func: \", slices)\n",
        "        slices = np.cumsum(slices)\n",
        "        print(\"Cumsum of samples for each reward func: \", slices)\n",
        "        test_rewards = [\n",
        "            (SingleTaskRewards, 'binary_bottom_left_burner', np.array([1, 0, 0, 0, 0, 0, 0])),\n",
        "            (SingleTaskRewards, 'binary_top_left_burner', np.array([0, 1, 0, 0, 0, 0, 0])),\n",
        "            (SingleTaskRewards, 'binary_light_switch', np.array([0, 0, 1, 0, 0, 0, 0])),\n",
        "            (SingleTaskRewards, 'binary_slide_cabinet', np.array([0, 0, 0, 1, 0, 0, 0])),\n",
        "            (SingleTaskRewards, 'binary_hinge_cabinet', np.array([0, 0, 0, 0, 1, 0, 0])),\n",
        "            (SingleTaskRewards, 'binary_microwave', np.array([0, 0, 0, 0, 0, 1, 0])),\n",
        "            (SingleTaskRewards, 'binary_kettle', np.array([0, 0, 0, 0, 0, 0, 1])),\n",
        "        ]\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    for i in tqdm.tqdm(range(1, FLAGS.max_steps + 1),\n",
        "                       smoothing=0.1,\n",
        "                       dynamic_ncols=True):\n",
        "\n",
        "        # Sample a batch of trajectories.\n",
        "        # Get future states from the trajectory, and random states.\n",
        "        num_traj_states = min(8, FLAGS.reward_pairs_encode-1)\n",
        "\n",
        "        num_random_states = FLAGS.reward_pairs_encode - num_traj_states\n",
        "        num_random_states_decode = FLAGS.reward_pairs_decode\n",
        "        batch = dataset_gc.sample_traj_random(FLAGS.batch_size, num_traj_states, num_random_states, num_random_states_decode)\n",
        "        # The first index of traj_states contains the CURRENT state.\n",
        "        assert batch['traj_states'].shape == (FLAGS.batch_size, num_traj_states, dataset['observations'].shape[-1])\n",
        "        assert batch['random_states'].shape == (FLAGS.batch_size, num_random_states, dataset['observations'].shape[-1])\n",
        "        assert batch['random_states_decode'].shape == (FLAGS.batch_size, num_random_states_decode, dataset['observations'].shape[-1])\n",
        "\n",
        "        encode_pairs = np.zeros((FLAGS.batch_size, FLAGS.reward_pairs_encode, dataset['observations'].shape[-1] + 1))\n",
        "        decode_pairs = np.zeros((FLAGS.batch_size, FLAGS.reward_pairs_decode, dataset['observations'].shape[-1] + 1))\n",
        "        rewards = np.zeros((FLAGS.batch_size))\n",
        "        masks = np.zeros((FLAGS.batch_size))\n",
        "        for j in range(len(reward_fns)):\n",
        "            batch_traj_states = batch['traj_states'][slices[j]:slices[j+1], :, :]\n",
        "            batch_random_states = batch['random_states'][slices[j]:slices[j+1], :, :]\n",
        "            batch_random_states_decode = batch['random_states_decode'][slices[j]:slices[j+1], :, :]\n",
        "            params_slice, encode_pairs_slice, decode_pairs_slice, rewards_slice, masks_slice = reward_fns[j].generate_params_and_pairs(batch_traj_states, batch_random_states, batch_random_states_decode)\n",
        "            encode_pairs[slices[j]:slices[j+1], :, :] = encode_pairs_slice\n",
        "            decode_pairs[slices[j]:slices[j+1], :, :] = decode_pairs_slice\n",
        "            rewards[slices[j]:slices[j+1]] = rewards_slice\n",
        "            masks[slices[j]:slices[j+1]] = masks_slice\n",
        "\n",
        "        assert len(encode_pairs.shape) == 3 # (batch_size, reward_pairs_encode, obs_dim + 1)\n",
        "        assert len(decode_pairs.shape) == 3 # (batch_size, reward_pairs_decode, obs_dim + 1)\n",
        "\n",
        "        batch['rewards'] = rewards\n",
        "        batch['masks'] = masks\n",
        "        batch['reward_pairs_encode'] = encode_pairs\n",
        "        batch['reward_pairs_decode'] = decode_pairs\n",
        "\n",
        "        if FLAGS.use_discrete_xy and 'ant' in FLAGS.env_name:\n",
        "            batch['observations'] = d4rl_ant.discretize_obs(batch['observations'])\n",
        "            batch['next_observations'] = d4rl_ant.discretize_obs(batch['next_observations'])\n",
        "\n",
        "        # Don't train agents using the auxilliary physics states.\n",
        "        if 'walker' in FLAGS.env_name:\n",
        "            batch['observations'] = batch['observations'][:, :24]\n",
        "            batch['next_observations'] = batch['next_observations'][:, :24]\n",
        "        elif 'cheetah' in FLAGS.env_name:\n",
        "            batch['observations'] = batch['observations'][:, :17]\n",
        "            batch['next_observations'] = batch['next_observations'][:, :17]\n",
        "\n",
        "        if agent is None:\n",
        "            agent = create_learner(FLAGS.seed,\n",
        "                batch,\n",
        "                transformer_params=FLAGS.transformer.to_dict(),\n",
        "                max_steps=FLAGS.max_steps,\n",
        "                **FLAGS.agent)\n",
        "            if FLAGS.load_dir is not None:\n",
        "                agent = checkpoints.restore_checkpoint(FLAGS.load_dir, agent)\n",
        "\n",
        "        agent, update_info = agent.update(batch,\n",
        "                                        train_encoder=(i <= FLAGS.agent['warmup_steps']),\n",
        "                                        train_actor=(i > FLAGS.agent['warmup_steps']),\n",
        "                                        train_critic=(i > FLAGS.agent['warmup_steps'])\n",
        "        )\n",
        "\n",
        "        if i % FLAGS.log_interval == 0:\n",
        "            train_metrics = {f'training/{k}': v for k, v in update_info.items()}\n",
        "\n",
        "            # Debug logs for wandb.\n",
        "            if i == FLAGS.log_interval:\n",
        "                fig, ax = plt.subplots(figsize=(3.0, 3.0))\n",
        "                reward_pair_rewards = batch['reward_pairs_encode'][:, :, -1].flatten()\n",
        "                ax.hist(reward_pair_rewards, bins=32)\n",
        "                ax.set_title(\"Reward Values (For Reward Pairs)\")\n",
        "                wandb.log({\"Reward Values (For Reward Pairs)\": wandb.Image(fig)}, step=i)\n",
        "                fig.clf()\n",
        "                plt.cla()\n",
        "                plt.clf()\n",
        "                plt.close('all')\n",
        "\n",
        "            wandb.log(train_metrics, step=i)\n",
        "\n",
        "        # Evaluate the unsupervised rewards on the evaluation rewards.\n",
        "        if i % 10000 == 0 and i < FLAGS.agent['warmup_steps']:\n",
        "            eval_rew_metrics = {}\n",
        "            for k, test_reward in enumerate(test_rewards):\n",
        "                test_reward_generator, test_reward_label, test_reward_params = test_reward\n",
        "                random_states_encode = dataset.sample(FLAGS.reward_pairs_encode_test)['observations']\n",
        "                random_states_decode = dataset.sample(FLAGS.reward_pairs_encode_test)['observations']\n",
        "                test_reward_pairs = test_reward_generator.make_encoder_pairs_testing(test_reward_params[None], \\\n",
        "                                                                                        random_states_encode[None])\n",
        "                test_reward_pairs_decode = test_reward_generator.make_encoder_pairs_testing(test_reward_params[None], \\\n",
        "                                                                                        random_states_decode[None])\n",
        "                assert test_reward_pairs.shape == (1, FLAGS.reward_pairs_encode_test, dataset['observations'].shape[-1] + 1)\n",
        "                true_decode_rewards = test_reward_pairs_decode[0, :, -1] # (reward_pairs_encode_test, )\n",
        "                decode_predictions = agent.get_reward_pred(random_states_decode, test_reward_pairs)[0] # (reward_pairs_encode_test, )\n",
        "                assert true_decode_rewards.shape == decode_predictions.shape\n",
        "                loss = jnp.mean((true_decode_rewards - decode_predictions)**2)\n",
        "                eval_rew_metrics[f'rew_pred/{test_reward_label}'] = loss\n",
        "            if 'ant' in FLAGS.env_name:\n",
        "                # Merge separate metrics into simpler metrics.\n",
        "                total_goals = eval_rew_metrics['rew_pred/goal_bottom'] + eval_rew_metrics['rew_pred/goal_center'] + eval_rew_metrics['rew_pred/goal_top'] + eval_rew_metrics['rew_pred/goal_left'] + eval_rew_metrics['rew_pred/goal_right']\n",
        "                total_velocity = eval_rew_metrics['rew_pred/vel_left'] + eval_rew_metrics['rew_pred/vel_up'] + eval_rew_metrics['rew_pred/vel_down'] + eval_rew_metrics['rew_pred/vel_right']\n",
        "                total_simplex = eval_rew_metrics['rew_pred/simplex_1'] + eval_rew_metrics['rew_pred/simplex_2'] + eval_rew_metrics['rew_pred/simplex_3'] + eval_rew_metrics['rew_pred/simplex_4'] + eval_rew_metrics['rew_pred/simplex_5']\n",
        "                total_path = eval_rew_metrics['rew_pred/path_center'] + eval_rew_metrics['rew_pred/path_loop'] + eval_rew_metrics['rew_pred/path_edges']\n",
        "                eval_rew_metrics['rew_pred_total/total_goals'] = total_goals\n",
        "                eval_rew_metrics['rew_pred_total/total_velocity'] = total_velocity\n",
        "                eval_rew_metrics['rew_pred_total/total_simplex'] = total_simplex\n",
        "                eval_rew_metrics['rew_pred_total/total_path'] = total_path\n",
        "                wandb.log(eval_rew_metrics, step=i)\n",
        "            elif 'dmc' in FLAGS.env_name:\n",
        "                # Merge separate metrics into simpler metrics.\n",
        "                total_goals = eval_rew_metrics['rew_pred/goal_1'] + eval_rew_metrics['rew_pred/goal_2'] + eval_rew_metrics['rew_pred/goal_3'] + eval_rew_metrics['rew_pred/goal_4'] + eval_rew_metrics['rew_pred/goal_5']\n",
        "                if 'cheetah' in FLAGS.env_name:\n",
        "                    total_vel = eval_rew_metrics['rew_pred/vel10Back'] + eval_rew_metrics['rew_pred/vel2Back'] + eval_rew_metrics['rew_pred/vel2'] + eval_rew_metrics['rew_pred/vel10']\n",
        "                elif 'walker' in FLAGS.env_name:\n",
        "                    total_vel = eval_rew_metrics['rew_pred/vel0.1'] + eval_rew_metrics['rew_pred/vel1'] + eval_rew_metrics['rew_pred/vel4'] + eval_rew_metrics['rew_pred/vel8']\n",
        "                eval_rew_metrics['rew_pred_total/total_goals'] = total_goals\n",
        "                eval_rew_metrics['rew_pred_total/total_vel'] = total_vel\n",
        "                wandb.log(eval_rew_metrics, step=i)\n",
        "\n",
        "        # Evaluate on test tasks. These are training tasks AND test tasks.\n",
        "        if i % FLAGS.eval_interval == 0 or (i == 10000 and FLAGS.eval_interval < 10006000):\n",
        "            print(\"Performing Eval Loop\")\n",
        "            record_video = i % FLAGS.video_interval == 0\n",
        "            eval_metrics = {}\n",
        "\n",
        "            for k, test_reward in enumerate(test_rewards):\n",
        "                test_reward_generator, test_reward_label, test_reward_params = test_reward\n",
        "                print(\"Eval on reward function\", test_reward_label)\n",
        "\n",
        "                # Update eval env to record the right reward.\n",
        "                def override_reward(s):\n",
        "                    r = test_reward_generator.compute_reward(s[None,:], test_reward_params[None, :])\n",
        "                    return r[0]\n",
        "                eval_env.env.reward_fn = override_reward\n",
        "                random_states_encode = dataset.sample(FLAGS.reward_pairs_encode_test)['observations']\n",
        "                test_reward_pairs = test_reward_generator.make_encoder_pairs_testing(test_reward_params[None], \\\n",
        "                                                                                        random_states_encode[None])\n",
        "                assert test_reward_pairs.shape == (1, FLAGS.reward_pairs_encode_test, dataset['observations'].shape[-1] + 1)\n",
        "\n",
        "                # Run policy.\n",
        "                policy_fn = functools.partial(supply_rng(agent.sample_actions), temperature=0.0, reward_pairs=test_reward_pairs)\n",
        "                if 'dmc' in FLAGS.env_name:\n",
        "                    eval_info, trajs = evaluate(policy_fn, eval_env, num_episodes=FLAGS.eval_episodes, record_video=record_video, return_trajectories=True, clip_return_at_goal=('goal' in test_reward_label), use_discrete_xy=False, clip_margin=100)\n",
        "                elif 'antmaze' in FLAGS.env_name:\n",
        "                    eval_info, trajs = evaluate(policy_fn, eval_env, num_episodes=FLAGS.eval_episodes, record_video=record_video, return_trajectories=True, clip_return_at_goal=('goal' in test_reward_label), use_discrete_xy=FLAGS.use_discrete_xy)\n",
        "                elif 'kitchen' in FLAGS.env_name:\n",
        "                    eval_info, trajs = evaluate(policy_fn, eval_env, num_episodes=FLAGS.eval_episodes, record_video=record_video, return_trajectories=True, clip_return_at_goal=('goal' in test_reward_label), use_discrete_xy=False, binary_return=('binary' in test_reward_label))\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "                eval_metrics[f'evaluation/{test_reward_label}.return'] = eval_info['episode.return']\n",
        "                if record_video:\n",
        "                    wandb.log({f'{test_reward_label}.video': eval_info['video']}, step=i)\n",
        "\n",
        "                # Antmaze Specific Logging\n",
        "                if 'antmaze' in FLAGS.env_name and 'large' in FLAGS.env_name and FLAGS.env_name.startswith('antmaze'):\n",
        "                    #import fre.experiment.ant_helper as ant_helper\n",
        "                    # Make an image of the trajectories.\n",
        "                    traj_image = d4rl_ant.trajectory_image(eval_env, trajs)\n",
        "                    # eval_metrics[f'trajectories/{test_reward_label}'] = wandb.Image(traj_image)\n",
        "\n",
        "                    # Make image of reward function predictions.\n",
        "                    test_reward_expand = np.tile(test_reward_params[None, :], (280, 1)) # (280, 3)\n",
        "                    ground_truth_rew = lambda s_grid : test_reward_generator.compute_reward(s_grid, test_reward_expand)\n",
        "                    true_rew_img = ant_helper.value_image(eval_env, dataset, ground_truth_rew, None)\n",
        "                    # eval_metrics[f'draw_true/{test_reward_label}'] = wandb.Image(true_rew_img)\n",
        "\n",
        "                    mask = []\n",
        "                    for pair in test_reward_pairs[0]:\n",
        "                        mask.append(pair[:2])\n",
        "                    mask_rew_img = ant_helper.value_image(eval_env, dataset, ground_truth_rew, mask)\n",
        "                    # eval_metrics[f'draw_mask/{test_reward_label}'] = wandb.Image(mask_rew_img)\n",
        "\n",
        "                    pred_rew = lambda s_grid : agent.get_reward_pred(s_grid, test_reward_pairs)\n",
        "                    pred_rew_img = ant_helper.value_image(eval_env, dataset, pred_rew, None)\n",
        "                    # eval_metrics[f'draw_pred/{test_reward_label}'] = wandb.Image(pred_rew_img)\n",
        "\n",
        "                    def pred_value(s_grid):\n",
        "                        if FLAGS.use_discrete_xy and 'ant' in FLAGS.env_name:\n",
        "                            s_grid = d4rl_ant.discretize_obs(s_grid)\n",
        "                        return agent.get_value_pred(s_grid, test_reward_pairs)\n",
        "                    pred_value_img = ant_helper.value_image(eval_env, dataset, pred_value, None, clip=False)\n",
        "                    # eval_metrics[f'draw_value1/{test_reward_label}'] = wandb.Image(pred_value_img1)\n",
        "\n",
        "\n",
        "                    full_img = np.concatenate([\n",
        "                        np.concatenate([true_rew_img, mask_rew_img], axis=0),\n",
        "                        np.concatenate([pred_rew_img, traj_image], axis=0),\n",
        "                        np.concatenate([pred_value_img, pred_value_img], axis=0)\n",
        "                    ], axis=1)\n",
        "                    print(\"Min/Max of full_img is\", np.min(full_img), np.max(full_img))\n",
        "                    # if any nans, breakpoint.\n",
        "                    if np.isnan(full_img).any():\n",
        "                        breakpoint()\n",
        "                    eval_metrics[f'draw/{test_reward_label}'] = wandb.Image(full_img)\n",
        "\n",
        "            if 'ant' in FLAGS.env_name:\n",
        "                # Merge separate metrics into simpler metrics.\n",
        "                total_goals = eval_metrics['evaluation/goal_bottom.return'] + eval_metrics['evaluation/goal_center.return'] + eval_metrics['evaluation/goal_top.return'] + eval_metrics['evaluation/goal_left.return'] + eval_metrics['evaluation/goal_right.return']\n",
        "                total_velocity = eval_metrics['evaluation/vel_left.return'] + eval_metrics['evaluation/vel_up.return'] + eval_metrics['evaluation/vel_down.return'] + eval_metrics['evaluation/vel_right.return']\n",
        "                total_simplex = eval_metrics['evaluation/simplex_1.return'] + eval_metrics['evaluation/simplex_2.return'] + eval_metrics['evaluation/simplex_3.return'] + eval_metrics['evaluation/simplex_4.return'] + eval_metrics['evaluation/simplex_5.return']\n",
        "                total_path = eval_metrics['evaluation/path_center.return'] + eval_metrics['evaluation/path_loop.return'] + eval_metrics['evaluation/path_edges.return']\n",
        "                eval_metrics['evaluation_total/total_goals'] = total_goals\n",
        "                eval_metrics['evaluation_total/total_velocity'] = total_velocity\n",
        "                eval_metrics['evaluation_total/total_simplex'] = total_simplex\n",
        "                eval_metrics['evaluation_total/total_path'] = total_path\n",
        "                print(eval_metrics)\n",
        "            elif 'dmc' in FLAGS.env_name:\n",
        "                total_goals = eval_metrics['evaluation/goal_1.return'] + eval_metrics['evaluation/goal_2.return'] + eval_metrics['evaluation/goal_3.return'] + eval_metrics['evaluation/goal_4.return'] + eval_metrics['evaluation/goal_5.return']\n",
        "                eval_metrics['evaluation/total_goals'] = total_goals\n",
        "            elif 'kitchen' in FLAGS.env_name:\n",
        "                total_test = 0.\n",
        "                for test_reward in test_rewards:\n",
        "                    total_test += eval_metrics[f'evaluation/{test_reward[1]}.return']\n",
        "                eval_metrics['evaluation/total_test'] = total_test\n",
        "\n",
        "            wandb.log(eval_metrics, step=i)\n",
        "\n",
        "        if i % FLAGS.save_interval == 0 and FLAGS.save_dir is not None:\n",
        "            checkpoints.save_checkpoint(FLAGS.save_dir, agent, i)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(main)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "kdELk4-mmIa1",
        "outputId": "0deb5dc6-730d-47c5-a85e-95f5552ff1e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'verbosity'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5c561e915191>\u001b[0m in \u001b[0;36m<cell line: 1236>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    298\u001b[0m   \"\"\"\n\u001b[1;32m    299\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     args = _run_init(\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mflags_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_init\u001b[0;34m(argv, flags_parser)\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0mcommand_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_process_name_useful\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[0;31m# Set up absl logging handler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_absl_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m   args = _register_and_parse_flags_with_usage(\n\u001b[1;32m    370\u001b[0m       \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/logging/__init__.py\u001b[0m in \u001b[0;36muse_absl_handler\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1223\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mabsl_handler\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsl_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbosity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_logging_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logger_levels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_logger_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;34m\"\"\"Returns the Flag object for the flag --name.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_hide_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'verbosity'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52D1JXWNmVVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}